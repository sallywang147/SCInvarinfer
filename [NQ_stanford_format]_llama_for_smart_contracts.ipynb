{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNus7v6peSEQej8klHWzQ40",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9c64bd6b1b514c4e8e9a9c744773086c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_985465dbc5214f88940cbefc5ad25942",
              "IPY_MODEL_5ecb51e1b9054275ac84670d46ea4f1c",
              "IPY_MODEL_14be3baf749342bfaf4d9cf9c97ea9ca"
            ],
            "layout": "IPY_MODEL_e966c24c91094abbaf1ffcc83f4bb036"
          }
        },
        "985465dbc5214f88940cbefc5ad25942": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02ebf78955ed4450b32c945d3cced85f",
            "placeholder": "​",
            "style": "IPY_MODEL_5ac89856246f4fe08a3047adb09e9d77",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5ecb51e1b9054275ac84670d46ea4f1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2d0a257be8f4425937d54a6f5553474",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_25cbb7a867df457b99640246e920e91a",
            "value": 3
          }
        },
        "14be3baf749342bfaf4d9cf9c97ea9ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_943b36e5da3146e288936f41763a27ea",
            "placeholder": "​",
            "style": "IPY_MODEL_0292b3213c17414a83cbd6627e23a5e0",
            "value": " 3/3 [02:07&lt;00:00, 41.40s/it]"
          }
        },
        "e966c24c91094abbaf1ffcc83f4bb036": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02ebf78955ed4450b32c945d3cced85f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ac89856246f4fe08a3047adb09e9d77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2d0a257be8f4425937d54a6f5553474": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25cbb7a867df457b99640246e920e91a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "943b36e5da3146e288936f41763a27ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0292b3213c17414a83cbd6627e23a5e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sallywang147/SCInvarinfer/blob/main/%5BNQ_stanford_format%5D_llama_for_smart_contracts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lILBOi-WpPWw",
        "outputId": "5e17d502-b3d2-48e1-fdf3-7cded0e603eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet accelerate\n",
        "!pip install -i --quiet https://test.pypi.org/simple/ bitsandbytes\n",
        "!pip install --quiet sentencepiece\n",
        "!pip install --quiet git+https://github.com/huggingface/transformers\n",
        "!pip install --quiet rich[jupyter]\n",
        "!pip install --quiet deepspeed\n",
        "!pip install --quiet openai\n",
        "!pip install --quiet gradio\n",
        "!pip install --quiet peft\n",
        "!pip install --quiet --upgrade datasets\n",
        "!pip install --quiet simplejson\n",
        "!pip install --quiet accelerate\n",
        "!pip install --quiet bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KELJ4pTduNrW",
        "outputId": "46162286-9e08-4340-a15c-64a1f5335c98"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: The index url \"--quiet\" seems invalid, please provide a scheme.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: --quiet, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting https://test.pypi.org/simple/\n",
            "  Downloading https://test.pypi.org/simple/ (9.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31m  ERROR: Cannot unpack file /tmp/pip-unpack-8nf9c1pn/simple.html (downloaded from /tmp/pip-req-build-zdsevgr4, content-type: text/html); cannot detect archive format\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Cannot determine archive format of /tmp/pip-req-build-zdsevgr4\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m765.4/765.4 KB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 KB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 KB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 KB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.8/136.8 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vanilla Llama v. 8bit_Quantized Llama##\n",
        "\n",
        "vanilla llama is slightly more capable than quantized llama, but not by much. For example, vanilla llama tends to repeat the same information less. "
      ],
      "metadata": {
        "id": "7bYljYiYAYcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import accelerate\n",
        "import bitsandbytes\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import AutoTokenizer, LlamaTokenizer, LlamaForCausalLM\n",
        "\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "            \"sallywww/trained_llama_stanford_format\" )\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(\n",
        "            \"sallywww/trained_llama_stanford_format\"\n",
        "        )\n",
        "#model = LlamaForCausalLM.from_pretrained('sallywww/Llama-7B')\n",
        "#tokenizer = AutoTokenizer.from_pretrained('sallywww/Llama-7B')\n",
        "\n",
        "#model.push_to_hub('sallywww/Llama-7B')\n",
        "#tokenizer.push_to_hub('sallywww/Llama-7B')"
      ],
      "metadata": {
        "id": "D2oAIPSSxSNf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77,
          "referenced_widgets": [
            "9c64bd6b1b514c4e8e9a9c744773086c",
            "985465dbc5214f88940cbefc5ad25942",
            "5ecb51e1b9054275ac84670d46ea4f1c",
            "14be3baf749342bfaf4d9cf9c97ea9ca",
            "e966c24c91094abbaf1ffcc83f4bb036",
            "02ebf78955ed4450b32c945d3cced85f",
            "5ac89856246f4fe08a3047adb09e9d77",
            "f2d0a257be8f4425937d54a6f5553474",
            "25cbb7a867df457b99640246e920e91a",
            "943b36e5da3146e288936f41763a27ea",
            "0292b3213c17414a83cbd6627e23a5e0"
          ]
        },
        "outputId": "6e3b4741-2042-4927-a43a-38678087cd92"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c64bd6b1b514c4e8e9a9c744773086c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = \"a loop invariant of smart contracts is \" \n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate\n",
        "generate_ids = model.generate(inputs.input_ids, max_length=100)\n",
        "tokenizer.batch_decode(generate_ids, \n",
        "                       skip_special_tokens=True, \n",
        "                       temperature=0.8, \n",
        "                       top_p=0.95, \n",
        "                       top_k=50, \n",
        "                       repeat_penalty=0.2,\n",
        "                       clean_up_tokenization_spaces=False)[0]\n"
      ],
      "metadata": {
        "id": "SSQCT4xwDt2-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e3b215cb-027b-450b-f096-3122d98a34c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' a loop invariant of smart contracts is  assert(x == y);'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inference with trained llama##\n",
        "\n",
        "Let's make some inferences without targeted instruction finetuning yet"
      ],
      "metadata": {
        "id": "SxnEHRJU-AXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contract = 'suicide.sol'\n",
        "lora_param = '/content/drive/MyDrive/lora-llama7b_insft_100e'\n",
        "test_file = open(f'/content/drive/MyDrive/experiments/baseline benchmark/{contract}', \"r\")\n",
        "test_data = test_file.read()"
      ],
      "metadata": {
        "id": "GRCW_RRKWqEm"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import random\n",
        "import torch\n",
        "#import gradio as gr\n",
        "import torch.nn as nn\n",
        "#import bitsandbytes\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "\n",
        "def infer_invariants(model, tokenizer, test_data, \\\n",
        "                     temperature, top_p, top_k,  max_new_tokens):\n",
        "  model.cuda()\n",
        "  model.eval()\n",
        "  inputs = tokenizer(test_data, return_tensors=\"pt\")\n",
        "  with torch.no_grad():\n",
        "      outputs = model.generate(input_ids=inputs[\"input_ids\"].cuda(), \\\n",
        "       temperature=temperature, \n",
        "       top_p=top_p, \n",
        "       top_k=top_k, \n",
        "      max_new_tokens=max_new_tokens)\n",
        "      invariants = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]\n",
        "  return invariants"
      ],
      "metadata": {
        "id": "oDS0kKTU3mBL"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = infer_invariants(model=model, \n",
        "                          tokenizer=tokenizer, \n",
        "                          test_data=test_data,\n",
        "                          temperature=None, \n",
        "                          top_p=None, \n",
        "                          top_k=None, \n",
        "                          max_new_tokens=670)\n",
        "print(output.strip())\n",
        "with open(f\"/content/drive/MyDrive/experiments/evaluation/noPEFT_finetuned_llama/{contract}\", \"w\", encoding=\"utf-8\") as file:  \n",
        "    file.write(output.strip()+\"\\n\")\n",
        "file.close()"
      ],
      "metadata": {
        "id": "CWM2HSG5D-CM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd06d75b-7f03-4aea-c193-90c3a1dc8580"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1  // SPDX-License-Identifier: MIT\n",
            "2  pragma solidity >=0.4.24 <0.6.0;\n",
            "3  \n",
            "4  contract suicide{\n",
            "5         function initOwners(address[] _owners,\n",
            "6                             uint _required){\n",
            "7         if (m_numOwners > 0) throw;\n",
            "8         m_numOwners = _owners.length + 1;\n",
            "9         m_owners[1] = uint(msg.sender);\n",
            "10         m_ownerIndex[uint(msg.sender)] = 1;\n",
            "11         m_required = _required;\n",
            "12 \n",
            "13         }\n",
            "14 \n",
            "15         function suicide(address _to) {\n",
            "16             uint ownerIndex = m_ownerIndex[uint(msg.sender)];\n",
            "17             if (ownerIndex == 0) return;\n",
            "18             var pending = m_pending[sha3(msg.data)];\n",
            "19             if (pending.yetNeeded == 0) {\n",
            "20             pending.yetNeeded = m_required;\n",
            "21             pending.ownersDone = 0;\n",
            "22             }\n",
            "23             uint ownerIndexBit = 2**ownerIndex;\n",
            "24             if (pending.ownersDone   ownerIndexBit == 0) {\n",
            "25             if (pending.yetNeeded <= 1)\n",
            "26                 suicide(_to);\n",
            "27             else {\n",
            "28                 pending.yetNeeded--;\n",
            "29                 pending.ownersDone |= ownerIndexBit;\n",
            "30             }\n",
            "31             }\n",
            "32         }\n",
            "33  } 15+ assert(_required > 0); \n",
            "15+  assert(m_numOwners > 0);\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for d in os.listdir(): \n",
        "  if os.path.isdir(d) and d.startswith('lora-'):\n",
        "      print(d)\n",
        "      lora_model = d\n",
        "  \n",
        "inference = generate_text(\n",
        "                lora_param,\n",
        "                test_data,\n",
        "                #temperature=0.8,\n",
        "                #top_p=0.95,\n",
        "                #top_k=50,\n",
        "                #repeat_penalty=0.02,\n",
        "                max_new_tokens=750)"
      ],
      "metadata": {
        "id": "BErBaeZh9xjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-3a70aCSAoAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Instruction Finetuning##\n",
        "\n",
        "We're mostly going to change the dataset format here and add additional instructions for llama "
      ],
      "metadata": {
        "id": "9eSwRi0cAZHF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FwMAxtCoAglw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPT-style finetuning: OOM error persisted##"
      ],
      "metadata": {
        "id": "1ksQlD8qhyV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "from numpy import random\n",
        "import pandas as pd\n",
        "import gspread\n",
        "#autenticating to google\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "#defining my worksheet\n",
        "worksheet = gc.open('short_invariants_for_gpt').sheet1\n",
        "#get_all_values gives a list of rows\n",
        "rows = worksheet.get_all_values()\n",
        "#Convert to a DataFrame \n",
        "cols = ['Target']\n",
        "df = pd.DataFrame(rows, columns=cols)\n",
        "df"
      ],
      "metadata": {
        "id": "SB1TLeCiD53K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_params={\n",
        "    \"MODEL\":\"llama_model\",             # model_type: t5-large\n",
        "    \"MAX_LENGTH\": 3036,  # max length of source text\n",
        "   # \"SEED\": random.randint(1000)    # randomized seeds to shuffle test set\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "Wm0QPuWiGPGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlYaKW9h4ai_"
      },
      "source": [
        "# Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vLQPGAn4v17"
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from rich.console import Console\n",
        "from tqdm import tqdm, trange\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import torch\n",
        "import csv\n",
        "\n",
        "class GPTDataSetClass(Dataset):\n",
        "  \"\"\"\n",
        "  Creating a custom dataset for reading the dataset and \n",
        "  loading it into the dataloader to pass it to the neural network for finetuning the model\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, target_label, truncate=False, \\\n",
        "               gpt2_type=model, \\\n",
        "               max_length=2048):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.target_tokens = []\n",
        "\n",
        "    for row in df['Target']:\n",
        "        self.target_tokens.append(torch.tensor(\n",
        "            self.tokenizer.encode(f\"<|{target_label}|>{row[:max_length]}<|endoftext|>\")\n",
        "            ))  \n",
        "    if truncate:\n",
        "            self.target_tokens = self.target_tokens[:20000]\n",
        "    self.length = len(self.target_tokens)   \n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.target_tokens[index]   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Accumulated batch size (since GPT2 is so big)\n",
        "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
        "    if packed_tensor is None:\n",
        "        return new_tensor, True, None\n",
        "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
        "        return packed_tensor, False, new_tensor\n",
        "    else:\n",
        "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
        "        return packed_tensor, True, None"
      ],
      "metadata": {
        "id": "gJ2ccsedzcAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    dataset, model, tokenizer,\n",
        "    batch_size=0.1, epochs=3, lr=2e-5,\n",
        "    max_seq_len=2048, warmup_steps=200,\n",
        "    gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\",\n",
        "    test_mode=False, save_model_on_epoch=False,\n",
        "):\n",
        "    acc_steps = 10\n",
        "    device=torch.device(\"cuda\")\n",
        "    model = model\n",
        "    model.train()\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
        "    )\n",
        "\n",
        "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "    loss=0\n",
        "    accumulating_batch_count = 0\n",
        "    input_tensor = None\n",
        "    loss_list = []\n",
        "    epoch_list = []\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = []\n",
        "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
        "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
        "\n",
        "            if carry_on and idx != len(train_dataloader) - 1:\n",
        "                continue\n",
        "\n",
        "            input_tensor = input_tensor.to('cuda')\n",
        "            outputs = model(input_tensor, labels=input_tensor)  \n",
        "            loss = outputs[0] \n",
        "            total_loss.append(float(loss.item()))       \n",
        "            loss.backward()\n",
        "            torch.cuda.empty_cache()                      \n",
        "            if (accumulating_batch_count % batch_size) == 0:\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                model.zero_grad()\n",
        "  \n",
        "            accumulating_batch_count += 1\n",
        "            input_tensor = None\n",
        "\n",
        "        #training_logger.add_row(str(epoch), str(np.mean(total_loss)))       \n",
        "        if save_model_on_epoch:\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
        "            )\n",
        "        loss_list.append(np.mean(total_loss))\n",
        "        epoch_list.append(epoch) \n",
        "        print(f\"for epoch {epoch} the loss is {np.mean(total_loss)}\\n\")\n",
        "   # console.print(training_logger)   \n",
        "   # plot_loss(epoch_list, loss_list)\n",
        "    return model"
      ],
      "metadata": {
        "id": "PTVyI2fzzn3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_GPT2(df, model_params):   \n",
        "  #console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
        "\n",
        "  # tokenzier for encoding the text\n",
        "  dataset = GPTDataSetClass(df['Target'], truncate=False, gpt2_type=model) \n",
        "  #Get the tokenizer and model\n",
        "  #tokenizer = GPT2Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
        " # model = GPT2LMHeadModel.from_pretrained(model_params[\"MODEL\"])  \n",
        "  trained_model = train(dataset, model, tokenizer)\n",
        "  console.log(f\"[Saving Model]...\\n\")\n",
        "  #Saving the model after training\n",
        "  path = os.path.join('/content/output', \"model_files\")\n",
        "  model.save_pretrained(path)\n",
        "  tokenizer.save_pretrained(path)\n",
        "  console.print(f\"\"\"[Model] Model saved @ {os.path.join('/content/output', \"model_files\")}\\n\"\"\")\n",
        "  \n",
        "  # logging\n",
        "  console.log(f\"[Data]: Reading Raw data...\\n\")\n",
        "\n",
        "\n",
        "  # Creation of Dataset and Dataloader\n",
        "  # Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
        "\n",
        "  console.print(f\"FULL Dataset: {df.shape}\")\n",
        "  return trained_model, tokenizer\n"
      ],
      "metadata": {
        "id": "nOHr5lM82h53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    entry_count=10,\n",
        "    entry_length=2048, #maximum number of words\n",
        "    top_p=0.8,\n",
        "    temperature=1.,\n",
        "):\n",
        "    model.eval()\n",
        "    generated_num = 0\n",
        "    generated_list = []\n",
        "\n",
        "    filter_value = -float(\"Inf\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for entry_idx in trange(entry_count):\n",
        "\n",
        "            entry_finished = False\n",
        "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "\n",
        "            for i in range(entry_length):\n",
        "                outputs = model(generated, labels=generated)\n",
        "                loss, logits = outputs[:2]\n",
        "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
        "                    ..., :-1\n",
        "                ].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                logits[:, indices_to_remove] = filter_value\n",
        "\n",
        "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
        "                generated = torch.cat((generated, next_token), dim=1)\n",
        "\n",
        "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
        "                    entry_finished = True\n",
        "\n",
        "                if entry_finished:\n",
        "\n",
        "                    generated_num = generated_num + 1\n",
        "\n",
        "                    output_list = list(generated.squeeze().numpy())\n",
        "                    output_text = tokenizer.decode(output_list)\n",
        "                    generated_list.append(output_text + '\\n')\n",
        "                    break\n",
        "            \n",
        "            if not entry_finished:\n",
        "              output_list = list(generated.squeeze().numpy())\n",
        "              output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n",
        "              generated_list.append(output_text + '\\n')\n",
        "                \n",
        "    return generated_list\n",
        "\n",
        "#Function to generate multiple sentences. Test data should be a dataframe\n",
        "def text_generation(model, tokenizer, test_data):\n",
        "  generated_code = []\n",
        "  for i in range(len(test_data)):    \n",
        "    x = generate(model.to('cpu'), tokenizer, test_data['Target'][i], entry_count=1)\n",
        "    generated_code.append(x)\n",
        "  return generated_code\n",
        "\n",
        "#Run the functions to generate the lyrics\n",
        "\n",
        "def test_fine_tuned_gpt2(model, tokenizer, df): \n",
        "    test_set = df.sample(n=1)\n",
        "    df = df.loc[~df.index.isin(test_set.index)]\n",
        "\n",
        "    #Reset the indexes\n",
        "    test_set = test_set.reset_index()\n",
        "    df = df.reset_index()\n",
        "\n",
        "    #For the test set only, keep last 20 words in a new column, then remove them from original column\n",
        "    test_set['Target'] = test_set['Target'].str.split().apply(' '.join)\n",
        "    generated_code = text_generation(model, tokenizer, test_set)\n",
        "    print(generated_code)\n"
      ],
      "metadata": {
        "id": "ujnNVivM1bMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To solve CUDA out of memory error\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "os.environ['CUDA_VISIBLE_DEVICES']='0, 1, 2, 3, 4, 5, 6, 7, 8'\n",
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
      ],
      "metadata": {
        "id": "atnMJANnJlf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model, trained_tokenizer = fine_tune_GPT2(df, model_params)"
      ],
      "metadata": {
        "id": "GtBLzx0bHiXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-4qjuS_RhdxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "import deepspeed\n",
        "import torch\n",
        "from beartype import beartype\n",
        "from beartype.typing import Tuple\n",
        "from einops import rearrange\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "local_rank, world_size = setup_model_parallel()\n",
        "            # use load_model_test for testing\n",
        "  self.model, self.tokenizer = load_model(\n",
        "   ckpt_dir=model,\n",
        "   tokenizer_path=tokenizer,\n",
        "   local_rank=local_rank,\n",
        "   world_size=world_size,\n",
        "   froze_embeddings=config.froze_embeddings,\n",
        "   use_fairscale=config.use_fairscale,\n",
        "   max_batch_size=config.batch_size,\n",
        "    )\n",
        "        elif config.model in hf_models_seq_2_seq:\n",
        "    \n",
        "            # galactica tokenizer eos_token is None\n",
        "            if self.tokenizer.eos_token is None:\n",
        "                self.tokenizer.eos_token = \"</s>\"\n",
        "                self.tokenizer.eos_token_id = 0\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                config.model,\n",
        "            )\n",
        "            self.model.to(config.device)\n",
        "\n",
        "        # load the model from model_folder\n",
        "        self.load()\n",
        "\n",
        "    @beartype\n",
        "    def load(self) -> None:\n",
        "        \"\"\"Load the model from the path\n",
        "\n",
        "        Args:\n",
        "            path (str): path to the model\n",
        "        \"\"\"\n",
        "        path = ModelLoader.check_model_path(\n",
        "            config=self.config,\n",
        "            is_checkpoint=False,\n",
        "            current_epoch=None,\n",
        "        )\n",
        "        if path is not None:\n",
        "            print(\"Loading ...\")\n",
        "            model_dict = torch.load(path)\n",
        "            self.model.load_state_dict(model_dict[\"model\"])\n",
        "            self.head.load_state_dict(model_dict[\"head\"])\n",
        "\n",
        "    @beartype\n",
        "    def save(self) -> None:\n",
        "        \"\"\"Save the model to the path\n",
        "\n",
        "        Args:\n",
        "            path (Optional[str], optional): Path to store the model.\n",
        "                Defaults to None.\n",
        "        \"\"\"\n",
        "        model_folder, model_name, path = ModelLoader.get_model_path(\n",
        "            config=self.config,\n",
        "            is_checkpoint=False,\n",
        "            current_epoch=None,\n",
        "        )\n",
        "        print(f\"Saving model to {path} ...\")\n",
        "        torch.save(\n",
        "            {\"model\": self.model.state_dict(), \"head\": self.head.state_dict()},\n",
        "            path,\n",
        "        )\n",
        "\n",
        "    def parameters(self, **kwargs):\n",
        "        \"\"\"Return the parameters of the model\n",
        "\n",
        "        Args:\n",
        "            **kwargs:\n",
        "        \"\"\"\n",
        "        return self.model.parameters()\n",
        "\n",
        "    @beartype\n",
        "    def forward(\n",
        "        self, sequences: torch.Tensor, sequences_mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Generate logits to have probability distribution over the vocabulary\n",
        "            of the actions\n",
        "\n",
        "        Args:\n",
        "            sequences (torch.Tensor): Sequences of states and actions used to\n",
        "                    compute token logits for the whole list of sequences\n",
        "            attention_mask (torch.Tensor): Mask for the sequences attention\n",
        "\n",
        "        Returns:\n",
        "            logits (torch.Tensor): Logits for the actions taken\n",
        "        \"\"\"\n",
        "        model_output = self.model.forward(\n",
        "            sequences, attention_mask=sequences_mask\n",
        "        )\n",
        "        # need to return logits for the actions\n",
        "        if self.config.model in hf_models_causal_lm:\n",
        "            model_output = model_output.logits\n",
        "        if self.config.debug:\n",
        "            print(\"ActorModel.forward\")\n",
        "            print(\"model_output_logits shape\", model_output.shape)\n",
        "            print(\"model_output logits\", model_output)\n",
        "        return model_output\n",
        "\n",
        "    @beartype\n",
        "    @torch.no_grad()\n",
        "    def generate(\n",
        "        self, states: torch.Tensor, state_mask: torch.Tensor\n",
        "    ) -> Tuple:\n",
        "        \"\"\"Generate actions and sequences=[states, actions] from state\n",
        "            (i.e. input of the prompt generator model)\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor): the input of the user\n",
        "            state_mask (torch.Tensor): Mask for the state input (for padding)\n",
        "\n",
        "        Returns:\n",
        "            actions (torch.Tensor): Actions generated from the state\n",
        "            sequences (torch.Tensor): Sequences generated from the\n",
        "                state as [states, actions]\n",
        "        \"\"\"\n",
        "        temperature = self.config.temperature\n",
        "        # max sequence length for the actor (i.e. prompt + completion)\n",
        "        # from config file - it depends by the model used\n",
        "        max_sequence_length = self.config.max_sequence_length\n",
        "        # max tokens generated by the actor (completion only) from config file\n",
        "        max_tokens = self.config.max_tokens\n",
        "        # temperature for the actor\n",
        "        max_generation_possible = max_sequence_length - states.shape[1]\n",
        "        # take the minimum between the maximum token that you want to generate\n",
        "        # and the token that is possible to generate given the maximum sequence\n",
        "        # supported\n",
        "        max_completion = min(max_tokens, max_generation_possible)\n",
        "        if max_completion <= 0:\n",
        "            raise ValueError(\n",
        "                \"The maximum completion available is <= 0 the prompt is too \"\n",
        "                + \"long w.r.t the model sequence length\"\n",
        "            )\n",
        "        # the max_length is then the input length + the completion length\n",
        "        max_length = states.shape[1] + max_completion\n",
        "        # generate\n",
        "        sequences = self.model.generate(\n",
        "            input_ids=states,\n",
        "            attention_mask=state_mask,\n",
        "            temperature=temperature,\n",
        "            max_length=max_length,\n",
        "        )\n",
        "        actions = sequences[:, states.shape[1] :]  # noqa E203\n",
        "        if self.config.debug:\n",
        "            print(\"ActorModel.generate\")\n",
        "            print(\"state\", states)\n",
        "            print(\"state shape\", states.shape)\n",
        "            print(\"sequence shape\", sequences.shape)\n",
        "            print(\"sequence\", sequences)\n",
        "            print(\"actions shape\", actions.shape)\n",
        "            print(\"actions\", actions)\n",
        "        return actions, sequences\n",
        "\n",
        "\n",
        "class ActorDataset(Dataset):\n",
        "    \"\"\"Dataset for the pretraining of the actor model\n",
        "    read a json file with the following format:\n",
        "    [\n",
        "        {\n",
        "            \"user_input\": \"...\"\n",
        "            \"completion\": \"...\"\n",
        "        },\n",
        "        ...\n",
        "    ]\n",
        "    Where:\n",
        "        user_input: the input of the user\n",
        "        completion: the output of the user\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path: str) -> None:\n",
        "        self.path = path\n",
        "        with open(path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "            self.data = [d[\"user_input\"] + \" \" + d[\"completion\"] for d in data]\n",
        "        self.len = len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def __len__(\n",
        "        self,\n",
        "    ):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "class ActorTrainer:\n",
        "    \"\"\"Used to pre-train the actor model to generate better prompts.\n",
        "\n",
        "    Args:\n",
        "        config (ConfigActor): Configuration for the actor model\n",
        "\n",
        "    Attributes:\n",
        "        config (ConfigActor): Configuration for the actor model\n",
        "        model (ActorModel): Actor model\n",
        "        loss_function (torch.nn.CrossEntropyLoss): Loss function\n",
        "        optimizer (torch.optim.Adam): Optimizer\n",
        "        validation_flag (bool): Flag to indicate if the validation dataset\n",
        "            is provided\n",
        "        training_stats (TrainingStats): Training statistics\n",
        "\n",
        "    Methods:\n",
        "        train: Train the actor model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: ConfigActor) -> None:\n",
        "        # load the model, optimizer, loss function and config\n",
        "        self.config = config\n",
        "        self.model = ActorModel(config)\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            self.model.parameters(), lr=config.lr\n",
        "        )\n",
        "\n",
        "        # check checkpoint, datasets and other data\n",
        "        self.validation_flag = False\n",
        "        self.training_stats = TrainingStats()\n",
        "        if config.validation_dataset_path is not None:\n",
        "            self.validation_flag = True\n",
        "\n",
        "        # create dataloaders\n",
        "        self.train_dataset = ActorDataset(config.train_dataset_path)\n",
        "        self.train_dataloader = DataLoader(\n",
        "            self.train_dataset, batch_size=config.batch_size\n",
        "        )\n",
        "        if self.validation_flag:\n",
        "            self.eval_dataset = ActorDataset(config.validation_dataset_path)\n",
        "            self.validation_dataloader = DataLoader(\n",
        "                self.eval_dataset, batch_size=config.batch_size\n",
        "            )\n",
        "\n",
        "        # initialize deepspeed\n",
        "        self.model_engine = None\n",
        "        if config.deepspeed_enable is True:\n",
        "            if config.deepspeed_config_path is None:\n",
        "                raise ValueError(\n",
        "                    \"DeepSpeed config path is None, but deepspeed is enabled\"\n",
        "                )\n",
        "            if os.path.exists(config.deepspeed_config_path) is False:\n",
        "                raise ValueError(\n",
        "                    f\"DeepSpeed config path {config.deepspeed_config_path}\"\n",
        "                    f\"does not exist\"\n",
        "                )\n",
        "            (\n",
        "                self.model_engine,\n",
        "                self.optimizer,\n",
        "                self.train_dataloader,\n",
        "                _,\n",
        "            ) = deepspeed.initialize(\n",
        "                args=None,\n",
        "                model=self.model,\n",
        "                model_parameters=self.model.parameters(),\n",
        "                training_data=self.train_dataloader,\n",
        "                config=self.config.deepspeed_config_path,\n",
        "            )\n",
        "\n",
        "    @beartype\n",
        "    def save_checkpoint(\n",
        "        self,\n",
        "        current_epoch: int,\n",
        "        current_step: int,\n",
        "        max_epochs: int,\n",
        "        max_steps: int,\n",
        "    ) -> None:\n",
        "\n",
        "        print(\n",
        "            f\"Saving checkpoint for epoch {current_epoch + 1}, \"\n",
        "            f\"step {current_step + 1} ...\"\n",
        "        )\n",
        "        model_folder, model_name, path = ModelLoader.get_model_path(\n",
        "            config=self.config,\n",
        "            is_checkpoint=True,\n",
        "            current_epoch=current_epoch,\n",
        "            current_step=current_step,\n",
        "            max_epochs=max_epochs,\n",
        "            max_steps=max_steps,\n",
        "        )\n",
        "        if os.path.exists(path):\n",
        "            os.remove(path)\n",
        "        torch.save(\n",
        "            {\n",
        "                \"state_dict\": self.model.state_dict(),\n",
        "                \"optim_state_dict\": self.optimizer.state_dict(),\n",
        "                \"training_stats\": self.training_stats,\n",
        "                \"epoch\": current_epoch,\n",
        "                \"step\": current_step,\n",
        "            },\n",
        "            path,\n",
        "        )\n",
        "\n",
        "    @beartype\n",
        "    def load_checkpoint(\n",
        "        self,\n",
        "    ) -> Tuple[int, int]:\n",
        "        \"\"\"Load a checkpoint from the model folder\"\"\"\n",
        "\n",
        "        print(\"Looking for checkpoints...\")\n",
        "        path = ModelLoader.check_model_path(\n",
        "            config=self.config,\n",
        "            is_checkpoint=True,\n",
        "            current_epoch=None,\n",
        "        )\n",
        "        if path is not None:\n",
        "            print(\"Loading ...\")\n",
        "            checkpoint = torch.load(path)\n",
        "            epoch = checkpoint[\"epoch\"]\n",
        "            self.model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "            self.optimizer.load_state_dict(checkpoint[\"optim_state_dict\"])\n",
        "            self.trainign_stats = checkpoint[\"training_stats\"]\n",
        "            step = checkpoint[\"step\"]\n",
        "            return epoch, step + 1  # return the next episode to train\n",
        "        return 0, 0\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "    ) -> None:\n",
        "        print(\"Start Actor Model Pretraining\")\n",
        "\n",
        "        # get config parameters\n",
        "        batch_size = self.config.batch_size\n",
        "        epochs = self.config.epochs\n",
        "        device = self.config.device\n",
        "        checkpoint_steps = self.config.checkpoint_steps\n",
        "\n",
        "        # compute the number of iterations\n",
        "        n_iter = int(len(self.train_dataset) / batch_size)\n",
        "\n",
        "        # load model_checkpoint\n",
        "        start_epoch, start_step = self.load_checkpoint()\n",
        "\n",
        "        # counter for the checkpoint\n",
        "        cnt_checkpoint = 1\n",
        "\n",
        "        # traing loop\n",
        "        for epoch in range(start_epoch, epochs):\n",
        "            self.model.train()\n",
        "            for i, input_output in enumerate(self.train_dataloader):\n",
        "                if i < start_step:\n",
        "                    continue\n",
        "                with torch.no_grad():\n",
        "                    input_output_tokenized = self.model.tokenizer(\n",
        "                        input_output,\n",
        "                        return_tensors=\"pt\",\n",
        "                        padding=True,\n",
        "                    )\n",
        "                    training_output = input_output_tokenized[\"input_ids\"][\n",
        "                        :, 1:\n",
        "                    ]\n",
        "                    training_input = input_output_tokenized[\"input_ids\"][\n",
        "                        :, :-1\n",
        "                    ]\n",
        "                    attention_mask = input_output_tokenized[\"attention_mask\"][\n",
        "                        :, :-1\n",
        "                    ]\n",
        "                    training_output = training_output.to(device)\n",
        "                    training_input = training_input.to(device)\n",
        "                    attention_mask = attention_mask.to(device)\n",
        "\n",
        "                # forward pass\n",
        "                if self.config.deepspeed_enable:\n",
        "                    est_output = self.model_engine(\n",
        "                        training_input, attention_mask\n",
        "                    )\n",
        "                else:\n",
        "                    est_output = self.model(training_input, attention_mask)\n",
        "                est_output = rearrange(est_output, \"b s v -> (b s) v\")\n",
        "                training_output = rearrange(training_output, \"b s -> (b s)\")\n",
        "                loss = self.loss_function(est_output, training_output)\n",
        "                self.training_stats.training_loss.append(loss.item())\n",
        "\n",
        "                # backward pass\n",
        "                if self.config.deepspeed_enable:\n",
        "                    self.model_engine.backward(loss)\n",
        "                    self.model_engine.step()\n",
        "                else:\n",
        "                    self.optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "\n",
        "                # print progress\n",
        "                if i % self.config.iteration_per_print == 0:\n",
        "                    print(\n",
        "                        f\"Epoch: {epoch+1}/{epochs}, \"\n",
        "                        f\"Iteration: {i+1}/{n_iter}, \"\n",
        "                        f\"Training Loss: {loss}\"\n",
        "                    )\n",
        "                # save checkpoint periodically\n",
        "                if cnt_checkpoint % checkpoint_steps == 0:\n",
        "                    self.save_checkpoint(epoch, i, epochs, n_iter)\n",
        "                    cnt_checkpoint = 1\n",
        "                else:\n",
        "                    cnt_checkpoint += 1\n",
        "\n",
        "            if self.validation_flag:\n",
        "                self.model.eval()\n",
        "                for i, input_output in enumerate(self.validation_dataloader):\n",
        "                    input_output_tokenized = self.model.tokenizer(\n",
        "                        input_output, return_tensors=\"pt\", padding=True\n",
        "                    )\n",
        "                    validation_output = input_output_tokenized[\"input_ids\"][\n",
        "                        :, 1:\n",
        "                    ]\n",
        "                    validation_input = input_output_tokenized[\"input_ids\"][\n",
        "                        :, :-1\n",
        "                    ]\n",
        "                    attention_mask = input_output_tokenized[\"attention_mask\"][\n",
        "                        :, :-1\n",
        "                    ]\n",
        "\n",
        "                    # forward pass\n",
        "                    est_output = self.model.forward(\n",
        "                        validation_input, attention_mask\n",
        "                    )\n",
        "                    validation_output = rearrange(\n",
        "                        validation_output, \"b s -> (b s)\"\n",
        "                    )\n",
        "                    est_output = rearrange(est_output, \"b s v -> (b s) v\")\n",
        "                    loss = self.loss_function(est_output, validation_output)\n",
        "                    self.training_stats.validation_loss.append(loss.item())\n",
        "                    # print progress\n",
        "                    if i % self.config.iteration_per_print == 0:\n",
        "                        print(\n",
        "                            f\"Epoch: {epoch+1}/{epochs}, \"\n",
        "                            f\"Iteration: {i+1}/{n_iter}, \"\n",
        "                            f\"Validation Loss: {loss}\"\n",
        "                        )\n",
        "        self.model.save()\n",
        "        print(\"Training Finished \")"
      ],
      "metadata": {
        "id": "5MreDvACfcCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_test_contrct(path, ratio): \n",
        "  col = ['Target']\n",
        "  f = open(path, \"r\")\n",
        "  file = f.read()\n",
        "  test_df = pd.DataFrame([file], columns=col)\n",
        "  program_length = len(test_df['Target'][0].split())\n",
        "  prompt_ratio = ratio\n",
        "  prompt_length = int(prompt_ratio * program_length)\n",
        "  return test_df, prompt_length\n",
        "\n",
        "def truncate_test(df, prompt_length):\n",
        "  copy_1 = df.copy(deep=True)\n",
        "  copy_2 = df.copy(deep=True)\n",
        "  #true\n",
        "  a = copy_1['Target'].str.split().str[-prompt_length:].apply(' '.join)[0]\n",
        "  #masked out program \n",
        "  b = copy_2['Target'].str.split().str[:-prompt_length].apply(' '.join)[0]\n",
        "  return a, b"
      ],
      "metadata": {
        "id": "_hKPs7PCcuHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df, n = generate_test_contrct('/content/drive/MyDrive/experiments/baseline benchmark/Replica.sol', 0.95)\n",
        "truth, prompt = truncate_test(test_df, n)\n",
        "col = ['Target']\n",
        "prompt_df = pd.DataFrame([prompt], columns=col)\n",
        "gpt_out = text_generation(trained_model, tokenizer, prompt_df)\n",
        "print('this is the GPT prompt without T5: \\n', prompt_df['Target'][0])\n",
        "print('this is the GPT prediction without T5: \\n', gpt_out)"
      ],
      "metadata": {
        "id": "z5vMNi2eHxkh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}