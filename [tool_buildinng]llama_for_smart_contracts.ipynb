{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM9Acc6K9SguLcjZMXUftvd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f3d973dd757e463795f1b0a83ec8ecde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a151a3498bfa47a99d08dd315fc0c4e9",
              "IPY_MODEL_cf61d750a5114a04ad6d07d4d1559ca2",
              "IPY_MODEL_3819433d2da64d1fbdb1c4db576dd2c2"
            ],
            "layout": "IPY_MODEL_0a29b26729ca4082a51b9f110f4183c8"
          }
        },
        "a151a3498bfa47a99d08dd315fc0c4e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3aa717a90254472ae3abde06f894b66",
            "placeholder": "​",
            "style": "IPY_MODEL_116b093405d749399ed06d753d4041cc",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "cf61d750a5114a04ad6d07d4d1559ca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40f3af28a11e456280b6f8a9e6f1096f",
            "max": 578,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d3b5c854dcd4e81ad6f0620c82c07de",
            "value": 578
          }
        },
        "3819433d2da64d1fbdb1c4db576dd2c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cfa3c2a929f4a6da941bc71527f3bdc",
            "placeholder": "​",
            "style": "IPY_MODEL_be546aa6e9ce4d27b7b854f89c283d15",
            "value": " 578/578 [00:00&lt;00:00, 26.4kB/s]"
          }
        },
        "0a29b26729ca4082a51b9f110f4183c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3aa717a90254472ae3abde06f894b66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "116b093405d749399ed06d753d4041cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40f3af28a11e456280b6f8a9e6f1096f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d3b5c854dcd4e81ad6f0620c82c07de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4cfa3c2a929f4a6da941bc71527f3bdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be546aa6e9ce4d27b7b854f89c283d15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba9e9f7dfe344af8877a6bd875b7ea21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_11508ba71e5a4d84ba653725bbad9de7",
              "IPY_MODEL_f7a18385f046427ea786a971f3acadfd",
              "IPY_MODEL_aa54f71ee53643f8b4a28a34d5a0488f"
            ],
            "layout": "IPY_MODEL_b545b3aa11f64b50b7bf4c3722e2530a"
          }
        },
        "11508ba71e5a4d84ba653725bbad9de7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e0fd0445500446ca92ccdb5c3767a82",
            "placeholder": "​",
            "style": "IPY_MODEL_a680d8059fc44274808392177cab26da",
            "value": "Downloading (…)model.bin.index.json: 100%"
          }
        },
        "f7a18385f046427ea786a971f3acadfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_768d1f82a14b49ac9c45580fb0cf4695",
            "max": 26788,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74ee1d01b91a4a1592751d2e9602a177",
            "value": 26788
          }
        },
        "aa54f71ee53643f8b4a28a34d5a0488f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6267013da78343169fdbf62dbcbabab7",
            "placeholder": "​",
            "style": "IPY_MODEL_5d15e89806f640d4800f20311109db6f",
            "value": " 26.8k/26.8k [00:00&lt;00:00, 121kB/s]"
          }
        },
        "b545b3aa11f64b50b7bf4c3722e2530a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e0fd0445500446ca92ccdb5c3767a82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a680d8059fc44274808392177cab26da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "768d1f82a14b49ac9c45580fb0cf4695": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74ee1d01b91a4a1592751d2e9602a177": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6267013da78343169fdbf62dbcbabab7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d15e89806f640d4800f20311109db6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "390a68d8fe06487f9ce6ca29c06a4c7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53d83cd65b2542bfbffe7ddc63169cbc",
              "IPY_MODEL_238a5f2bdb1a4c8aac476675c1bbf470",
              "IPY_MODEL_24e9cbcfdf4742358fa36b70045b412a"
            ],
            "layout": "IPY_MODEL_59b06f3fa4534266ad0d0131b163af78"
          }
        },
        "53d83cd65b2542bfbffe7ddc63169cbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a7ca10edd7f4ae6a5ee787205b24881",
            "placeholder": "​",
            "style": "IPY_MODEL_db94f19e98c74e6baee2f3ddb9030c44",
            "value": "Downloading shards:   0%"
          }
        },
        "238a5f2bdb1a4c8aac476675c1bbf470": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d165dd7d81db4281a22d892d6383041d",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15b141be597449698623cf6194e6e0ff",
            "value": 0
          }
        },
        "24e9cbcfdf4742358fa36b70045b412a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66bb76577d1a41d7ae84970130e8eb2c",
            "placeholder": "​",
            "style": "IPY_MODEL_9a9ac38ac26f4a2188803a0d00464eb2",
            "value": " 0/3 [00:39&lt;?, ?it/s]"
          }
        },
        "59b06f3fa4534266ad0d0131b163af78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a7ca10edd7f4ae6a5ee787205b24881": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db94f19e98c74e6baee2f3ddb9030c44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d165dd7d81db4281a22d892d6383041d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15b141be597449698623cf6194e6e0ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66bb76577d1a41d7ae84970130e8eb2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a9ac38ac26f4a2188803a0d00464eb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bee8b95ecd7456c830e27756d3c443f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3560787a81354d2d9fd956d8a402cbd3",
              "IPY_MODEL_8d7d8d4d08a54b7cb072d91cec420e81",
              "IPY_MODEL_a29e9c29063a468e9a7c8ceb05b4ccf0"
            ],
            "layout": "IPY_MODEL_b5295dab2a4c4f01ae790cace2d43c50"
          }
        },
        "3560787a81354d2d9fd956d8a402cbd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31b39bd727d94612b9bd2279c4285deb",
            "placeholder": "​",
            "style": "IPY_MODEL_df12af03cf604e288c9e9a01bf4dab9e",
            "value": "Downloading (…)l-00001-of-00003.bin:  37%"
          }
        },
        "8d7d8d4d08a54b7cb072d91cec420e81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d4b5bd694614911ab222edd1e397903",
            "max": 9877989586,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_86749fd492c542dca4c659f4e73e8e56",
            "value": 3638558720
          }
        },
        "a29e9c29063a468e9a7c8ceb05b4ccf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_836d5952e56e4d16b4662df0903bd61f",
            "placeholder": "​",
            "style": "IPY_MODEL_4a0e49b8cfd74513a7133bde9bb8db11",
            "value": " 3.64G/9.88G [00:49&lt;01:03, 98.1MB/s]"
          }
        },
        "b5295dab2a4c4f01ae790cace2d43c50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31b39bd727d94612b9bd2279c4285deb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df12af03cf604e288c9e9a01bf4dab9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d4b5bd694614911ab222edd1e397903": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86749fd492c542dca4c659f4e73e8e56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "836d5952e56e4d16b4662df0903bd61f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a0e49b8cfd74513a7133bde9bb8db11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sallywang147/SCInvarinfer/blob/main/%5Btool_buildinng%5Dllama_for_smart_contracts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lILBOi-WpPWw",
        "outputId": "331004c3-efc3-4c23-f0ae-de8ad38ad15a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install rich[jupyter]\n",
        "!pip install deepspeed\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KELJ4pTduNrW",
        "outputId": "2faf2081-f28a-4e84-89af-b74575a5fac8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-xhdtz_vy\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-xhdtz_vy\n",
            "  Resolved https://github.com/huggingface/transformers to commit 4e94c6c00847ddf809b18b962edcbb3dabeb202d\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (2.27.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (3.4)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.28.0.dev0-py3-none-any.whl size=6758861 sha256=8ce712082cd8766f632b40193feb5c49e09347c99513374ea3e887a200f47fbb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6wa3byp1/wheels/14/a0/7b/8f6b25ba4110aa215fcb8d6aedd6cd4f9b9b6619190999ac2b\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.28.0.dev0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rich[jupyter]\n",
            "  Downloading rich-13.3.2-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.7/238.7 KB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygments<3.0.0,>=2.13.0\n",
            "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown-it-py<3.0.0,>=2.2.0\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipywidgets<9,>=7.5.1 in /usr/local/lib/python3.9/dist-packages (from rich[jupyter]) (7.7.1)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (5.7.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (7.9.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (3.0.5)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (0.2.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (5.3.4)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (3.6.2)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.5.1->rich[jupyter]) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.5.1->rich[jupyter]) (6.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.2.0)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (63.4.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (2.0.10)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (4.4.2)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.9/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (6.5.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.8.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (5.7.3)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.8.0)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.5.6)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.17.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.5.3)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.16.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (5.3.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (23.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (3.1.2)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (21.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.9/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets<9,>=7.5.1->rich[jupyter]) (2.8.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.16.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.2.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect->ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.7.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (3.1.1)\n",
            "Requirement already satisfied: notebook-shim>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.2.2)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.9/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.23.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (2.1.2)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.2.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (4.9.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.2.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (6.0.0)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.7.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (4.11.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (23.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.7.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.9/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (2.16.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.9/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (4.3.3)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.9/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (21.2.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (22.2.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.19.3)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (3.6.2)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.9/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.5.1)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.15.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (2.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.5.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.9/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.9/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (3.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (2.21)\n",
            "Installing collected packages: pygments, mdurl, jedi, markdown-it-py, rich\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.6.1\n",
            "    Uninstalling Pygments-2.6.1:\n",
            "      Successfully uninstalled Pygments-2.6.1\n",
            "Successfully installed jedi-0.18.2 markdown-it-py-2.2.0 mdurl-0.1.2 pygments-2.14.0 rich-13.3.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pygments"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deepspeed\n",
            "  Downloading deepspeed-0.8.3.tar.gz (765 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m765.4/765.4 KB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hjson\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 KB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from deepspeed) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from deepspeed) (23.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from deepspeed) (5.9.4)\n",
            "Collecting py-cpuinfo\n",
            "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.9/dist-packages (from deepspeed) (1.10.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from deepspeed) (1.13.1+cu116)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from deepspeed) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic->deepspeed) (4.5.0)\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.8.3-py3-none-any.whl size=776422 sha256=02e999925bb4659ada5040d5d1dd5f5634987c2b186364d20d9da89032c44697\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/ea/8f/0768328ba436ed66f602d8d3b809624448c9eb627434176d04\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: py-cpuinfo, ninja, hjson, deepspeed\n",
            "Successfully installed deepspeed-0.8.3 hjson-3.1.0 ninja-1.11.1 py-cpuinfo-9.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.2-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.2 yarl-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Llama_Weights = '/content/drive/MyDrive/experiments/llama/LLaMA/7B/'\n",
        "Output = 'drive/MyDrive/experiments/HF_Llama'"
      ],
      "metadata": {
        "id": "2JW9JFKlv9US"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python /content/drive/MyDrive/experiments/llama/convert_llama_weights_to_hf.py \\\n",
        "#--input_dir '/content/drive/MyDrive/experiments/llama/LLaMA/7B' \\\n",
        "# --model_size 7B --output_dir 'drive/MyDrive/experiments/HF_Llama'"
      ],
      "metadata": {
        "id": "6K35upsguFjR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vanilla Llama##"
      ],
      "metadata": {
        "id": "7bYljYiYAYcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import AutoTokenizer, LlamaTokenizer, LlamaForCausalLM\n",
        "\n",
        "\n",
        "model = LlamaForCausalLM.from_pretrained('sallywww/Llama-7B')\n",
        "tokenizer = AutoTokenizer.from_pretrained('sallywww/Llama-7B')\n",
        "\n",
        "#model.push_to_hub('sallywww/Llama-7B')\n",
        "#tokenizer.push_to_hub('sallywww/Llama-7B')"
      ],
      "metadata": {
        "id": "D2oAIPSSxSNf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658,
          "referenced_widgets": [
            "f3d973dd757e463795f1b0a83ec8ecde",
            "a151a3498bfa47a99d08dd315fc0c4e9",
            "cf61d750a5114a04ad6d07d4d1559ca2",
            "3819433d2da64d1fbdb1c4db576dd2c2",
            "0a29b26729ca4082a51b9f110f4183c8",
            "b3aa717a90254472ae3abde06f894b66",
            "116b093405d749399ed06d753d4041cc",
            "40f3af28a11e456280b6f8a9e6f1096f",
            "7d3b5c854dcd4e81ad6f0620c82c07de",
            "4cfa3c2a929f4a6da941bc71527f3bdc",
            "be546aa6e9ce4d27b7b854f89c283d15",
            "ba9e9f7dfe344af8877a6bd875b7ea21",
            "11508ba71e5a4d84ba653725bbad9de7",
            "f7a18385f046427ea786a971f3acadfd",
            "aa54f71ee53643f8b4a28a34d5a0488f",
            "b545b3aa11f64b50b7bf4c3722e2530a",
            "1e0fd0445500446ca92ccdb5c3767a82",
            "a680d8059fc44274808392177cab26da",
            "768d1f82a14b49ac9c45580fb0cf4695",
            "74ee1d01b91a4a1592751d2e9602a177",
            "6267013da78343169fdbf62dbcbabab7",
            "5d15e89806f640d4800f20311109db6f",
            "390a68d8fe06487f9ce6ca29c06a4c7b",
            "53d83cd65b2542bfbffe7ddc63169cbc",
            "238a5f2bdb1a4c8aac476675c1bbf470",
            "24e9cbcfdf4742358fa36b70045b412a",
            "59b06f3fa4534266ad0d0131b163af78",
            "4a7ca10edd7f4ae6a5ee787205b24881",
            "db94f19e98c74e6baee2f3ddb9030c44",
            "d165dd7d81db4281a22d892d6383041d",
            "15b141be597449698623cf6194e6e0ff",
            "66bb76577d1a41d7ae84970130e8eb2c",
            "9a9ac38ac26f4a2188803a0d00464eb2",
            "2bee8b95ecd7456c830e27756d3c443f",
            "3560787a81354d2d9fd956d8a402cbd3",
            "8d7d8d4d08a54b7cb072d91cec420e81",
            "a29e9c29063a468e9a7c8ceb05b4ccf0",
            "b5295dab2a4c4f01ae790cace2d43c50",
            "31b39bd727d94612b9bd2279c4285deb",
            "df12af03cf604e288c9e9a01bf4dab9e",
            "8d4b5bd694614911ab222edd1e397903",
            "86749fd492c542dca4c659f4e73e8e56",
            "836d5952e56e4d16b4662df0903bd61f",
            "4a0e49b8cfd74513a7133bde9bb8db11"
          ]
        },
        "outputId": "a7b47ebd-2d57-40c9-ae8e-79dba4a8ace4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3d973dd757e463795f1b0a83ec8ecde"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba9e9f7dfe344af8877a6bd875b7ea21"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "390a68d8fe06487f9ce6ca29c06a4c7b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)l-00001-of-00003.bin:   0%|          | 0.00/9.88G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2bee8b95ecd7456c830e27756d3c443f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-560a03fa651b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLlamaForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sallywww/Llama-7B'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sallywww/Llama-7B'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_sharded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2404\u001b[0m             \u001b[0;31m# rsolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2405\u001b[0;31m             resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n\u001b[0m\u001b[1;32m   2406\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2407\u001b[0m                 \u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, use_auth_token, user_agent, revision, subfolder, _commit_hash)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0;31m# Load from URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             cached_filename = cached_file(\n\u001b[0m\u001b[1;32m    926\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m                 \u001b[0mshard_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1324\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"downloading %s to %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1326\u001b[0;31m             http_get(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                 \u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                 \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mtemp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m     \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/tempfile.py\u001b[0m in \u001b[0;36mfunc_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0m_functools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m             \u001b[0;31m# Avoid closing the file as long as the wrapper is alive,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m             \u001b[0;31m# see issue #18879.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert(False)\n",
        "prompt = \"a loop invariant in computer science is \"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate\n",
        "generate_ids = model.generate(inputs.input_ids, max_length=40)\n",
        "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n"
      ],
      "metadata": {
        "id": "SSQCT4xwDt2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##loading and json-ize data##"
      ],
      "metadata": {
        "id": "bdCPq8HpMeIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install simplejson"
      ],
      "metadata": {
        "id": "TBBRkS1uCl6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7a15492-df5c-41bd-e217-6a1be9c6ec31"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting simplejson\n",
            "  Downloading simplejson-3.18.4-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.8/136.8 KB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: simplejson\n",
            "Successfully installed simplejson-3.18.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "import simplejson as json\n",
        "from numpy import random\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import pandas\n",
        "import xlrd\n",
        "import json\n",
        "import xlrd\n",
        "\n",
        "#workbook = xlrd.open_workbook('/invariants_line_number.xlsx', on_demand = True)\n",
        "df = pandas.read_excel(open('/content/invariants_data.xlsx','rb'))\n",
        "df"
      ],
      "metadata": {
        "id": "XtHDzEL8Bvya",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "outputId": "d284d0b7-51d1-4526-c1ac-5af1fe194030"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Source  \\\n",
              "0    1 pragma solidity >=0.4.24 <0.6.0;\\n\\n2 contra...   \n",
              "1    1 // SPDX-License-Identifier: MIT\\n2 pragma ex...   \n",
              "2    1 pragma solidity >=0.4.24 <0.6.0;\\n\\n2 contra...   \n",
              "3    1 pragma solidity >=0.4.24 <0.6.0;\\n2 contract...   \n",
              "4    1 pragma solidity >=0.4.24 <0.6.0;\\n\\n2 // Thi...   \n",
              "..                                                 ...   \n",
              "98   1 pragma solidity >=0.4.24 <0.6.0;\\n2 \\n3 cont...   \n",
              "99   1  // SPDX-License-Identifier: MIT\\n2  pragma ...   \n",
              "100  1  // SPDX-License-Identifier: MIT\\n2  pragma ...   \n",
              "101  1 / SPDX-License-Identifier: MIT\\n2  pragma so...   \n",
              "102  1   // SPDX-License-Identifier: MIT\\n2 pragma ...   \n",
              "\n",
              "                                                Target  \n",
              "0                             14+ assert (y == x + 4);  \n",
              "1    109+  require(admin == msg.sender, \"Ownable: c...  \n",
              "2                              7+ assert (y == x + 2);  \n",
              "3    20+ assert(funcA2(funcA1())==12);\\n20+ assert(...  \n",
              "4    24+ assert(a == x + 1);\\n32+ assert(a == x);\\n...  \n",
              "..                                                 ...  \n",
              "98            6+ assert (!false);\\n6+ assert (!false);  \n",
              "99   6+  assert(fee + value != 0);\\n6+  assert(fee ...  \n",
              "100  15+ assert(_required > 0); \\n15+  assert(m_num...  \n",
              "101  6+  assert(address(this)==msg.sender);\\n6+  as...  \n",
              "102  30+ assert(msg.sender == _contractRegistry);\\n...  \n",
              "\n",
              "[103 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9be7cf3e-3c73-4a7b-82bb-58d810e6e8fc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1 pragma solidity &gt;=0.4.24 &lt;0.6.0;\\n\\n2 contra...</td>\n",
              "      <td>14+ assert (y == x + 4);</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1 // SPDX-License-Identifier: MIT\\n2 pragma ex...</td>\n",
              "      <td>109+  require(admin == msg.sender, \"Ownable: c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1 pragma solidity &gt;=0.4.24 &lt;0.6.0;\\n\\n2 contra...</td>\n",
              "      <td>7+ assert (y == x + 2);</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1 pragma solidity &gt;=0.4.24 &lt;0.6.0;\\n2 contract...</td>\n",
              "      <td>20+ assert(funcA2(funcA1())==12);\\n20+ assert(...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1 pragma solidity &gt;=0.4.24 &lt;0.6.0;\\n\\n2 // Thi...</td>\n",
              "      <td>24+ assert(a == x + 1);\\n32+ assert(a == x);\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>1 pragma solidity &gt;=0.4.24 &lt;0.6.0;\\n2 \\n3 cont...</td>\n",
              "      <td>6+ assert (!false);\\n6+ assert (!false);</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>1  // SPDX-License-Identifier: MIT\\n2  pragma ...</td>\n",
              "      <td>6+  assert(fee + value != 0);\\n6+  assert(fee ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>1  // SPDX-License-Identifier: MIT\\n2  pragma ...</td>\n",
              "      <td>15+ assert(_required &gt; 0); \\n15+  assert(m_num...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>1 / SPDX-License-Identifier: MIT\\n2  pragma so...</td>\n",
              "      <td>6+  assert(address(this)==msg.sender);\\n6+  as...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>1   // SPDX-License-Identifier: MIT\\n2 pragma ...</td>\n",
              "      <td>30+ assert(msg.sender == _contractRegistry);\\n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9be7cf3e-3c73-4a7b-82bb-58d810e6e8fc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9be7cf3e-3c73-4a7b-82bb-58d810e6e8fc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9be7cf3e-3c73-4a7b-82bb-58d810e6e8fc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _make_w_io_base(f, mode: str):\n",
        "    if not isinstance(f, io.IOBase):\n",
        "        f_dirname = os.path.dirname(f)\n",
        "        if f_dirname != \"\":\n",
        "            os.makedirs(f_dirname, exist_ok=True)\n",
        "        f = open(f, mode=mode)\n",
        "    return f\n",
        "\n",
        "\n",
        "def _make_r_io_base(f, mode: str):\n",
        "    if not isinstance(f, io.IOBase):\n",
        "        f = open(f, mode=mode)\n",
        "    return f\n",
        "\n",
        "\n",
        "def jdump(obj, f, mode=\"w\", indent=4, default=str):\n",
        "    \"\"\"Dump a str or dictionary to a file in json format.\n",
        "    Args:\n",
        "        obj: An object to be written.\n",
        "        f: A string path to the location on disk.\n",
        "        mode: Mode for opening the file.\n",
        "        indent: Indent for storing json dictionaries.\n",
        "        default: A function to handle non-serializable entries; defaults to `str`.\n",
        "    \"\"\"\n",
        "    f = _make_w_io_base(f, mode)\n",
        "    if isinstance(obj, (dict, list)):\n",
        "        json.dump(obj, f, indent=indent, default=default)\n",
        "    elif isinstance(obj, str):\n",
        "        f.write(obj)\n",
        "    else:\n",
        "        raise ValueError(f\"Unexpected type: {type(obj)}\")\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def jload(f, mode=\"r\"):\n",
        "    \"\"\"Load a .json file into a dictionary.\"\"\"\n",
        "    f = _make_r_io_base(f, mode)\n",
        "    jdict = json.load(f)\n",
        "    f.close()\n",
        "    return jdict\n"
      ],
      "metadata": {
        "id": "ykNHXMR4K0eB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List to hold dictionaries\n",
        "def prepare_actor_model_data(df): \n",
        "    data_list=[]\n",
        "    for index, row in df.iterrows():\n",
        "        data = OrderedDict()\n",
        "        data[\"user_input\"] = row['Source']\n",
        "        data[\"completion\"] = row['Target']\n",
        "        data_list.append(data)\n",
        "    jdump(data_list, 'datasets/actor_training_data.json')\n",
        "\n",
        "def prepare_rlhf_model_data(df): \n",
        "    data_list=[]\n",
        "    for index, row in df.iterrows():\n",
        "        data = OrderedDict()\n",
        "        data[\"user_input\"] = row['Source']\n",
        "        data_list.append(data)\n",
        "    jdump(data, '/datasets/reward_training_data.json')\n",
        "\n",
        "\n",
        "def prepare_reward_model_data(df):\n",
        "    data_list=[]    \n",
        "    for index, row in df.iterrows():\n",
        "        data = OrderedDict()\n",
        "        data[\"user_input\"] = row['Source']\n",
        "        data[\"completion\"] = row['Target']\n",
        "        data[\"score\"] = 5\n",
        "        data_list.append(data)\n",
        "    jdump(data, '/content/datasets/rlhf_training_data.json')\n"
      ],
      "metadata": {
        "id": "VKnXo46McPRf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_actor_model_data(df)\n",
        "prepare_rlhf_model_data(df)\n",
        "prepare_reward_model_data(df)"
      ],
      "metadata": {
        "id": "7tfm3a3ql0gZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "#!zip -r /content/datasets.zip /content/datasets\n",
        "#files.download(\"/content/datasets.zip\")"
      ],
      "metadata": {
        "id": "5jgix6Qwj6XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\""
      ],
      "metadata": {
        "id": "zlsyFBdE9SiS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(\n",
        "    sources: Sequence[str],\n",
        "    targets: Sequence[str],\n",
        "    tokenizer: tokenizer,\n",
        ") -> Dict:\n",
        "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
        "    examples = [s + t for s, t in zip(sources, targets)]\n",
        "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
        "    input_ids = examples_tokenized[\"input_ids\"]\n",
        "    labels = copy.deepcopy(input_ids)\n",
        "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "        label[:source_len] = IGNORE_INDEX\n",
        "    return dict(input_ids=input_ids, labels=labels)\n",
        "\n",
        "\n",
        "def _tokenize_fn(strings: Sequence[str], tokenizer: tokenizer) -> Dict:   \n",
        "    \"\"\"Tokenize a list of strings.\"\"\"\n",
        "    \n",
        "    tokenized_list = [\n",
        "        tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"longest\",\n",
        "            max_length=tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        for text in strings\n",
        "    ]\n",
        "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "    input_ids_lens = labels_lens = [\n",
        "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
        "    ]\n",
        "    return dict(\n",
        "        input_ids=input_ids,\n",
        "        labels=labels,\n",
        "        input_ids_lens=input_ids_lens,\n",
        "        labels_lens=labels_lens,\n",
        "    )"
      ],
      "metadata": {
        "id": "j4ZafBDGUss_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "3ae3affe-5a98-4d11-e559-6b07846823ad"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-d86bc65549cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m def preprocess(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msources\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ) -> Dict:\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Sequence' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if tokenizer.pad_token is None or tokenizer.eos_token:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))  \n",
        "print(tokenizer.pad_token)"
      ],
      "metadata": {
        "id": "w7VAcWvOYB8v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "dca907f1-cf43-4160-d203-658ddf3909f1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-47da7e73095f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'pad_token'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'[PAD]'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_token_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!google.colab --NotebookApp.iopub_data_rate_limit=10000000000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKwk0zPcJODL",
        "outputId": "2991883d-3a62-4ddc-ec54-39c4a4a0c5c1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: google.colab: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/datasets/actor_training_data.json', \"r\") as f:\n",
        "    data = json.load(f)\n",
        "    with open('/content/datasets/train_data.txt', \"a\") as file:\n",
        "      for d in data:\n",
        "      #  item = [str(d[\"user_input\"]) + \"\\n\" + str(d[\"completion\"]) +\"\\n\" + \"\\n\" for d in data]\n",
        "        file.write(str(d[\"user_input\"]) + \"\\n\" + str(d[\"completion\"]) +\"\\n\" + \"\\n\" + \"\\n\")\n",
        "    file.close()\n"
      ],
      "metadata": {
        "id": "UTOQV6s7NkO4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "#files.download(\"/content/datasets/train_data.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "MW26RG1oMH77",
        "outputId": "a9187473-72b6-4fc9-93cf-46c3fef084b6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ea7dc109-5940-4fce-a9e3-908edb783020\", \"train_data.txt\", 413216)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Finetuning builder## "
      ],
      "metadata": {
        "id": "Yea7vM37AxuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "!pip install peft\n",
        "!pip install --upgrade datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a-p1H07EdQd",
        "outputId": "534b4bd3-6285-4c4e-cc97-66cac503b2c1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gradio\n",
            "  Downloading gradio-3.23.0-py3-none-any.whl (15.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn\n",
            "  Downloading uvicorn-0.21.1-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.9/dist-packages (from gradio) (1.10.6)\n",
            "Collecting huggingface-hub>=0.13.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown-it-py[linkify]>=2.0.0\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi\n",
            "  Downloading fastapi-0.95.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 KB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from gradio) (3.7.1)\n",
            "Collecting orjson\n",
            "  Downloading orjson-3.8.8-cp39-cp39-manylinux_2_28_x86_64.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from gradio) (8.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from gradio) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from gradio) (2023.3.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.9/dist-packages (from gradio) (2.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from gradio) (6.0)\n",
            "Collecting websockets>=10.0\n",
            "  Downloading websockets-10.4-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from gradio) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from gradio) (1.22.4)\n",
            "Collecting mdit-py-plugins<=0.3.3\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (4.2.2)\n",
            "Collecting aiofiles\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting httpx\n",
            "  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 KB\u001b[0m \u001b[31m730.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from gradio) (1.4.4)\n",
            "Collecting semantic-version\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.13.0->gradio) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.13.0->gradio) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.13.0->gradio) (3.10.0)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Collecting linkify-it-py<3,>=1\n",
            "  Downloading linkify_it_py-2.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->gradio) (2022.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (2.0.12)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (22.2.0)\n",
            "Collecting starlette<0.27.0,>=0.26.1\n",
            "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting httpcore<0.17.0,>=0.15.0\n",
            "  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 KB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from httpx->gradio) (2022.12.7)\n",
            "Collecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (4.39.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (5.12.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (3.0.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (1.0.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->gradio) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->gradio) (1.26.15)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from uvicorn->gradio) (8.1.3)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting anyio<5.0,>=3.0\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->gradio) (3.15.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
            "Collecting uc-micro-py\n",
            "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->gradio) (1.16.0)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4707 sha256=6ba44c201894c05af3d77ee970d4886eb4989b67b015444dc60752678b3f3c02\n",
            "  Stored in directory: /root/.cache/pip/wheels/91/e2/96/f676aa08bfd789328c6576cd0f1fde4a3d686703bb0c247697\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: rfc3986, pydub, ffmpy, websockets, uc-micro-py, sniffio, semantic-version, python-multipart, orjson, multidict, mdurl, h11, frozenlist, async-timeout, aiofiles, yarl, uvicorn, markdown-it-py, linkify-it-py, huggingface-hub, anyio, aiosignal, starlette, mdit-py-plugins, httpcore, aiohttp, httpx, fastapi, gradio\n",
            "Successfully installed aiofiles-23.1.0 aiohttp-3.8.4 aiosignal-1.3.1 anyio-3.6.2 async-timeout-4.0.2 fastapi-0.95.0 ffmpy-0.3.0 frozenlist-1.3.3 gradio-3.23.0 h11-0.14.0 httpcore-0.16.3 httpx-0.23.3 huggingface-hub-0.13.3 linkify-it-py-2.0.0 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 mdurl-0.1.2 multidict-6.0.4 orjson-3.8.8 pydub-0.25.1 python-multipart-0.0.6 rfc3986-1.5.0 semantic-version-2.10.0 sniffio-1.3.0 starlette-0.26.1 uc-micro-py-1.0.1 uvicorn-0.21.1 websockets-10.4 yarl-1.8.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting peft\n",
            "  Downloading peft-0.2.0-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from peft) (1.22.4)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.2-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.17.1-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.8/212.8 KB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from peft) (23.0)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.9/dist-packages (from peft) (1.13.1+cu116)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from peft) (6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from peft) (5.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.13.0->peft) (4.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (3.10.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->peft) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->peft) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->peft) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->peft) (1.26.15)\n",
            "Installing collected packages: tokenizers, accelerate, transformers, peft\n",
            "Successfully installed accelerate-0.17.1 peft-0.2.0 tokenizers-0.13.2 transformers-4.27.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, responses, multiprocess, datasets\n",
            "Successfully installed datasets-2.10.1 dill-0.3.6 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "![[ -d /content/llama_trainer_inferer ]] \\\n",
        "  || git clone https://github.com/sallywang147/llama_trainer_inferer.git /content/llama_trainer_inferer\n",
        "!cd /content/llama_trainer_inferer && git pull && pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdQC24QIFCqe",
        "outputId": "6d3fae06-e302-4feb-92e5-6bf423c9604c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/llama_trainer_inferer'...\n",
            "remote: Enumerating objects: 71, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 71 (delta 41), reused 15 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (71/71), 1.54 MiB | 1.39 MiB/s, done.\n",
            "Already up to date.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/transformers.git (from -r requirements.txt (line 4))\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-411kmd85\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-411kmd85\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit aef488c503d119414bd8cfa9fd34c311f3e442b9\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/huggingface/peft.git (from -r requirements.txt (line 7))\n",
            "  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-4u4_j5tj\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-4u4_j5tj\n",
            "  Resolved https://github.com/huggingface/peft.git to commit d8c3b6bca49e4aa6e0498b416ed9adc50cc1a5fd\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 1)) (2.10.1)\n",
            "Collecting loralib\n",
            "  Downloading loralib-0.1.1-py3-none-any.whl (8.8 kB)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 5)) (0.17.1)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.37.2-py3-none-any.whl (84.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gradio in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 8)) (3.23.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets->-r requirements.txt (line 1)) (0.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets->-r requirements.txt (line 1)) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets->-r requirements.txt (line 1)) (4.65.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets->-r requirements.txt (line 1)) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets->-r requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets->-r requirements.txt (line 1)) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets->-r requirements.txt (line 1)) (9.0.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets->-r requirements.txt (line 1)) (2023.3.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets->-r requirements.txt (line 1)) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from datasets->-r requirements.txt (line 1)) (0.13.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets->-r requirements.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets->-r requirements.txt (line 1)) (1.4.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets->-r requirements.txt (line 1)) (0.70.14)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0->-r requirements.txt (line 4)) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0->-r requirements.txt (line 4)) (3.10.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0->-r requirements.txt (line 4)) (2022.10.31)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate->-r requirements.txt (line 5)) (5.9.4)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from accelerate->-r requirements.txt (line 5)) (1.13.1+cu116)\n",
            "Requirement already satisfied: semantic-version in /usr/local/lib/python3.9/dist-packages (from gradio->-r requirements.txt (line 8)) (2.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from gradio->-r requirements.txt (line 8)) (4.5.0)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from gradio->-r requirements.txt (line 8)) (2.2.0)\n",
            "Requirement already satisfied: websockets>=10.0 in /usr/local/lib/python3.9/dist-packages (from gradio->-r requirements.txt (line 8)) (10.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.9/dist-packages (from gradio->-r requirements.txt (line 8)) (0.25.1)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.9/dist-packages (from gradio->-r requirements.txt (line 8)) (0.21.1)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.9/dist-packages (from gradio->-r requirements.txt (line 8)) (0.23.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from gradio->-r requirements.txt (line 8)) (3.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from gradio->-r requirements.txt (line 8)) (3.1.2)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from gradio->-r requirements.txt (line 8)) (4.2.2)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.9/dist-packages (from gradio->-r requirements.txt (line 8)) (23.1.0)\n",
            "Requirement already satisfied: mdit-py-plugins<=0.3.3 in /usr/local/lib/python3.9/dist-packages (from gradio->-r requirements.txt (line 8)) (0.3.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.9/dist-packages (from gradio->-r requirements.txt (line 8)) (1.10.6)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.9/dist-packages (from gradio->-r requirements.txt (line 8)) (0.0.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from gradio->-r requirements.txt (line 8)) (8.4.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.9/dist-packages (from gradio->-r requirements.txt (line 8)) (0.3.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.9/dist-packages (from gradio->-r requirements.txt (line 8)) (2.1.2)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.9/dist-packages (from gradio->-r requirements.txt (line 8)) (3.8.8)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.9/dist-packages (from gradio->-r requirements.txt (line 8)) (0.95.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 8)) (4.3.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 8)) (0.4)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 8)) (0.12.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (2.0.12)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (4.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio->-r requirements.txt (line 8)) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio->-r requirements.txt (line 8)) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->-r requirements.txt (line 1)) (2022.7.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (1.26.15)\n",
            "Requirement already satisfied: starlette<0.27.0,>=0.26.1 in /usr/local/lib/python3.9/dist-packages (from fastapi->gradio->-r requirements.txt (line 8)) (0.26.1)\n",
            "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /usr/local/lib/python3.9/dist-packages (from httpx->gradio->-r requirements.txt (line 8)) (0.16.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.9/dist-packages (from httpx->gradio->-r requirements.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /usr/local/lib/python3.9/dist-packages (from httpx->gradio->-r requirements.txt (line 8)) (1.5.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio->-r requirements.txt (line 8)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio->-r requirements.txt (line 8)) (1.4.4)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio->-r requirements.txt (line 8)) (5.12.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio->-r requirements.txt (line 8)) (1.0.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio->-r requirements.txt (line 8)) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio->-r requirements.txt (line 8)) (4.39.2)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.9/dist-packages (from uvicorn->gradio->-r requirements.txt (line 8)) (0.14.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from uvicorn->gradio->-r requirements.txt (line 8)) (8.1.3)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.9/dist-packages (from httpcore<0.17.0,>=0.15.0->httpx->gradio->-r requirements.txt (line 8)) (3.6.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->gradio->-r requirements.txt (line 8)) (3.15.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->-r requirements.txt (line 8)) (0.19.3)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.9/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio->-r requirements.txt (line 8)) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->-r requirements.txt (line 1)) (1.16.0)\n",
            "Building wheels for collected packages: transformers, peft\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.28.0.dev0-py3-none-any.whl size=6790719 sha256=eb10f29bc6c2f259ee8bf93a2b9275f385dd43622de6d53c7cb9e9fb5b23b04a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3htvpkow/wheels/f7/92/8c/752ff3bfcd3439805d8bbf641614da38ef3226e127ebea86ee\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for peft: filename=peft-0.3.0.dev0-py3-none-any.whl size=40706 sha256=e8b224a83da1ced7c1493bd3fb129d35178675c1d7aee8b6a375688374801f53\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3htvpkow/wheels/2d/60/1b/0edd9dc0f0c489738b1166bc1b0b560ee368f7721f89d06e3a\n",
            "Successfully built transformers peft\n",
            "Installing collected packages: sentencepiece, bitsandbytes, loralib, transformers, peft\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.27.2\n",
            "    Uninstalling transformers-4.27.2:\n",
            "      Successfully uninstalled transformers-4.27.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama_trainer_inferer && python main.py --share"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itQGznMQFHXQ",
        "outputId": "6677d7de-c8fb-483a-f8a8-03f878d5c469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/lib64-nvidia did not contain libcudart.so as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-a100-s-3r5sxbyg2o7et --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Running on public URL: https://24bb4e621f877f44cd.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import random\n",
        "import torch\n",
        "import gradio as gr\n",
        "import transformers\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer, GenerationConfig\n",
        "from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model, PeftModel\n",
        "\n",
        "model = None\n",
        "tokenizer = None\n",
        "peft_model = None\n",
        "\n",
        "def random_hyphenated_word():\n",
        "    word_list = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig']\n",
        "    word1 = random.choice(word_list)\n",
        "    word2 = random.choice(word_list)\n",
        "    return word1 + '-' + word2\n",
        "\n",
        "def maybe_load_models():\n",
        "    global model\n",
        "    global tokenizer\n",
        "\n",
        "    if model is None:\n",
        "        model = LlamaForCausalLM.from_pretrained(\n",
        "            \"decapoda-research/llama-7b-hf\",\n",
        "            load_in_8bit=True,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "\n",
        "    if tokenizer is None:\n",
        "        tokenizer = LlamaTokenizer.from_pretrained(\n",
        "            \"decapoda-research/llama-7b-hf\",\n",
        "        )\n",
        "\n",
        "def reset_models():\n",
        "    global model\n",
        "    global tokenizer\n",
        "\n",
        "    del model\n",
        "    del tokenizer\n",
        "\n",
        "    model = None\n",
        "    tokenizer = None\n",
        "\n",
        "def generate_text(\n",
        "    model_name, \n",
        "    text, \n",
        "    temperature, \n",
        "    top_p, \n",
        "    top_k, \n",
        "    repeat_penalty,\n",
        "    max_new_tokens,\n",
        "    progress=gr.Progress(track_tqdm=True)\n",
        "):\n",
        "    global model\n",
        "    global tokenizer\n",
        "\n",
        "    maybe_load_models()\n",
        "\n",
        "    tokenizer.pad_token_id = 0\n",
        "\n",
        "    if model_name and model_name != \"None\":\n",
        "        model = PeftModel.from_pretrained(\n",
        "            model, model_name,\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "\n",
        "    # llama_config = transformers.LlamaConfig()\n",
        "    # print(llama_config)\n",
        "\n",
        "    stopping_criteria_list = transformers.StoppingCriteriaList()\n",
        "    generation_config = GenerationConfig(\n",
        "        # Whether to use greedy decoding. If set to False,\n",
        "        do_sample=True,\n",
        "\n",
        "        # Controls the 'temperature' of the softmax distribution during sampling.\n",
        "        # Higher values (e.g., 1.0) make the model generate more diverse and random outputs, \n",
        "        # while lower values (e.g., 0.1) make it more deterministic and \n",
        "        # focused on the highest probability tokens.\n",
        "        temperature=temperature,  \n",
        "\n",
        "        # Sets the nucleus sampling threshold. In nucleus sampling, \n",
        "        # only the tokens whose cumulative probability exceeds 'top_p' are considered \n",
        "        # for sampling. This technique helps to reduce the number of low probability \n",
        "        # tokens considered during sampling, which can lead to more diverse and coherent outputs.\n",
        "        top_p=top_p,  \n",
        "\n",
        "        # Sets the number of top tokens to consider during sampling. \n",
        "        # In top-k sampling, only the 'top_k' tokens with the highest probabilities \n",
        "        # are considered for sampling. This method can lead to more focused and coherent \n",
        "        # outputs by reducing the impact of low probability tokens.\n",
        "        top_k=top_k,  \n",
        "\n",
        "        # Applies a penalty to the probability of tokens that have already been generated, \n",
        "        # discouraging the model from repeating the same words or phrases. The penalty is\n",
        "        # applied by dividing the token probability by a factor based on the number of times \n",
        "        # the token has appeared in the generated text.\n",
        "        repeat_penalty=repeat_penalty,\n",
        "\n",
        "        # Limits the maximum number of tokens generated in a single iteration. \n",
        "        # This can be useful to control the length of generated text, especially in tasks \n",
        "        # like text summarization or translation, where the output should not be excessively long.\n",
        "        max_new_tokens=max_new_tokens,  \n",
        "\n",
        "        # typical_p=1,\n",
        "        # stopping_criteria=stopping_criteria_list,\n",
        "        # eos_token_id=llama_config.eos_token_id,\n",
        "        # pad_token_id=llama_config.eos_token_id\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generation_output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=torch.ones_like(input_ids),\n",
        "            generation_config=generation_config,\n",
        "            # return_dict_in_generate=True,\n",
        "            # output_scores=True,\n",
        "            # eos_token_id=[tokenizer.eos_token_id],\n",
        "            use_cache=True,\n",
        "        )[0].cuda()\n",
        "\n",
        "    output_text = tokenizer.decode(generation_output)\n",
        "    return output_text.strip()\n",
        "\n",
        "def tokenize_and_train(\n",
        "    training_text,\n",
        "    max_seq_length,\n",
        "    micro_batch_size,\n",
        "    gradient_accumulation_steps,\n",
        "    epochs,\n",
        "    learning_rate,\n",
        "    lora_r,\n",
        "    lora_alpha,\n",
        "    lora_dropout,\n",
        "    model_name,\n",
        "    progress=gr.Progress(track_tqdm=True)\n",
        "):\n",
        "    global model\n",
        "    global tokenizer\n",
        "\n",
        "    reset_models()\n",
        "    maybe_load_models()\n",
        "\n",
        "    tokenizer.pad_token_id = 0\n",
        "\n",
        "    paragraphs = training_text.split(\"\\n\\n\\n\")\n",
        "    print(\"Number of samples: \" + str(len(paragraphs)))\n",
        "        \n",
        "    def tokenize(item):\n",
        "        result = tokenizer(\n",
        "            item[\"text\"],\n",
        "            truncation=True,\n",
        "            max_length=max_seq_length,\n",
        "            padding=\"max_length\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": result[\"input_ids\"][:-1],\n",
        "            \"attention_mask\": result[\"attention_mask\"][:-1],\n",
        "        }\n",
        "\n",
        "    def to_dict(text):\n",
        "        return {\"text\": text}\n",
        "\n",
        "    paragraphs = [to_dict(x) for x in paragraphs]\n",
        "    data = Dataset.from_list(paragraphs)\n",
        "    data = data.shuffle().map(lambda x: tokenize(x))\n",
        "\n",
        "    model = prepare_model_for_int8_training(model)\n",
        "\n",
        "    model = get_peft_model(model, LoraConfig(\n",
        "        r=lora_r,\n",
        "        lora_alpha=lora_alpha,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        lora_dropout=lora_dropout,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    ))\n",
        "\n",
        "    output_dir = f\"lora-{model_name}\"\n",
        "\n",
        "    print(\"Training...\")\n",
        "\n",
        "    training_args = transformers.TrainingArguments(\n",
        "        # Set the batch size for training on each device (GPU, CPU, or TPU).\n",
        "        per_device_train_batch_size=micro_batch_size, \n",
        "\n",
        "        # Number of steps for gradient accumulation. This is useful when the total \n",
        "        # batch size is too large to fit in GPU memory. The effective batch size \n",
        "        # will be the product of 'per_device_train_batch_size' and 'gradient_accumulation_steps'.\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,  \n",
        "\n",
        "        # Number of warmup steps for the learning rate scheduler. During these steps, \n",
        "        # the learning rate increases linearly from 0 to its initial value. Warmup helps\n",
        "        #  to reduce the risk of very large gradients at the beginning of training, \n",
        "        # which could destabilize the model.\n",
        "        # warmup_steps=100, \n",
        "\n",
        "        # The total number of training steps. The training process will end once this \n",
        "        # number is reached, even if not all the training epochs are completed.\n",
        "        # max_steps=1500, \n",
        "\n",
        "        # The total number of epochs (complete passes through the training data) \n",
        "        # to perform during the training process.\n",
        "        num_train_epochs=epochs,  \n",
        "\n",
        "        # The initial learning rate to be used during training.\n",
        "        learning_rate=learning_rate, \n",
        "\n",
        "        # Enables mixed precision training using 16-bit floating point numbers (FP16). \n",
        "        # This can speed up training and reduce GPU memory consumption without \n",
        "        # sacrificing too much model accuracy.\n",
        "        fp16=True,  \n",
        "\n",
        "        # The frequency (in terms of steps) of logging training metrics and statistics \n",
        "        # like loss, learning rate, etc. In this case, it logs after every 20 steps.\n",
        "        logging_steps=20, \n",
        "\n",
        "        # The output directory where the trained model, checkpoints, \n",
        "        # and other training artifacts will be saved.\n",
        "        output_dir=output_dir, \n",
        "\n",
        "        # The maximum number of checkpoints to keep. When this limit is reached, \n",
        "        # the oldest checkpoint will be deleted to save a new one. In this case, \n",
        "        # a maximum of 3 checkpoints will be kept.\n",
        "        save_total_limit=3,  \n",
        "    )\n",
        "\n",
        "\n",
        "    trainer = transformers.Trainer(\n",
        "        # The pre-trained model that you want to fine-tune or train from scratch. \n",
        "        # 'model' should be an instance of a Hugging Face Transformer model, such as BERT, GPT-2, T5, etc.\n",
        "        model=model, \n",
        "\n",
        "        # The dataset to be used for training. 'data' should be a PyTorch Dataset or \n",
        "        # a compatible format, containing the input samples and labels or masks (if required).\n",
        "        train_dataset=data, \n",
        "\n",
        "        # The TrainingArguments instance created earlier, which contains various \n",
        "        # hyperparameters and configurations for the training process.\n",
        "        args=training_args, \n",
        "\n",
        "        # A callable that takes a batch of samples and returns a batch of inputs for the model. \n",
        "        # This is used to prepare the input samples for training by batching, padding, and possibly masking.\n",
        "        data_collator=transformers.DataCollatorForLanguageModeling( \n",
        "            tokenizer,  \n",
        "            # Whether to use masked language modeling (MLM) during training. \n",
        "            # MLM is a training technique used in models like BERT, where some tokens in the \n",
        "            # input are replaced by a mask token, and the model tries to predict the \n",
        "            # original tokens. In this case, MLM is set to False, indicating that it will not be used.\n",
        "            mlm=False, \n",
        "        ),\n",
        "    )\n",
        "\n",
        "    result = trainer.train(resume_from_checkpoint=False)\n",
        "\n",
        "    model.save_pretrained(output_dir)\n",
        "    \n",
        "    reset_models()\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "with gr.Blocks(\n",
        "    css=\"#refresh-button { max-width: 32px }\", \n",
        "    title=\"Simple LLaMA Finetuner\") as demo:\n",
        "    \n",
        "    with gr.Tab(\"Finetuning\"):\n",
        "\n",
        "        with gr.Column():\n",
        "            training_text = gr.Textbox(lines=12, label=\"Training Data\", info=\"Each sequence must be separated by a double newline\")\n",
        "\n",
        "            max_seq_length = gr.Slider(\n",
        "                minimum=1, maximum=4096, value=512,\n",
        "                label=\"Max Sequence Length\", \n",
        "                info=\"The maximum length of each sample text sequence. Sequences longer than this will be truncated.\"\n",
        "            )\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                micro_batch_size = gr.Slider(\n",
        "                    minimum=1, maximum=100, value=1, \n",
        "                    label=\"Micro Batch Size\", \n",
        "                    info=\"The number of examples in each mini-batch for gradient computation. A smaller micro_batch_size reduces memory usage but may increase training time.\"\n",
        "                )\n",
        "\n",
        "                gradient_accumulation_steps = gr.Slider(\n",
        "                    minimum=1, maximum=10, value=1, \n",
        "                    label=\"Gradient Accumulation Steps\", \n",
        "                    info=\"The number of steps to accumulate gradients before updating model parameters. This can be used to simulate a larger effective batch size without increasing memory usage.\"\n",
        "                )\n",
        "\n",
        "                epochs = gr.Slider(\n",
        "                    minimum=1, maximum=100, value=1, \n",
        "                    label=\"Epochs\",\n",
        "                    info=\"The number of times to iterate over the entire training dataset. A larger number of epochs may improve model performance but also increase the risk of overfitting.\")\n",
        "\n",
        "                learning_rate = gr.Slider(\n",
        "                    minimum=0.00001, maximum=0.01, value=3e-4,\n",
        "                    label=\"Learning Rate\",\n",
        "                    info=\"The initial learning rate for the optimizer. A higher learning rate may speed up convergence but also cause instability or divergence. A lower learning rate may require more steps to reach optimal performance but also avoid overshooting or oscillating around local minima.\"\n",
        "                )\n",
        "\n",
        "            with gr.Column():\n",
        "                lora_r = gr.Slider(\n",
        "                    minimum=1, maximum=16, value=8, \n",
        "                    label=\"LoRA R\",\n",
        "                    info=\"The rank parameter for LoRA, which controls the dimensionality of the rank decomposition matrices. A larger lora_r increases the expressiveness and flexibility of LoRA but also increases the number of trainable parameters and memory usage.\"\n",
        "                )\n",
        "\n",
        "                lora_alpha = gr.Slider(\n",
        "                    minimum=1, maximum=128, value=16, \n",
        "                    label=\"LoRA Alpha\",\n",
        "                    info=\"The scaling parameter for LoRA, which controls how much LoRA affects the original pre-trained model weights. A larger lora_alpha amplifies the impact of LoRA but may also distort or override the pre-trained knowledge.\"\n",
        "                )\n",
        "                \n",
        "                lora_dropout = gr.Slider(\n",
        "                    minimum=0, maximum=1, value=0.01,\n",
        "                    label=\"LoRA Dropout\",\n",
        "                    info=\"The dropout probability for LoRA, which controls the fraction of LoRA parameters that are set to zero during training. A larger lora_dropout increases the regularization effect of LoRA but also increases the risk of underfitting.\"\n",
        "                )\n",
        "\n",
        "                with gr.Column():\n",
        "                    model_name = gr.Textbox(\n",
        "                        lines=1, label=\"LoRA Model Name\", value=random_hyphenated_word()\n",
        "                    )\n",
        "\n",
        "                    with gr.Row():\n",
        "                        train_btn = gr.Button(\n",
        "                            \"Train\", variant=\"primary\", label=\"Train\", \n",
        "                        )\n",
        "\n",
        "                        abort_button = gr.Button(\n",
        "                            \"Abort\", label=\"Abort\", \n",
        "                        )\n",
        "    \n",
        "        output_text = gr.Text(\"Training Status\")\n",
        "\n",
        "        train_progress = train_btn.click(\n",
        "            fn=tokenize_and_train,\n",
        "            inputs=[\n",
        "                training_text,\n",
        "                max_seq_length,\n",
        "                micro_batch_size,\n",
        "                gradient_accumulation_steps,\n",
        "                epochs,\n",
        "                learning_rate,\n",
        "                lora_r,\n",
        "                lora_alpha,\n",
        "                lora_dropout,\n",
        "                model_name\n",
        "            ],\n",
        "            outputs=output_text\n",
        "        )\n",
        "\n",
        "        abort_button.click(None, None, None, cancels=[train_progress])\n",
        "\n",
        "    with gr.Tab(\"Inference\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                with gr.Row():\n",
        "                        lora_model = gr.Dropdown(\n",
        "                            label=\"LoRA Model\",\n",
        "                        )\n",
        "                        refresh_models_list = gr.Button(\n",
        "                            \"Reload Models\",\n",
        "                            elem_id=\"refresh-button\"\n",
        "                        )\n",
        "                inference_text = gr.Textbox(lines=7, label=\"Input Text\")   \n",
        "            inference_output = gr.Textbox(lines=12, label=\"Output Text\")\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                #  temperature, top_p, top_k, repeat_penalty, max_new_tokens\n",
        "                temperature = gr.Slider(\n",
        "                    minimum=0, maximum=1.99, value=0.7, step=0.01,\n",
        "                    label=\"Temperature\",\n",
        "                    info=\"Controls the 'temperature' of the softmax distribution during sampling. Higher values (e.g., 1.0) make the model generate more diverse and random outputs, while lower values (e.g., 0.1) make it more deterministic and focused on the highest probability tokens.\"\n",
        "                )\n",
        "\n",
        "                top_p = gr.Slider(\n",
        "                    minimum=0, maximum=1, value=0.2, step=0.01,\n",
        "                    label=\"Top P\",\n",
        "                    info=\"Sets the nucleus sampling threshold. In nucleus sampling, only the tokens whose cumulative probability exceeds 'top_p' are considered  for sampling. This technique helps to reduce the number of low probability tokens considered during sampling, which can lead to more diverse and coherent outputs.\"\n",
        "                )\n",
        "\n",
        "                top_k = gr.Slider(\n",
        "                    minimum=0, maximum=200, value=50, step=1,\n",
        "                    label=\"Top K\",\n",
        "                    info=\"Sets the number of top tokens to consider during sampling. In top-k sampling, only the 'top_k' tokens with the highest probabilities are considered for sampling. This method can lead to more focused and coherent outputs by reducing the impact of low probability tokens.\"\n",
        "                )\n",
        "\n",
        "                repeat_penalty = gr.Slider(\n",
        "                    minimum=0, maximum=1.5, value=0.8, step=0.01,\n",
        "                    label=\"Repeat Penalty\",\n",
        "                    info=\"Applies a penalty to the probability of tokens that have already been generated, discouraging the model from repeating the same words or phrases. The penalty is applied by dividing the token probability by a factor based on the number of times the token has appeared in the generated text.\"\n",
        "                )\n",
        "\n",
        "                max_new_tokens = gr.Slider(\n",
        "                    minimum=0, maximum=4096, value=50, step=1,\n",
        "                    label=\"Max New Tokens\",\n",
        "                    info=\"Limits the maximum number of tokens generated in a single iteration.\"\n",
        "                )\n",
        "            with gr.Column():\n",
        "                with gr.Row():\n",
        "                    generate_btn = gr.Button(\n",
        "                        \"Generate\", variant=\"primary\", label=\"Generate\", \n",
        "                    )\n",
        "\n",
        "                    inference_abort_button = gr.Button(\n",
        "                        \"Abort\", label=\"Abort\", \n",
        "                    )\n",
        "            \n",
        "        inference_progress = generate_btn.click(\n",
        "            fn=generate_text,\n",
        "            inputs=[\n",
        "                lora_model,\n",
        "                inference_text,\n",
        "                temperature,\n",
        "                top_p,\n",
        "                top_k,\n",
        "                repeat_penalty,\n",
        "                max_new_tokens\n",
        "            ],\n",
        "            outputs=inference_output,\n",
        "        )\n",
        "\n",
        "        lora_model.change(\n",
        "            fn=reset_models\n",
        "        )\n",
        "\n",
        "        def update_models_list():\n",
        "            return gr.Dropdown.update(choices=[\"None\"] + [\n",
        "                d for d in os.listdir() if os.path.isdir(d) and d.startswith('lora-')\n",
        "            ], value=\"None\")\n",
        "\n",
        "        refresh_models_list.click(\n",
        "            update_models_list,  \n",
        "            inputs=None, \n",
        "            outputs=lora_model,\n",
        "        )\n",
        "\n",
        "    \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Simple LLaMA Finetuner\")\n",
        "    parser.add_argument(\"-s\", \"--share\", action=\"store_true\", help=\"Enable sharing of the Gradio interface\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    demo.queue().launch(share=args.share)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "aHShhrilEVte",
        "outputId": "1500061c-4365-49f6-a383-b753a68ff6ae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-79547b1c7651>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgradio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gradio'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path ='/content/datasets/actor_training_data.json'"
      ],
      "metadata": {
        "id": "0BcuoMN95gxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import logging\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, Dict, Sequence\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import Trainer\n",
        "import dataclasses\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import io\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "from typing import Optional, Sequence, Union\n",
        "\n",
        "import openai\n",
        "import tqdm\n",
        "from openai import openai_object\n",
        "import copy\n",
        "\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "\n",
        "def _make_r_io_base(f, mode: str):\n",
        "    if not isinstance(f, io.IOBase):\n",
        "        f = open(f, mode=mode)\n",
        "    return f\n",
        "\n",
        "def jload(f, mode=\"r\"):\n",
        "    \"\"\"Load a .json file into a dictionary.\"\"\"\n",
        "    f = _make_r_io_base(f, mode)\n",
        "    jdict = json.load(f)\n",
        "    f.close()\n",
        "    return jdict\n",
        "\n",
        "def smart_tokenizer_and_embedding_resize(\n",
        "    special_tokens_dict: Dict,\n",
        "    tokenizer: tokenizer,\n",
        "    model: model,\n",
        "):\n",
        "    \"\"\"Resize tokenizer and embedding.\n",
        "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
        "    \"\"\"\n",
        "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if num_new_tokens > 0:\n",
        "        input_embeddings = model.get_input_embeddings().weight.data\n",
        "        output_embeddings = model.get_output_embeddings().weight.data\n",
        "\n",
        "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "\n",
        "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
        "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
        "\n",
        "\n",
        "def _tokenize_fn(strings: Sequence[str], tokenizer: tokenizer) -> Dict:\n",
        "    \"\"\"Tokenize a list of strings.\"\"\"\n",
        "    tokenized_list = [\n",
        "        tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"longest\",\n",
        "            max_length=tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        for text in strings\n",
        "    ]\n",
        "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "    input_ids_lens = labels_lens = [\n",
        "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
        "    ]\n",
        "    return dict(\n",
        "        input_ids=input_ids,\n",
        "        labels=labels,\n",
        "        input_ids_lens=input_ids_lens,\n",
        "        labels_lens=labels_lens,\n",
        "    )\n",
        "\n",
        "\n",
        "def preprocess(\n",
        "    sources: Sequence[str],\n",
        "    targets: Sequence[str],\n",
        "    tokenizer: tokenizer,\n",
        ") -> Dict:\n",
        "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
        "    examples = [s + t for s, t in zip(sources, targets)]\n",
        "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
        "    input_ids = examples_tokenized[\"input_ids\"]\n",
        "    labels = copy.deepcopy(input_ids)\n",
        "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "        label[:source_len] = IGNORE_INDEX\n",
        "    return dict(input_ids=input_ids, labels=labels)\n",
        "\n",
        "\n",
        "class SupervisedDataset(Dataset):\n",
        "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
        "    def __init__(self, data_path: str, tokenizer: tokenizer) -> None:\n",
        "        logging.warning(\"Loading data...\")\n",
        "        self.data_path = data_path\n",
        "        with open(data_path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "            self.data =  [str(d[\"user_input\"]) + \"\\n\" + str(d[\"completion\"]) for d in data]\n",
        "        self.len = len(self.data)\n",
        "        sources = [f\"{example['user_input']}{tokenizer.eos_token}\" for example in data]\n",
        "        targets = [f\"{example['completion']}{tokenizer.eos_token}\" for example in data]\n",
        "        data_dict = preprocess(sources, targets, tokenizer)\n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "    \n",
        "    def __len__(self):\n",
        "        #print(f\"comparing length with chatlllama definition: chatllama: {self.data[idx]} v.s. stanford version{self.input_ids}\")\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
        "\n",
        "\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
        "    def __init__(self, tokenizer: tokenizer) -> None:\n",
        "        logging.warning(\"data collator for supervised learning...\")\n",
        "        self.tokenizer=tokenizer\n",
        "    \n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )\n",
        "\n",
        "def make_supervised_data_module(tokenizer: tokenizer, data_path) -> Dict:\n",
        "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
        "    train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_path)\n",
        "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
        "    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)"
      ],
      "metadata": {
        "id": "aahblvHVp2c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\"working_dir\")\n",
        "training_args = training_args.set_lr_scheduler(name=\"cosine\", warmup_ratio=0.05)\n",
        "training_args.fp16 = True\n",
        "training_args.auto_find_batch_size=False\n",
        "training_args.per_device_train_batch_size=1\n",
        "training_args.per_device_train_batch_size=1\n",
        "training_args.output_dir='./output'\n",
        "training_args.num_train_epochs = 3 \n",
        "training_args.gradient_accumulation_steps = 8 \n",
        "training_args.evaluation_strategy=\"no\" \n",
        "training_args.save_strategy=\"steps\" \n",
        "training_args.save_steps=2000 \n",
        "training_args.save_total_limit=1 \n",
        "training_args.learning_rate=2e-5 \n",
        "training_args.weight_decay=0. \n",
        "training_args.warmup_ratio=0.03 \n",
        "training_args.lr_scheduler_type=\"cosine\" \n",
        "training_args.logging_steps=1  \n",
        "training_args.fsdp=\"full_shard auto_wrap\"\n",
        "training_args.fsdp_transformer_layer_cls_to_wrap='LLaMADecoderLayer' \n",
        "training_args.tf32=True\n",
        "training_args"
      ],
      "metadata": {
        "id": "FucYlJbQ7Ayo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
        "from torch.distributed.fsdp.fully_sharded_data_parallel import (\n",
        "    CPUOffload,\n",
        "    BackwardPrefetch,\n",
        ")\n",
        "from torch.distributed.fsdp.wrap import (\n",
        "    size_based_auto_wrap_policy,\n",
        "    enable_wrap,\n",
        "    wrap,\n",
        ")\n",
        "\n",
        "def train(rank, model, tokenizer):\n",
        "    if tokenizer.pad_token is None:\n",
        "        smart_tokenizer_and_embedding_resize(\n",
        "            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
        "            tokenizer=tokenizer,\n",
        "            model=model,\n",
        "        )\n",
        "\n",
        "    tokenizer.add_special_tokens(\n",
        "        {\n",
        "             \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "             \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "             \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "         }\n",
        "     )\n",
        "    data_module = make_supervised_data_module(tokenizer=tokenizer, data_path=data_path)\n",
        "    my_auto_wrap_policy = functools.partial(\n",
        "        size_based_auto_wrap_policy, min_num_params=100\n",
        "    )\n",
        "    torch.cuda.set_device(rank)\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=200, num_training_steps=-1\n",
        "    )\n",
        "    train_loader = DataLoader(data_module, batch_size=2, shuffle=True)\n",
        "    for i in train_loader:\n",
        "      print(i)\n",
        "    model = model.to(rank)\n",
        "    #model = FSDP(model)\n",
        "    ddp_loss = torch.zeros(2).to(rank)\n",
        "    for epoch in range(3):\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = train_loader['input_ids'].to(rank)\n",
        "            attention_mask = batch['attention_mask'].to(rank)\n",
        "            labels = batch['labels'].to(rank)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs[0]\n",
        "            loss.backward()\n",
        "            scheduler.step()\n",
        "            ddp_loss[0] += loss.item()\n",
        "            ddp_loss[1] += len(data)\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n",
        "    if rank == 0:\n",
        "        print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, ddp_loss[0] / ddp_loss[1]))\n",
        "\n"
      ],
      "metadata": {
        "id": "V9ohcuy46ZHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(0, model, tokenizer)"
      ],
      "metadata": {
        "id": "ZlbfaokCecqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "os.environ['CUDA_VISIBLE_DEVICES']='0, 1, 2, 3, 4, 5, 6, 7, 8'\n",
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
      ],
      "metadata": {
        "id": "nl0oS2QmRuin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train(model, tokenizer)"
      ],
      "metadata": {
        "id": "ckc6RjnCKukt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install libnvinfer_plugin.so.8"
      ],
      "metadata": {
        "id": "1l6tdTyLtblG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!torchrun --nproc_per_node 1 /content/drive/MyDrive/train.py "
      ],
      "metadata": {
        "id": "fYIc9GM49PmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!deepspeed --num_gpus=0 /content/drive/MyDrive/train.py --deepspeed /content/drive/MyDrive/artifacts/config/ds_config.json"
      ],
      "metadata": {
        "id": "WuIA7nfn7VgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert(False)\n",
        "import copy\n",
        "import logging\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, Dict, Sequence\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import Trainer\n",
        "import dataclasses\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import io\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "from typing import Optional, Sequence, Union\n",
        "\n",
        "import openai\n",
        "import tqdm\n",
        "from openai import openai_object\n",
        "import copy\n",
        "\n",
        "#import utils\n",
        "\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "PROMPT_DICT = [{\n",
        "    \"user_input\":\"solidity program\",\n",
        "    \"completion\": \"invariants\"}]\n",
        "\n",
        "'''\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    model_name_or_path:Optional[str] = field(default='/content/drive/MyDrive/experiments/llama/LLaMA/7B')\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataArguments:\n",
        "    data_path: str = field(default=None, metadata={\"help\": \"/content/datasets\"})\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainingArguments(transformers.TrainingArguments):\n",
        "    cache_dir: Optional[str] = field(default=None)\n",
        "    optim: str = field(default=\"adamw_torch\")\n",
        "    model_max_length: int = field(\n",
        "        default=512,\n",
        "        metadata={\"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"},\n",
        "    )\n",
        "'''\n",
        "def _make_r_io_base(f, mode: str):\n",
        "    if not isinstance(f, io.IOBase):\n",
        "        f = open(f, mode=mode)\n",
        "    return f\n",
        "\n",
        "def jload(f, mode=\"r\"):\n",
        "    \"\"\"Load a .json file into a dictionary.\"\"\"\n",
        "    f = _make_r_io_base(f, mode)\n",
        "    jdict = json.load(f)\n",
        "    f.close()\n",
        "    return jdict\n",
        "\n",
        "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
        "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
        "    state_dict = trainer.model.state_dict()\n",
        "    if trainer.args.should_save:\n",
        "        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
        "        del state_dict\n",
        "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n",
        "\n",
        "\n",
        "def smart_tokenizer_and_embedding_resize(\n",
        "    special_tokens_dict: Dict,\n",
        "    tokenizer: transformers.PreTrainedTokenizer,\n",
        "    model: transformers.PreTrainedModel,\n",
        "):\n",
        "    \"\"\"Resize tokenizer and embedding.\n",
        "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
        "    \"\"\"\n",
        "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if num_new_tokens > 0:\n",
        "        input_embeddings = model.get_input_embeddings().weight.data\n",
        "        output_embeddings = model.get_output_embeddings().weight.data\n",
        "\n",
        "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "\n",
        "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
        "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
        "\n",
        "\n",
        "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
        "    \"\"\"Tokenize a list of strings.\"\"\"\n",
        "    tokenized_list = [\n",
        "        tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"longest\",\n",
        "            max_length=tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        for text in strings\n",
        "    ]\n",
        "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "    input_ids_lens = labels_lens = [\n",
        "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
        "    ]\n",
        "    return dict(\n",
        "        input_ids=input_ids,\n",
        "        labels=labels,\n",
        "        input_ids_lens=input_ids_lens,\n",
        "        labels_lens=labels_lens,\n",
        "    )\n",
        "\n",
        "\n",
        "def preprocess(\n",
        "    sources: Sequence[str],\n",
        "    targets: Sequence[str],\n",
        "    tokenizer: transformers.PreTrainedTokenizer,\n",
        ") -> Dict:\n",
        "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
        "    examples = [s + t for s, t in zip(sources, targets)]\n",
        "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
        "    input_ids = examples_tokenized[\"input_ids\"]\n",
        "    labels = copy.deepcopy(input_ids)\n",
        "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "        label[:source_len] = IGNORE_INDEX\n",
        "    return dict(input_ids=input_ids, labels=labels)\n",
        "\n",
        "\n",
        "class SupervisedDataset(Dataset):\n",
        "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer):\n",
        "        super(SupervisedDataset, self).__init__()\n",
        "        logging.warning(\"Loading data...\")\n",
        "        list_data_dict = jload(data_path)\n",
        "\n",
        "        logging.warning(\"Formatting inputs...\")\n",
        "        prompt_input, prompt_no_input = PROMPT_DICT[\"user_input\"], PROMPT_DICT[\"completion\"]\n",
        "        sources = [\n",
        "            prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\" else prompt_no_input.format_map(example)\n",
        "            for example in list_data_dict\n",
        "        ]\n",
        "        targets = [f\"{example['output']}{tokenizer.eos_token}\" for example in list_data_dict]\n",
        "\n",
        "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
        "        data_dict = preprocess(sources, targets, tokenizer)\n",
        "\n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )\n",
        "\n",
        "'''\n",
        "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer, data_args) -> Dict:\n",
        "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
        "    train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path)\n",
        "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
        "    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)\n",
        "\n",
        "\n",
        "def train():\n",
        "    parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
        "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
        "\n",
        "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        cache_dir=training_args.cache_dir,\n",
        "    )\n",
        "\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        cache_dir=training_args.cache_dir,\n",
        "        model_max_length=training_args.model_max_length,\n",
        "        padding_side=\"right\",\n",
        "        use_fast=False,\n",
        "    )\n",
        "    if tokenizer.pad_token is None:\n",
        "        smart_tokenizer_and_embedding_resize(\n",
        "            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
        "            tokenizer=tokenizer,\n",
        "            model=model,\n",
        "        )\n",
        "    if \"llama\" in model_args.model_name_or_path:\n",
        "        tokenizer.add_special_tokens(\n",
        "            {\n",
        "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n",
        "    trainer = Trainer(model=model, tokenizer=tokenizer, args=training_args, **data_module)\n",
        "    trainer.train()\n",
        "    trainer.save_state()\n",
        "    safe_save_model_for_hf_trainer(trainer=trainer, output_dir='/content/saved_model')\n",
        " \n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "#    train()"
      ],
      "metadata": {
        "id": "bfZC4kepAWMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tatsu-lab/stanford_alpaca.git"
      ],
      "metadata": {
        "id": "_hSIKVY9e7Vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chatllama-py\n",
        "!git clone https://github.com/facebookresearch/llama.git\n",
        "!pip install -r /content/llama/requirements.txt\n",
        "!pip install -e /content/llama/.\n"
      ],
      "metadata": {
        "id": "8p8DnNn4fLQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export WORLD_SIZE=1\n",
        "!export LOCAL_RANK=0\n",
        "!export RANK=0\n",
        "!export _MODEL_PARALLEL_GROUP=1"
      ],
      "metadata": {
        "id": "Q2zHLPqGzJ9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!torchrun --nproc_per_node=1 drive/MyDrive/artifacts/main.py -t ACTOR drive/MyDrive/artifacts/config/config.yaml"
      ],
      "metadata": {
        "id": "HdkGBmnnoo73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!torchrun --nproc_per_node=1 --master_port=22 /content/stanford_alpaca/train.py \\\n",
        "    --model_name_or_path '/content/drive/MyDrive/experiments/HF_Llama/llama-7b'\\\n",
        "    --data_path '/content/datasets/actor_training_data.json' \\\n",
        "    --bf16 True \\\n",
        "    --output_dir '/content/saved_model' \\\n",
        "    --num_train_epochs 3 \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --per_device_eval_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --evaluation_strategy \"no\" \\\n",
        "    --save_strategy \"steps\" \\\n",
        "    --save_steps 2000 \\\n",
        "    --save_total_limit 1 \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --weight_decay 0. \\\n",
        "    --warmup_ratio 0.03 \\\n",
        "    --lr_scheduler_type \"cosine\" \\\n",
        "    --logging_steps 1 \\\n",
        "    --fsdp \"full_shard auto_wrap\" \\\n",
        "    --fsdp_transformer_layer_cls_to_wrap 'LLaMADecoderLayer' \\\n",
        "    --tf32 True"
      ],
      "metadata": {
        "id": "OgPvq5Dvf4E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Finetuning##"
      ],
      "metadata": {
        "id": "SeKTNjbuD75U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chatllama-py"
      ],
      "metadata": {
        "id": "xqmOQKjvhhDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!zip -r /content/raw_datasets.zip /content/datasets\n",
        "#from google.colab import files\n",
        "#files.download(\"/content/raw_datasets.zip\")"
      ],
      "metadata": {
        "id": "2h5lq9EeDxZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepspeed\n",
        "!pip install mpi4py\n",
        "!pip install ninja"
      ],
      "metadata": {
        "id": "VzZ-zYLEwrOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget -O artifacts.zip https://nbllabartifacts.blob.core.windows.net/chatllama/artifacts.zip\\?sp\\=r\\&st\\=2023-03-08T14:53:24Z\\&se\\=2100-03-08T22:53:24Z\\&spr\\=https\\&sv\\=2021-06-08\\&sr\\=b\\&sig\\=jqr%2B2ZkR0SW9RjV0pDOdQ%2BDulLXLjbZ36vmNd4XxxyQ%3D\n",
        "#!unzip artifacts.zip\n"
      ],
      "metadata": {
        "id": "DrXX9phFLEm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/llama.git\n",
        "!pip install -r /content/llama/requirements.txt\n",
        "!pip install -e /content/llama/."
      ],
      "metadata": {
        "id": "X8irCLfEfULu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!deepspeed /content/artifacts/main.py -t ACTOR /content/artifacts/config/config.yaml"
      ],
      "metadata": {
        "id": "YlinPyD8mSlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPT-style finetuning: OOM error persisted##"
      ],
      "metadata": {
        "id": "1ksQlD8qhyV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "from numpy import random\n",
        "import pandas as pd\n",
        "import gspread\n",
        "#autenticating to google\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "#defining my worksheet\n",
        "worksheet = gc.open('short_invariants_for_gpt').sheet1\n",
        "#get_all_values gives a list of rows\n",
        "rows = worksheet.get_all_values()\n",
        "#Convert to a DataFrame \n",
        "cols = ['Target']\n",
        "df = pd.DataFrame(rows, columns=cols)\n",
        "df"
      ],
      "metadata": {
        "id": "SB1TLeCiD53K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_params={\n",
        "    \"MODEL\":\"llama_model\",             # model_type: t5-large\n",
        "    \"MAX_LENGTH\": 3036,  # max length of source text\n",
        "   # \"SEED\": random.randint(1000)    # randomized seeds to shuffle test set\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "Wm0QPuWiGPGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlYaKW9h4ai_"
      },
      "source": [
        "# Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vLQPGAn4v17"
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from rich.console import Console\n",
        "from tqdm import tqdm, trange\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import torch\n",
        "import csv\n",
        "\n",
        "class GPTDataSetClass(Dataset):\n",
        "  \"\"\"\n",
        "  Creating a custom dataset for reading the dataset and \n",
        "  loading it into the dataloader to pass it to the neural network for finetuning the model\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, target_label, truncate=False, \\\n",
        "               gpt2_type=model, \\\n",
        "               max_length=2048):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.target_tokens = []\n",
        "\n",
        "    for row in df['Target']:\n",
        "        self.target_tokens.append(torch.tensor(\n",
        "            self.tokenizer.encode(f\"<|{target_label}|>{row[:max_length]}<|endoftext|>\")\n",
        "            ))  \n",
        "    if truncate:\n",
        "            self.target_tokens = self.target_tokens[:20000]\n",
        "    self.length = len(self.target_tokens)   \n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.target_tokens[index]   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Accumulated batch size (since GPT2 is so big)\n",
        "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
        "    if packed_tensor is None:\n",
        "        return new_tensor, True, None\n",
        "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
        "        return packed_tensor, False, new_tensor\n",
        "    else:\n",
        "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
        "        return packed_tensor, True, None"
      ],
      "metadata": {
        "id": "gJ2ccsedzcAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    dataset, model, tokenizer,\n",
        "    batch_size=0.1, epochs=3, lr=2e-5,\n",
        "    max_seq_len=2048, warmup_steps=200,\n",
        "    gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\",\n",
        "    test_mode=False, save_model_on_epoch=False,\n",
        "):\n",
        "    acc_steps = 10\n",
        "    device=torch.device(\"cuda\")\n",
        "    model = model\n",
        "    model.train()\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
        "    )\n",
        "\n",
        "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "    loss=0\n",
        "    accumulating_batch_count = 0\n",
        "    input_tensor = None\n",
        "    loss_list = []\n",
        "    epoch_list = []\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = []\n",
        "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
        "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
        "\n",
        "            if carry_on and idx != len(train_dataloader) - 1:\n",
        "                continue\n",
        "\n",
        "            input_tensor = input_tensor.to('cuda')\n",
        "            outputs = model(input_tensor, labels=input_tensor)  \n",
        "            loss = outputs[0] \n",
        "            total_loss.append(float(loss.item()))       \n",
        "            loss.backward()\n",
        "            torch.cuda.empty_cache()                      \n",
        "            if (accumulating_batch_count % batch_size) == 0:\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                model.zero_grad()\n",
        "  \n",
        "            accumulating_batch_count += 1\n",
        "            input_tensor = None\n",
        "\n",
        "        #training_logger.add_row(str(epoch), str(np.mean(total_loss)))       \n",
        "        if save_model_on_epoch:\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
        "            )\n",
        "        loss_list.append(np.mean(total_loss))\n",
        "        epoch_list.append(epoch) \n",
        "        print(f\"for epoch {epoch} the loss is {np.mean(total_loss)}\\n\")\n",
        "   # console.print(training_logger)   \n",
        "   # plot_loss(epoch_list, loss_list)\n",
        "    return model"
      ],
      "metadata": {
        "id": "PTVyI2fzzn3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_GPT2(df, model_params):   \n",
        "  #console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
        "\n",
        "  # tokenzier for encoding the text\n",
        "  dataset = GPTDataSetClass(df['Target'], truncate=False, gpt2_type=model) \n",
        "  #Get the tokenizer and model\n",
        "  #tokenizer = GPT2Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
        " # model = GPT2LMHeadModel.from_pretrained(model_params[\"MODEL\"])  \n",
        "  trained_model = train(dataset, model, tokenizer)\n",
        "  console.log(f\"[Saving Model]...\\n\")\n",
        "  #Saving the model after training\n",
        "  path = os.path.join('/content/output', \"model_files\")\n",
        "  model.save_pretrained(path)\n",
        "  tokenizer.save_pretrained(path)\n",
        "  console.print(f\"\"\"[Model] Model saved @ {os.path.join('/content/output', \"model_files\")}\\n\"\"\")\n",
        "  \n",
        "  # logging\n",
        "  console.log(f\"[Data]: Reading Raw data...\\n\")\n",
        "\n",
        "\n",
        "  # Creation of Dataset and Dataloader\n",
        "  # Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
        "\n",
        "  console.print(f\"FULL Dataset: {df.shape}\")\n",
        "  return trained_model, tokenizer\n"
      ],
      "metadata": {
        "id": "nOHr5lM82h53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    entry_count=10,\n",
        "    entry_length=2048, #maximum number of words\n",
        "    top_p=0.8,\n",
        "    temperature=1.,\n",
        "):\n",
        "    model.eval()\n",
        "    generated_num = 0\n",
        "    generated_list = []\n",
        "\n",
        "    filter_value = -float(\"Inf\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for entry_idx in trange(entry_count):\n",
        "\n",
        "            entry_finished = False\n",
        "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "\n",
        "            for i in range(entry_length):\n",
        "                outputs = model(generated, labels=generated)\n",
        "                loss, logits = outputs[:2]\n",
        "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
        "                    ..., :-1\n",
        "                ].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                logits[:, indices_to_remove] = filter_value\n",
        "\n",
        "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
        "                generated = torch.cat((generated, next_token), dim=1)\n",
        "\n",
        "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
        "                    entry_finished = True\n",
        "\n",
        "                if entry_finished:\n",
        "\n",
        "                    generated_num = generated_num + 1\n",
        "\n",
        "                    output_list = list(generated.squeeze().numpy())\n",
        "                    output_text = tokenizer.decode(output_list)\n",
        "                    generated_list.append(output_text + '\\n')\n",
        "                    break\n",
        "            \n",
        "            if not entry_finished:\n",
        "              output_list = list(generated.squeeze().numpy())\n",
        "              output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n",
        "              generated_list.append(output_text + '\\n')\n",
        "                \n",
        "    return generated_list\n",
        "\n",
        "#Function to generate multiple sentences. Test data should be a dataframe\n",
        "def text_generation(model, tokenizer, test_data):\n",
        "  generated_code = []\n",
        "  for i in range(len(test_data)):    \n",
        "    x = generate(model.to('cpu'), tokenizer, test_data['Target'][i], entry_count=1)\n",
        "    generated_code.append(x)\n",
        "  return generated_code\n",
        "\n",
        "#Run the functions to generate the lyrics\n",
        "\n",
        "def test_fine_tuned_gpt2(model, tokenizer, df): \n",
        "    test_set = df.sample(n=1)\n",
        "    df = df.loc[~df.index.isin(test_set.index)]\n",
        "\n",
        "    #Reset the indexes\n",
        "    test_set = test_set.reset_index()\n",
        "    df = df.reset_index()\n",
        "\n",
        "    #For the test set only, keep last 20 words in a new column, then remove them from original column\n",
        "    test_set['Target'] = test_set['Target'].str.split().apply(' '.join)\n",
        "    generated_code = text_generation(model, tokenizer, test_set)\n",
        "    print(generated_code)\n"
      ],
      "metadata": {
        "id": "ujnNVivM1bMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To solve CUDA out of memory error\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "os.environ['CUDA_VISIBLE_DEVICES']='0, 1, 2, 3, 4, 5, 6, 7, 8'\n",
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
      ],
      "metadata": {
        "id": "atnMJANnJlf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model, trained_tokenizer = fine_tune_GPT2(df, model_params)"
      ],
      "metadata": {
        "id": "GtBLzx0bHiXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-4qjuS_RhdxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "import deepspeed\n",
        "import torch\n",
        "from beartype import beartype\n",
        "from beartype.typing import Tuple\n",
        "from einops import rearrange\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "local_rank, world_size = setup_model_parallel()\n",
        "            # use load_model_test for testing\n",
        "  self.model, self.tokenizer = load_model(\n",
        "   ckpt_dir=model,\n",
        "   tokenizer_path=tokenizer,\n",
        "   local_rank=local_rank,\n",
        "   world_size=world_size,\n",
        "   froze_embeddings=config.froze_embeddings,\n",
        "   use_fairscale=config.use_fairscale,\n",
        "   max_batch_size=config.batch_size,\n",
        "    )\n",
        "        elif config.model in hf_models_seq_2_seq:\n",
        "    \n",
        "            # galactica tokenizer eos_token is None\n",
        "            if self.tokenizer.eos_token is None:\n",
        "                self.tokenizer.eos_token = \"</s>\"\n",
        "                self.tokenizer.eos_token_id = 0\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                config.model,\n",
        "            )\n",
        "            self.model.to(config.device)\n",
        "\n",
        "        # load the model from model_folder\n",
        "        self.load()\n",
        "\n",
        "    @beartype\n",
        "    def load(self) -> None:\n",
        "        \"\"\"Load the model from the path\n",
        "\n",
        "        Args:\n",
        "            path (str): path to the model\n",
        "        \"\"\"\n",
        "        path = ModelLoader.check_model_path(\n",
        "            config=self.config,\n",
        "            is_checkpoint=False,\n",
        "            current_epoch=None,\n",
        "        )\n",
        "        if path is not None:\n",
        "            print(\"Loading ...\")\n",
        "            model_dict = torch.load(path)\n",
        "            self.model.load_state_dict(model_dict[\"model\"])\n",
        "            self.head.load_state_dict(model_dict[\"head\"])\n",
        "\n",
        "    @beartype\n",
        "    def save(self) -> None:\n",
        "        \"\"\"Save the model to the path\n",
        "\n",
        "        Args:\n",
        "            path (Optional[str], optional): Path to store the model.\n",
        "                Defaults to None.\n",
        "        \"\"\"\n",
        "        model_folder, model_name, path = ModelLoader.get_model_path(\n",
        "            config=self.config,\n",
        "            is_checkpoint=False,\n",
        "            current_epoch=None,\n",
        "        )\n",
        "        print(f\"Saving model to {path} ...\")\n",
        "        torch.save(\n",
        "            {\"model\": self.model.state_dict(), \"head\": self.head.state_dict()},\n",
        "            path,\n",
        "        )\n",
        "\n",
        "    def parameters(self, **kwargs):\n",
        "        \"\"\"Return the parameters of the model\n",
        "\n",
        "        Args:\n",
        "            **kwargs:\n",
        "        \"\"\"\n",
        "        return self.model.parameters()\n",
        "\n",
        "    @beartype\n",
        "    def forward(\n",
        "        self, sequences: torch.Tensor, sequences_mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Generate logits to have probability distribution over the vocabulary\n",
        "            of the actions\n",
        "\n",
        "        Args:\n",
        "            sequences (torch.Tensor): Sequences of states and actions used to\n",
        "                    compute token logits for the whole list of sequences\n",
        "            attention_mask (torch.Tensor): Mask for the sequences attention\n",
        "\n",
        "        Returns:\n",
        "            logits (torch.Tensor): Logits for the actions taken\n",
        "        \"\"\"\n",
        "        model_output = self.model.forward(\n",
        "            sequences, attention_mask=sequences_mask\n",
        "        )\n",
        "        # need to return logits for the actions\n",
        "        if self.config.model in hf_models_causal_lm:\n",
        "            model_output = model_output.logits\n",
        "        if self.config.debug:\n",
        "            print(\"ActorModel.forward\")\n",
        "            print(\"model_output_logits shape\", model_output.shape)\n",
        "            print(\"model_output logits\", model_output)\n",
        "        return model_output\n",
        "\n",
        "    @beartype\n",
        "    @torch.no_grad()\n",
        "    def generate(\n",
        "        self, states: torch.Tensor, state_mask: torch.Tensor\n",
        "    ) -> Tuple:\n",
        "        \"\"\"Generate actions and sequences=[states, actions] from state\n",
        "            (i.e. input of the prompt generator model)\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor): the input of the user\n",
        "            state_mask (torch.Tensor): Mask for the state input (for padding)\n",
        "\n",
        "        Returns:\n",
        "            actions (torch.Tensor): Actions generated from the state\n",
        "            sequences (torch.Tensor): Sequences generated from the\n",
        "                state as [states, actions]\n",
        "        \"\"\"\n",
        "        temperature = self.config.temperature\n",
        "        # max sequence length for the actor (i.e. prompt + completion)\n",
        "        # from config file - it depends by the model used\n",
        "        max_sequence_length = self.config.max_sequence_length\n",
        "        # max tokens generated by the actor (completion only) from config file\n",
        "        max_tokens = self.config.max_tokens\n",
        "        # temperature for the actor\n",
        "        max_generation_possible = max_sequence_length - states.shape[1]\n",
        "        # take the minimum between the maximum token that you want to generate\n",
        "        # and the token that is possible to generate given the maximum sequence\n",
        "        # supported\n",
        "        max_completion = min(max_tokens, max_generation_possible)\n",
        "        if max_completion <= 0:\n",
        "            raise ValueError(\n",
        "                \"The maximum completion available is <= 0 the prompt is too \"\n",
        "                + \"long w.r.t the model sequence length\"\n",
        "            )\n",
        "        # the max_length is then the input length + the completion length\n",
        "        max_length = states.shape[1] + max_completion\n",
        "        # generate\n",
        "        sequences = self.model.generate(\n",
        "            input_ids=states,\n",
        "            attention_mask=state_mask,\n",
        "            temperature=temperature,\n",
        "            max_length=max_length,\n",
        "        )\n",
        "        actions = sequences[:, states.shape[1] :]  # noqa E203\n",
        "        if self.config.debug:\n",
        "            print(\"ActorModel.generate\")\n",
        "            print(\"state\", states)\n",
        "            print(\"state shape\", states.shape)\n",
        "            print(\"sequence shape\", sequences.shape)\n",
        "            print(\"sequence\", sequences)\n",
        "            print(\"actions shape\", actions.shape)\n",
        "            print(\"actions\", actions)\n",
        "        return actions, sequences\n",
        "\n",
        "\n",
        "class ActorDataset(Dataset):\n",
        "    \"\"\"Dataset for the pretraining of the actor model\n",
        "    read a json file with the following format:\n",
        "    [\n",
        "        {\n",
        "            \"user_input\": \"...\"\n",
        "            \"completion\": \"...\"\n",
        "        },\n",
        "        ...\n",
        "    ]\n",
        "    Where:\n",
        "        user_input: the input of the user\n",
        "        completion: the output of the user\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path: str) -> None:\n",
        "        self.path = path\n",
        "        with open(path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "            self.data = [d[\"user_input\"] + \" \" + d[\"completion\"] for d in data]\n",
        "        self.len = len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def __len__(\n",
        "        self,\n",
        "    ):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "class ActorTrainer:\n",
        "    \"\"\"Used to pre-train the actor model to generate better prompts.\n",
        "\n",
        "    Args:\n",
        "        config (ConfigActor): Configuration for the actor model\n",
        "\n",
        "    Attributes:\n",
        "        config (ConfigActor): Configuration for the actor model\n",
        "        model (ActorModel): Actor model\n",
        "        loss_function (torch.nn.CrossEntropyLoss): Loss function\n",
        "        optimizer (torch.optim.Adam): Optimizer\n",
        "        validation_flag (bool): Flag to indicate if the validation dataset\n",
        "            is provided\n",
        "        training_stats (TrainingStats): Training statistics\n",
        "\n",
        "    Methods:\n",
        "        train: Train the actor model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: ConfigActor) -> None:\n",
        "        # load the model, optimizer, loss function and config\n",
        "        self.config = config\n",
        "        self.model = ActorModel(config)\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            self.model.parameters(), lr=config.lr\n",
        "        )\n",
        "\n",
        "        # check checkpoint, datasets and other data\n",
        "        self.validation_flag = False\n",
        "        self.training_stats = TrainingStats()\n",
        "        if config.validation_dataset_path is not None:\n",
        "            self.validation_flag = True\n",
        "\n",
        "        # create dataloaders\n",
        "        self.train_dataset = ActorDataset(config.train_dataset_path)\n",
        "        self.train_dataloader = DataLoader(\n",
        "            self.train_dataset, batch_size=config.batch_size\n",
        "        )\n",
        "        if self.validation_flag:\n",
        "            self.eval_dataset = ActorDataset(config.validation_dataset_path)\n",
        "            self.validation_dataloader = DataLoader(\n",
        "                self.eval_dataset, batch_size=config.batch_size\n",
        "            )\n",
        "\n",
        "        # initialize deepspeed\n",
        "        self.model_engine = None\n",
        "        if config.deepspeed_enable is True:\n",
        "            if config.deepspeed_config_path is None:\n",
        "                raise ValueError(\n",
        "                    \"DeepSpeed config path is None, but deepspeed is enabled\"\n",
        "                )\n",
        "            if os.path.exists(config.deepspeed_config_path) is False:\n",
        "                raise ValueError(\n",
        "                    f\"DeepSpeed config path {config.deepspeed_config_path}\"\n",
        "                    f\"does not exist\"\n",
        "                )\n",
        "            (\n",
        "                self.model_engine,\n",
        "                self.optimizer,\n",
        "                self.train_dataloader,\n",
        "                _,\n",
        "            ) = deepspeed.initialize(\n",
        "                args=None,\n",
        "                model=self.model,\n",
        "                model_parameters=self.model.parameters(),\n",
        "                training_data=self.train_dataloader,\n",
        "                config=self.config.deepspeed_config_path,\n",
        "            )\n",
        "\n",
        "    @beartype\n",
        "    def save_checkpoint(\n",
        "        self,\n",
        "        current_epoch: int,\n",
        "        current_step: int,\n",
        "        max_epochs: int,\n",
        "        max_steps: int,\n",
        "    ) -> None:\n",
        "\n",
        "        print(\n",
        "            f\"Saving checkpoint for epoch {current_epoch + 1}, \"\n",
        "            f\"step {current_step + 1} ...\"\n",
        "        )\n",
        "        model_folder, model_name, path = ModelLoader.get_model_path(\n",
        "            config=self.config,\n",
        "            is_checkpoint=True,\n",
        "            current_epoch=current_epoch,\n",
        "            current_step=current_step,\n",
        "            max_epochs=max_epochs,\n",
        "            max_steps=max_steps,\n",
        "        )\n",
        "        if os.path.exists(path):\n",
        "            os.remove(path)\n",
        "        torch.save(\n",
        "            {\n",
        "                \"state_dict\": self.model.state_dict(),\n",
        "                \"optim_state_dict\": self.optimizer.state_dict(),\n",
        "                \"training_stats\": self.training_stats,\n",
        "                \"epoch\": current_epoch,\n",
        "                \"step\": current_step,\n",
        "            },\n",
        "            path,\n",
        "        )\n",
        "\n",
        "    @beartype\n",
        "    def load_checkpoint(\n",
        "        self,\n",
        "    ) -> Tuple[int, int]:\n",
        "        \"\"\"Load a checkpoint from the model folder\"\"\"\n",
        "\n",
        "        print(\"Looking for checkpoints...\")\n",
        "        path = ModelLoader.check_model_path(\n",
        "            config=self.config,\n",
        "            is_checkpoint=True,\n",
        "            current_epoch=None,\n",
        "        )\n",
        "        if path is not None:\n",
        "            print(\"Loading ...\")\n",
        "            checkpoint = torch.load(path)\n",
        "            epoch = checkpoint[\"epoch\"]\n",
        "            self.model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "            self.optimizer.load_state_dict(checkpoint[\"optim_state_dict\"])\n",
        "            self.trainign_stats = checkpoint[\"training_stats\"]\n",
        "            step = checkpoint[\"step\"]\n",
        "            return epoch, step + 1  # return the next episode to train\n",
        "        return 0, 0\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "    ) -> None:\n",
        "        print(\"Start Actor Model Pretraining\")\n",
        "\n",
        "        # get config parameters\n",
        "        batch_size = self.config.batch_size\n",
        "        epochs = self.config.epochs\n",
        "        device = self.config.device\n",
        "        checkpoint_steps = self.config.checkpoint_steps\n",
        "\n",
        "        # compute the number of iterations\n",
        "        n_iter = int(len(self.train_dataset) / batch_size)\n",
        "\n",
        "        # load model_checkpoint\n",
        "        start_epoch, start_step = self.load_checkpoint()\n",
        "\n",
        "        # counter for the checkpoint\n",
        "        cnt_checkpoint = 1\n",
        "\n",
        "        # traing loop\n",
        "        for epoch in range(start_epoch, epochs):\n",
        "            self.model.train()\n",
        "            for i, input_output in enumerate(self.train_dataloader):\n",
        "                if i < start_step:\n",
        "                    continue\n",
        "                with torch.no_grad():\n",
        "                    input_output_tokenized = self.model.tokenizer(\n",
        "                        input_output,\n",
        "                        return_tensors=\"pt\",\n",
        "                        padding=True,\n",
        "                    )\n",
        "                    training_output = input_output_tokenized[\"input_ids\"][\n",
        "                        :, 1:\n",
        "                    ]\n",
        "                    training_input = input_output_tokenized[\"input_ids\"][\n",
        "                        :, :-1\n",
        "                    ]\n",
        "                    attention_mask = input_output_tokenized[\"attention_mask\"][\n",
        "                        :, :-1\n",
        "                    ]\n",
        "                    training_output = training_output.to(device)\n",
        "                    training_input = training_input.to(device)\n",
        "                    attention_mask = attention_mask.to(device)\n",
        "\n",
        "                # forward pass\n",
        "                if self.config.deepspeed_enable:\n",
        "                    est_output = self.model_engine(\n",
        "                        training_input, attention_mask\n",
        "                    )\n",
        "                else:\n",
        "                    est_output = self.model(training_input, attention_mask)\n",
        "                est_output = rearrange(est_output, \"b s v -> (b s) v\")\n",
        "                training_output = rearrange(training_output, \"b s -> (b s)\")\n",
        "                loss = self.loss_function(est_output, training_output)\n",
        "                self.training_stats.training_loss.append(loss.item())\n",
        "\n",
        "                # backward pass\n",
        "                if self.config.deepspeed_enable:\n",
        "                    self.model_engine.backward(loss)\n",
        "                    self.model_engine.step()\n",
        "                else:\n",
        "                    self.optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "\n",
        "                # print progress\n",
        "                if i % self.config.iteration_per_print == 0:\n",
        "                    print(\n",
        "                        f\"Epoch: {epoch+1}/{epochs}, \"\n",
        "                        f\"Iteration: {i+1}/{n_iter}, \"\n",
        "                        f\"Training Loss: {loss}\"\n",
        "                    )\n",
        "                # save checkpoint periodically\n",
        "                if cnt_checkpoint % checkpoint_steps == 0:\n",
        "                    self.save_checkpoint(epoch, i, epochs, n_iter)\n",
        "                    cnt_checkpoint = 1\n",
        "                else:\n",
        "                    cnt_checkpoint += 1\n",
        "\n",
        "            if self.validation_flag:\n",
        "                self.model.eval()\n",
        "                for i, input_output in enumerate(self.validation_dataloader):\n",
        "                    input_output_tokenized = self.model.tokenizer(\n",
        "                        input_output, return_tensors=\"pt\", padding=True\n",
        "                    )\n",
        "                    validation_output = input_output_tokenized[\"input_ids\"][\n",
        "                        :, 1:\n",
        "                    ]\n",
        "                    validation_input = input_output_tokenized[\"input_ids\"][\n",
        "                        :, :-1\n",
        "                    ]\n",
        "                    attention_mask = input_output_tokenized[\"attention_mask\"][\n",
        "                        :, :-1\n",
        "                    ]\n",
        "\n",
        "                    # forward pass\n",
        "                    est_output = self.model.forward(\n",
        "                        validation_input, attention_mask\n",
        "                    )\n",
        "                    validation_output = rearrange(\n",
        "                        validation_output, \"b s -> (b s)\"\n",
        "                    )\n",
        "                    est_output = rearrange(est_output, \"b s v -> (b s) v\")\n",
        "                    loss = self.loss_function(est_output, validation_output)\n",
        "                    self.training_stats.validation_loss.append(loss.item())\n",
        "                    # print progress\n",
        "                    if i % self.config.iteration_per_print == 0:\n",
        "                        print(\n",
        "                            f\"Epoch: {epoch+1}/{epochs}, \"\n",
        "                            f\"Iteration: {i+1}/{n_iter}, \"\n",
        "                            f\"Validation Loss: {loss}\"\n",
        "                        )\n",
        "        self.model.save()\n",
        "        print(\"Training Finished \")"
      ],
      "metadata": {
        "id": "5MreDvACfcCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_test_contrct(path, ratio): \n",
        "  col = ['Target']\n",
        "  f = open(path, \"r\")\n",
        "  file = f.read()\n",
        "  test_df = pd.DataFrame([file], columns=col)\n",
        "  program_length = len(test_df['Target'][0].split())\n",
        "  prompt_ratio = ratio\n",
        "  prompt_length = int(prompt_ratio * program_length)\n",
        "  return test_df, prompt_length\n",
        "\n",
        "def truncate_test(df, prompt_length):\n",
        "  copy_1 = df.copy(deep=True)\n",
        "  copy_2 = df.copy(deep=True)\n",
        "  #true\n",
        "  a = copy_1['Target'].str.split().str[-prompt_length:].apply(' '.join)[0]\n",
        "  #masked out program \n",
        "  b = copy_2['Target'].str.split().str[:-prompt_length].apply(' '.join)[0]\n",
        "  return a, b"
      ],
      "metadata": {
        "id": "_hKPs7PCcuHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df, n = generate_test_contrct('/content/drive/MyDrive/experiments/baseline benchmark/Replica.sol', 0.95)\n",
        "truth, prompt = truncate_test(test_df, n)\n",
        "col = ['Target']\n",
        "prompt_df = pd.DataFrame([prompt], columns=col)\n",
        "gpt_out = text_generation(trained_model, tokenizer, prompt_df)\n",
        "print('this is the GPT prompt without T5: \\n', prompt_df['Target'][0])\n",
        "print('this is the GPT prediction without T5: \\n', gpt_out)"
      ],
      "metadata": {
        "id": "z5vMNi2eHxkh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}