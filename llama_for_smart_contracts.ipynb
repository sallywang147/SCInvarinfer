{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPUWhVtFNNavYWgOrdJqhhV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sallywang147/SCInvarinfer/blob/main/llama_for_smart_contracts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lILBOi-WpPWw",
        "outputId": "1754edb6-890b-4cb2-fcad-63b4092049bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install rich[jupyter]\n",
        "!pip install deepspeed\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KELJ4pTduNrW",
        "outputId": "3ddfd70a-3dda-47ad-cad3-1d2be282a7da"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (0.1.97)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-fr7ts69l\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-fr7ts69l\n",
            "  Resolved https://github.com/huggingface/transformers to commit 5fd4e3c87c685fba2dd9615be62131748a8b5ee3\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (0.13.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (3.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (2.0.12)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rich[jupyter] in /usr/local/lib/python3.9/dist-packages (13.3.2)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from rich[jupyter]) (2.14.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from rich[jupyter]) (2.2.0)\n",
            "Requirement already satisfied: ipywidgets<9,>=7.5.1 in /usr/local/lib/python3.9/dist-packages (from rich[jupyter]) (7.7.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (3.0.5)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (3.6.2)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (5.7.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (7.9.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (5.3.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich[jupyter]) (0.1.2)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.5.1->rich[jupyter]) (6.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.5.1->rich[jupyter]) (6.1.12)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (2.0.10)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (63.4.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.2.0)\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.18.2)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.9/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (6.5.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.8.3)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (23.2.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (5.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (3.1.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.17.1)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.5.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (21.3.0)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (6.5.4)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.8.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (5.7.3)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.5.3)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.9/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets<9,>=7.5.1->rich[jupyter]) (2.8.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.2.6)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.16.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect->ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.7.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (3.1.1)\n",
            "Requirement already satisfied: notebook-shim>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.2.2)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.9/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.23.6)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (6.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (23.0)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.7.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (4.11.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.5.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (4.9.2)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (2.1.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.2.2)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.2.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.7.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.9/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (2.16.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.9/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (4.3.3)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.9/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (21.2.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.19.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (22.2.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.9/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.5.1)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (3.6.2)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.15.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (2.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.5.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.9/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.9/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (3.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (2.21)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: deepspeed in /usr/local/lib/python3.9/dist-packages (0.8.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from deepspeed) (5.9.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.9/dist-packages (from deepspeed) (1.10.6)\n",
            "Requirement already satisfied: hjson in /usr/local/lib/python3.9/dist-packages (from deepspeed) (3.1.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.9/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.9/dist-packages (from deepspeed) (1.11.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from deepspeed) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from deepspeed) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from deepspeed) (23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from deepspeed) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic->deepspeed) (4.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.9/dist-packages (0.27.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.8.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Llama_Weights = '/content/drive/MyDrive/experiments/llama/LLaMA/7B/'\n",
        "Output = 'drive/MyDrive/experiments/HF_Llama'"
      ],
      "metadata": {
        "id": "2JW9JFKlv9US"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python /content/drive/MyDrive/experiments/llama/convert_llama_weights_to_hf.py \\\n",
        "#--input_dir '/content/drive/MyDrive/experiments/llama/LLaMA/7B' \\\n",
        "# --model_size 7B --output_dir 'drive/MyDrive/experiments/HF_Llama'"
      ],
      "metadata": {
        "id": "6K35upsguFjR"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vanilla Llama##"
      ],
      "metadata": {
        "id": "7bYljYiYAYcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import AutoTokenizer, LlamaTokenizer, LlamaForCausalLM\n",
        "\n",
        "\n",
        "model = LlamaForCausalLM.from_pretrained('sallywww/Llama-7B')\n",
        "tokenizer = AutoTokenizer.from_pretrained('sallywww/Llama-7B')\n",
        "\n",
        "#model.push_to_hub('sallywww/Llama-7B')\n",
        "#tokenizer.push_to_hub('sallywww/Llama-7B')"
      ],
      "metadata": {
        "id": "D2oAIPSSxSNf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "outputId": "e4bc9172-4bf4-419a-abab-082de2ca1be4"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-560a03fa651b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLlamaForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sallywww/Llama-7B'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sallywww/Llama-7B'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2492\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_contexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2493\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2495\u001b[0m         \u001b[0;31m# Check first if we are `from_pt`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLlamaModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLlamaDecoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLlamaRMSNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrms_norm_eps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLlamaDecoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLlamaRMSNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrms_norm_eps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         )\n\u001b[0;32m--> 274\u001b[0;31m         self.mlp = LlamaMLP(\n\u001b[0m\u001b[1;32m    275\u001b[0m             \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mintermediate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hidden_size, intermediate_size, hidden_act)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintermediate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintermediate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mACT2FN\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhidden_act\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# https://github.com/pytorch/pytorch/issues/57109\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mfan_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_fan_in_and_fan_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/init.py\u001b[0m in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m  \u001b[0;31m# Calculate uniform bounds from standard deviation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert(False)\n",
        "prompt = \"a loop invariant in computer science is \"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate\n",
        "generate_ids = model.generate(inputs.input_ids, max_length=40)\n",
        "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n"
      ],
      "metadata": {
        "id": "SSQCT4xwDt2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##loading and json-ize data##"
      ],
      "metadata": {
        "id": "bdCPq8HpMeIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install simplejson"
      ],
      "metadata": {
        "id": "TBBRkS1uCl6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da8af3fa-d932-41ff-d807-62930c816501"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.9/dist-packages (3.18.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "import simplejson as json\n",
        "from numpy import random\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import pandas\n",
        "import xlrd\n",
        "import json\n",
        "import xlrd\n",
        "\n",
        "#workbook = xlrd.open_workbook('/invariants_line_number.xlsx', on_demand = True)\n",
        "df = pandas.read_excel(open('/content/invariants_data.xlsx','rb'))\n",
        "df"
      ],
      "metadata": {
        "id": "XtHDzEL8Bvya",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "cb6e47c0-12c9-4d7c-95f7-cd09cc4773c5"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Source  \\\n",
              "0    1 pragma solidity >=0.4.24 <0.6.0;\\n\\n2 contra...   \n",
              "1    1 // SPDX-License-Identifier: MIT\\n2 pragma ex...   \n",
              "2    1 pragma solidity >=0.4.24 <0.6.0;\\n\\n2 contra...   \n",
              "3    1 pragma solidity >=0.4.24 <0.6.0;\\n2 contract...   \n",
              "4    1 pragma solidity >=0.4.24 <0.6.0;\\n\\n2 // Thi...   \n",
              "..                                                 ...   \n",
              "98   1 pragma solidity >=0.4.24 <0.6.0;\\n2 \\n3 cont...   \n",
              "99   1  // SPDX-License-Identifier: MIT\\n2  pragma ...   \n",
              "100  1  // SPDX-License-Identifier: MIT\\n2  pragma ...   \n",
              "101  1 / SPDX-License-Identifier: MIT\\n2  pragma so...   \n",
              "102  1   // SPDX-License-Identifier: MIT\\n2 pragma ...   \n",
              "\n",
              "                                                Target  \n",
              "0                             14+ assert (y == x + 4);  \n",
              "1    109+  require(admin == msg.sender, \"Ownable: c...  \n",
              "2                              7+ assert (y == x + 2);  \n",
              "3    20+ assert(funcA2(funcA1())==12);\\n20+ assert(...  \n",
              "4    24+ assert(a == x + 1);\\n32+ assert(a == x);\\n...  \n",
              "..                                                 ...  \n",
              "98            6+ assert (!false);\\n6+ assert (!false);  \n",
              "99   6+  assert(fee + value != 0);\\n6+  assert(fee ...  \n",
              "100  15+ assert(_required > 0); \\n15+  assert(m_num...  \n",
              "101  6+  assert(address(this)==msg.sender);\\n6+  as...  \n",
              "102  30+ assert(msg.sender == _contractRegistry);\\n...  \n",
              "\n",
              "[103 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e47dfbc3-f3f4-46b2-a976-f5e193b88051\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1 pragma solidity &gt;=0.4.24 &lt;0.6.0;\\n\\n2 contra...</td>\n",
              "      <td>14+ assert (y == x + 4);</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1 // SPDX-License-Identifier: MIT\\n2 pragma ex...</td>\n",
              "      <td>109+  require(admin == msg.sender, \"Ownable: c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1 pragma solidity &gt;=0.4.24 &lt;0.6.0;\\n\\n2 contra...</td>\n",
              "      <td>7+ assert (y == x + 2);</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1 pragma solidity &gt;=0.4.24 &lt;0.6.0;\\n2 contract...</td>\n",
              "      <td>20+ assert(funcA2(funcA1())==12);\\n20+ assert(...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1 pragma solidity &gt;=0.4.24 &lt;0.6.0;\\n\\n2 // Thi...</td>\n",
              "      <td>24+ assert(a == x + 1);\\n32+ assert(a == x);\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>1 pragma solidity &gt;=0.4.24 &lt;0.6.0;\\n2 \\n3 cont...</td>\n",
              "      <td>6+ assert (!false);\\n6+ assert (!false);</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>1  // SPDX-License-Identifier: MIT\\n2  pragma ...</td>\n",
              "      <td>6+  assert(fee + value != 0);\\n6+  assert(fee ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>1  // SPDX-License-Identifier: MIT\\n2  pragma ...</td>\n",
              "      <td>15+ assert(_required &gt; 0); \\n15+  assert(m_num...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>1 / SPDX-License-Identifier: MIT\\n2  pragma so...</td>\n",
              "      <td>6+  assert(address(this)==msg.sender);\\n6+  as...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>1   // SPDX-License-Identifier: MIT\\n2 pragma ...</td>\n",
              "      <td>30+ assert(msg.sender == _contractRegistry);\\n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e47dfbc3-f3f4-46b2-a976-f5e193b88051')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e47dfbc3-f3f4-46b2-a976-f5e193b88051 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e47dfbc3-f3f4-46b2-a976-f5e193b88051');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _make_w_io_base(f, mode: str):\n",
        "    if not isinstance(f, io.IOBase):\n",
        "        f_dirname = os.path.dirname(f)\n",
        "        if f_dirname != \"\":\n",
        "            os.makedirs(f_dirname, exist_ok=True)\n",
        "        f = open(f, mode=mode)\n",
        "    return f\n",
        "\n",
        "\n",
        "def _make_r_io_base(f, mode: str):\n",
        "    if not isinstance(f, io.IOBase):\n",
        "        f = open(f, mode=mode)\n",
        "    return f\n",
        "\n",
        "\n",
        "def jdump(obj, f, mode=\"w\", indent=4, default=str):\n",
        "    \"\"\"Dump a str or dictionary to a file in json format.\n",
        "    Args:\n",
        "        obj: An object to be written.\n",
        "        f: A string path to the location on disk.\n",
        "        mode: Mode for opening the file.\n",
        "        indent: Indent for storing json dictionaries.\n",
        "        default: A function to handle non-serializable entries; defaults to `str`.\n",
        "    \"\"\"\n",
        "    f = _make_w_io_base(f, mode)\n",
        "    if isinstance(obj, (dict, list)):\n",
        "        json.dump(obj, f, indent=indent, default=default)\n",
        "    elif isinstance(obj, str):\n",
        "        f.write(obj)\n",
        "    else:\n",
        "        raise ValueError(f\"Unexpected type: {type(obj)}\")\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def jload(f, mode=\"r\"):\n",
        "    \"\"\"Load a .json file into a dictionary.\"\"\"\n",
        "    f = _make_r_io_base(f, mode)\n",
        "    jdict = json.load(f)\n",
        "    f.close()\n",
        "    return jdict\n"
      ],
      "metadata": {
        "id": "ykNHXMR4K0eB"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List to hold dictionaries\n",
        "def prepare_actor_model_data(df): \n",
        "    data_list=[]\n",
        "    for index, row in df.iterrows():\n",
        "        data = OrderedDict()\n",
        "        data[\"user_input\"] = row['Source']\n",
        "        data[\"completion\"] = row['Target']\n",
        "        data_list.append(data)\n",
        "    jdump(data_list, 'datasets/actor_training_data.json')\n",
        "\n",
        "def prepare_rlhf_model_data(df): \n",
        "    data_list=[]\n",
        "    for index, row in df.iterrows():\n",
        "        data = OrderedDict()\n",
        "        data[\"user_input\"] = row['Source']\n",
        "        data_list.append(data)\n",
        "    jdump(data, '/datasets/reward_training_data.json')\n",
        "\n",
        "\n",
        "def prepare_reward_model_data(df):\n",
        "    data_list=[]    \n",
        "    for index, row in df.iterrows():\n",
        "        data = OrderedDict()\n",
        "        data[\"user_input\"] = row['Source']\n",
        "        data[\"completion\"] = row['Target']\n",
        "        data[\"score\"] = 5\n",
        "        data_list.append(data)\n",
        "    jdump(data, '/content/datasets/rlhf_training_data.json')\n"
      ],
      "metadata": {
        "id": "VKnXo46McPRf"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_actor_model_data(df)\n",
        "prepare_rlhf_model_data(df)\n",
        "prepare_reward_model_data(df)"
      ],
      "metadata": {
        "id": "7tfm3a3ql0gZ"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "#!zip -r /content/datasets.zip /content/datasets\n",
        "#files.download(\"/content/datasets.zip\")"
      ],
      "metadata": {
        "id": "5jgix6Qwj6XY"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\""
      ],
      "metadata": {
        "id": "zlsyFBdE9SiS"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(\n",
        "    sources: Sequence[str],\n",
        "    targets: Sequence[str],\n",
        "    tokenizer: tokenizer,\n",
        ") -> Dict:\n",
        "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
        "    examples = [s + t for s, t in zip(sources, targets)]\n",
        "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
        "    input_ids = examples_tokenized[\"input_ids\"]\n",
        "    labels = copy.deepcopy(input_ids)\n",
        "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "        label[:source_len] = IGNORE_INDEX\n",
        "    return dict(input_ids=input_ids, labels=labels)\n",
        "\n",
        "\n",
        "def _tokenize_fn(strings: Sequence[str], tokenizer: tokenizer) -> Dict:   \n",
        "    \"\"\"Tokenize a list of strings.\"\"\"\n",
        "    \n",
        "    tokenized_list = [\n",
        "        tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"longest\",\n",
        "            max_length=tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        for text in strings\n",
        "    ]\n",
        "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "    input_ids_lens = labels_lens = [\n",
        "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
        "    ]\n",
        "    return dict(\n",
        "        input_ids=input_ids,\n",
        "        labels=labels,\n",
        "        input_ids_lens=input_ids_lens,\n",
        "        labels_lens=labels_lens,\n",
        "    )"
      ],
      "metadata": {
        "id": "j4ZafBDGUss_"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tokenizer.pad_token is None or tokenizer.eos_token:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))  \n",
        "print(tokenizer.pad_token)"
      ],
      "metadata": {
        "id": "w7VAcWvOYB8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f8708a1-d636-4f71-fad8-196a74d3cfcc"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PAD]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/datasets/actor_training_data.json', \"r\") as f:\n",
        "    data = json.load(f)\n",
        "    for d in data:\n",
        "     item = [str(d[\"user_input\"]) + \"\\n\" + str(d[\"completion\"]) for d in data]\n",
        "    length = len(item)\n",
        "sources = [f\"{example['user_input']}{tokenizer.eos_token}\" for example in data]#\n",
        "targets = [f\"{example['completion']}{tokenizer.eos_token}\" for example in data]\n",
        "\n",
        "data_dict = preprocess(sources, targets, tokenizer)\n",
        "input_ids = data_dict[\"input_ids\"]\n",
        "labels = data_dict[\"labels\"]"
      ],
      "metadata": {
        "id": "UTOQV6s7NkO4"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Finetuning using stanford dataprocessing## "
      ],
      "metadata": {
        "id": "Yea7vM37AxuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "!pip install peft\n",
        "!pip install --upgrade datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a-p1H07EdQd",
        "outputId": "a144eefd-f0da-44ee-b86f-68338857427a"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.9/dist-packages (3.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from gradio) (4.5.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.9/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from gradio) (1.22.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.9/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.9/dist-packages (from gradio) (1.10.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from gradio) (3.8.4)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.9/dist-packages (from gradio) (0.3.0)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.9/dist-packages (from gradio) (0.23.3)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (2.2.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.9/dist-packages (from gradio) (0.21.1)\n",
            "Requirement already satisfied: semantic-version in /usr/local/lib/python3.9/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from gradio) (6.0)\n",
            "Requirement already satisfied: websockets>=10.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (10.4)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.9/dist-packages (from gradio) (23.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from gradio) (2.27.1)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.9/dist-packages (from gradio) (3.8.8)\n",
            "Requirement already satisfied: mdit-py-plugins<=0.3.3 in /usr/local/lib/python3.9/dist-packages (from gradio) (0.3.3)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.9/dist-packages (from gradio) (0.95.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from gradio) (2023.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.9/dist-packages (from gradio) (0.0.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from gradio) (8.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from gradio) (1.4.4)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.9/dist-packages (from gradio) (2.1.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->gradio) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->gradio) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->gradio) (3.10.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->gradio) (2022.7.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (2.0.12)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Requirement already satisfied: starlette<0.27.0,>=0.26.1 in /usr/local/lib/python3.9/dist-packages (from fastapi->gradio) (0.26.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from httpx->gradio) (2022.12.7)\n",
            "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /usr/local/lib/python3.9/dist-packages (from httpx->gradio) (1.5.0)\n",
            "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /usr/local/lib/python3.9/dist-packages (from httpx->gradio) (0.16.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.9/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (4.39.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (1.0.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (5.12.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->gradio) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->gradio) (3.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from uvicorn->gradio) (8.1.3)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.9/dist-packages (from uvicorn->gradio) (0.14.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.9/dist-packages (from httpcore<0.17.0,>=0.15.0->httpx->gradio) (3.6.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->gradio) (3.15.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.9/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->gradio) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.9/dist-packages (0.3.0.dev0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from peft) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from peft) (23.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.9/dist-packages (from peft) (0.17.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (from peft) (4.28.0.dev0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from peft) (5.9.4)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.9/dist-packages (from peft) (1.13.1+cu116)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from peft) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.13.0->peft) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (3.10.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (2022.10.31)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (0.13.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (2.27.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->peft) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->peft) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->peft) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->peft) (1.26.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.10.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "![[ -d /content/simple-llama-finetuner ]] \\\n",
        "  || git clone https://github.com/lxe/simple-llama-finetuner.git /content/simple-llama-finetuner\n",
        "!cd /content/simple-llama-finetuner && git pull && pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdQC24QIFCqe",
        "outputId": "e1b9d246-e039-416b-c94d-c502f6ab8ccd"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/transformers.git (from -r requirements.txt (line 4))\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-fz0ag_sk\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-fz0ag_sk\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 5fd4e3c87c685fba2dd9615be62131748a8b5ee3\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/huggingface/peft.git (from -r requirements.txt (line 7))\n",
            "  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-s3zse148\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-s3zse148\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/simple-llama-finetuner && python main.py --share"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itQGznMQFHXQ",
        "outputId": "3714af6e-6c70-4295-b7fa-dc713642f3b8"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/httpx/__init__.py\", line 2, in <module>\n",
            "    from ._api import delete, get, head, options, patch, post, put, request, stream\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/httpx/_api.py\", line 4, in <module>\n",
            "    from ._client import Client\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/httpx/_client.py\", line 9, in <module>\n",
            "    from ._auth import Auth, BasicAuth, FunctionAuth\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/httpx/_auth.py\", line 10, in <module>\n",
            "    from ._models import Request, Response\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/httpx/_models.py\", line 44, in <module>\n",
            "    from ._urls import URL\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/httpx/_urls.py\", line 5, in <module>\n",
            "    import rfc3986\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/rfc3986/__init__.py\", line 25, in <module>\n",
            "    from .api import iri_reference\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/rfc3986/api.py\", line 22, in <module>\n",
            "    from .iri import IRIReference\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/rfc3986/iri.py\", line 21, in <module>\n",
            "    from . import misc\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/rfc3986/misc.py\", line 63, in <module>\n",
            "    HOST_MATCHER = re.compile(\"^\" + abnf_regexp.HOST_RE + \"$\")\n",
            "  File \"/usr/lib/python3.9/re.py\", line 252, in compile\n",
            "    return _compile(pattern, flags)\n",
            "  File \"/usr/lib/python3.9/re.py\", line 304, in _compile\n",
            "    p = sre_compile.compile(pattern, flags)\n",
            "  File \"/usr/lib/python3.9/sre_compile.py\", line 792, in compile\n",
            "    code = _code(p, flags)\n",
            "  File \"/usr/lib/python3.9/sre_compile.py\", line 631, in _code\n",
            "    _compile(code, p.data, flags)\n",
            "  File \"/usr/lib/python3.9/sre_compile.py\", line 184, in _compile\n",
            "    _compile(code, p, _combine_flags(flags, add_flags, del_flags))\n",
            "  File \"/usr/lib/python3.9/sre_compile.py\", line 225, in _compile\n",
            "    _compile(code, av, flags)\n",
            "  File \"/usr/lib/python3.9/sre_compile.py\", line 184, in _compile\n",
            "    _compile(code, p, _combine_flags(flags, add_flags, del_flags))\n",
            "  File \"/usr/lib/python3.9/sre_compile.py\", line 225, in _compile\n",
            "    _compile(code, av, flags)\n",
            "  File \"/usr/lib/python3.9/sre_compile.py\", line 184, in _compile\n",
            "    _compile(code, p, _combine_flags(flags, add_flags, del_flags))\n",
            "  File \"/usr/lib/python3.9/sre_compile.py\", line 225, in _compile\n",
            "    _compile(code, av, flags)\n",
            "  File \"/usr/lib/python3.9/sre_compile.py\", line 184, in _compile\n",
            "    _compile(code, p, _combine_flags(flags, add_flags, del_flags))\n",
            "  File \"/usr/lib/python3.9/sre_compile.py\", line 172, in _compile\n",
            "    _compile(code, av[2], flags)\n",
            "  File \"/usr/lib/python3.9/sre_compile.py\", line 184, in _compile\n",
            "    _compile(code, p, _combine_flags(flags, add_flags, del_flags))\n",
            "  File \"/usr/lib/python3.9/sre_compile.py\", line 172, in _compile\n",
            "    _compile(code, av[2], flags)\n",
            "  File \"/usr/lib/python3.9/sre_compile.py\", line 184, in _compile\n",
            "    _compile(code, p, _combine_flags(flags, add_flags, del_flags))\n",
            "  File \"/usr/lib/python3.9/sre_compile.py\", line 164, in _compile\n",
            "    _compile(code, av[2], flags)\n",
            "  File \"/usr/lib/python3.9/sre_compile.py\", line 136, in _compile\n",
            "    charset, hascased = _optimize_charset(av, iscased, tolower, fixes)\n",
            "  File \"/usr/lib/python3.9/sre_compile.py\", line 388, in _optimize_charset\n",
            "    data = _mk_bitmap(charmap)\n",
            "  File \"/usr/lib/python3.9/sre_compile.py\", line 437, in _mk_bitmap\n",
            "    return [_int(s[i - _CODEBITS: i], 2)\n",
            "  File \"/usr/lib/python3.9/sre_compile.py\", line 437, in <listcomp>\n",
            "    return [_int(s[i - _CODEBITS: i], 2)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/simple-llama-finetuner/main.py\", line 5, in <module>\n",
            "    import gradio as gr\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/gradio/__init__.py\", line 3, in <module>\n",
            "    import gradio.components as components\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/gradio/components.py\", line 32, in <module>\n",
            "    from gradio import media_data, processing_utils, utils\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/gradio/processing_utils.py\", line 25, in <module>\n",
            "    from gradio import utils\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/gradio/utils.py\", line 41, in <module>\n",
            "    import httpx\n",
            "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 161, in __exit__\n",
            "  File \"<frozen importlib._bootstrap>\", line 114, in release\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import random\n",
        "import torch\n",
        "import gradio as gr\n",
        "import transformers\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer, GenerationConfig\n",
        "from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model, PeftModel\n",
        "\n",
        "model = None\n",
        "tokenizer = None\n",
        "peft_model = None\n",
        "\n",
        "def random_hyphenated_word():\n",
        "    word_list = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig']\n",
        "    word1 = random.choice(word_list)\n",
        "    word2 = random.choice(word_list)\n",
        "    return word1 + '-' + word2\n",
        "\n",
        "def maybe_load_models():\n",
        "    global model\n",
        "    global tokenizer\n",
        "\n",
        "    if model is None:\n",
        "        model = LlamaForCausalLM.from_pretrained(\n",
        "            \"decapoda-research/llama-7b-hf\",\n",
        "            load_in_8bit=True,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "\n",
        "    if tokenizer is None:\n",
        "        tokenizer = LlamaTokenizer.from_pretrained(\n",
        "            \"decapoda-research/llama-7b-hf\",\n",
        "        )\n",
        "\n",
        "def reset_models():\n",
        "    global model\n",
        "    global tokenizer\n",
        "\n",
        "    del model\n",
        "    del tokenizer\n",
        "\n",
        "    model = None\n",
        "    tokenizer = None\n",
        "\n",
        "def generate_text(\n",
        "    model_name, \n",
        "    text, \n",
        "    temperature, \n",
        "    top_p, \n",
        "    top_k, \n",
        "    repeat_penalty,\n",
        "    max_new_tokens,\n",
        "    progress=gr.Progress(track_tqdm=True)\n",
        "):\n",
        "    global model\n",
        "    global tokenizer\n",
        "\n",
        "    maybe_load_models()\n",
        "\n",
        "    tokenizer.pad_token_id = 0\n",
        "\n",
        "    if model_name and model_name != \"None\":\n",
        "        model = PeftModel.from_pretrained(\n",
        "            model, model_name,\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "\n",
        "    # llama_config = transformers.LlamaConfig()\n",
        "    # print(llama_config)\n",
        "\n",
        "    stopping_criteria_list = transformers.StoppingCriteriaList()\n",
        "    generation_config = GenerationConfig(\n",
        "        # Whether to use greedy decoding. If set to False,\n",
        "        do_sample=True,\n",
        "\n",
        "        # Controls the 'temperature' of the softmax distribution during sampling.\n",
        "        # Higher values (e.g., 1.0) make the model generate more diverse and random outputs, \n",
        "        # while lower values (e.g., 0.1) make it more deterministic and \n",
        "        # focused on the highest probability tokens.\n",
        "        temperature=temperature,  \n",
        "\n",
        "        # Sets the nucleus sampling threshold. In nucleus sampling, \n",
        "        # only the tokens whose cumulative probability exceeds 'top_p' are considered \n",
        "        # for sampling. This technique helps to reduce the number of low probability \n",
        "        # tokens considered during sampling, which can lead to more diverse and coherent outputs.\n",
        "        top_p=top_p,  \n",
        "\n",
        "        # Sets the number of top tokens to consider during sampling. \n",
        "        # In top-k sampling, only the 'top_k' tokens with the highest probabilities \n",
        "        # are considered for sampling. This method can lead to more focused and coherent \n",
        "        # outputs by reducing the impact of low probability tokens.\n",
        "        top_k=top_k,  \n",
        "\n",
        "        # Applies a penalty to the probability of tokens that have already been generated, \n",
        "        # discouraging the model from repeating the same words or phrases. The penalty is\n",
        "        # applied by dividing the token probability by a factor based on the number of times \n",
        "        # the token has appeared in the generated text.\n",
        "        repeat_penalty=repeat_penalty,\n",
        "\n",
        "        # Limits the maximum number of tokens generated in a single iteration. \n",
        "        # This can be useful to control the length of generated text, especially in tasks \n",
        "        # like text summarization or translation, where the output should not be excessively long.\n",
        "        max_new_tokens=max_new_tokens,  \n",
        "\n",
        "        # typical_p=1,\n",
        "        # stopping_criteria=stopping_criteria_list,\n",
        "        # eos_token_id=llama_config.eos_token_id,\n",
        "        # pad_token_id=llama_config.eos_token_id\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generation_output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=torch.ones_like(input_ids),\n",
        "            generation_config=generation_config,\n",
        "            # return_dict_in_generate=True,\n",
        "            # output_scores=True,\n",
        "            # eos_token_id=[tokenizer.eos_token_id],\n",
        "            use_cache=True,\n",
        "        )[0].cuda()\n",
        "\n",
        "    output_text = tokenizer.decode(generation_output)\n",
        "    return output_text.strip()\n",
        "\n",
        "def tokenize_and_train(\n",
        "    training_text,\n",
        "    max_seq_length,\n",
        "    micro_batch_size,\n",
        "    gradient_accumulation_steps,\n",
        "    epochs,\n",
        "    learning_rate,\n",
        "    lora_r,\n",
        "    lora_alpha,\n",
        "    lora_dropout,\n",
        "    model_name,\n",
        "    progress=gr.Progress(track_tqdm=True)\n",
        "):\n",
        "    global model\n",
        "    global tokenizer\n",
        "\n",
        "    reset_models()\n",
        "    maybe_load_models()\n",
        "\n",
        "    tokenizer.pad_token_id = 0\n",
        "\n",
        "    paragraphs = training_text.split(\"\\n\\n\\n\")\n",
        "    print(\"Number of samples: \" + str(len(paragraphs)))\n",
        "        \n",
        "    def tokenize(item):\n",
        "        result = tokenizer(\n",
        "            item[\"text\"],\n",
        "            truncation=True,\n",
        "            max_length=max_seq_length,\n",
        "            padding=\"max_length\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": result[\"input_ids\"][:-1],\n",
        "            \"attention_mask\": result[\"attention_mask\"][:-1],\n",
        "        }\n",
        "\n",
        "    def to_dict(text):\n",
        "        return {\"text\": text}\n",
        "\n",
        "    paragraphs = [to_dict(x) for x in paragraphs]\n",
        "    data = Dataset.from_list(paragraphs)\n",
        "    data = data.shuffle().map(lambda x: tokenize(x))\n",
        "\n",
        "    model = prepare_model_for_int8_training(model)\n",
        "\n",
        "    model = get_peft_model(model, LoraConfig(\n",
        "        r=lora_r,\n",
        "        lora_alpha=lora_alpha,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        lora_dropout=lora_dropout,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    ))\n",
        "\n",
        "    output_dir = f\"lora-{model_name}\"\n",
        "\n",
        "    print(\"Training...\")\n",
        "\n",
        "    training_args = transformers.TrainingArguments(\n",
        "        # Set the batch size for training on each device (GPU, CPU, or TPU).\n",
        "        per_device_train_batch_size=micro_batch_size, \n",
        "\n",
        "        # Number of steps for gradient accumulation. This is useful when the total \n",
        "        # batch size is too large to fit in GPU memory. The effective batch size \n",
        "        # will be the product of 'per_device_train_batch_size' and 'gradient_accumulation_steps'.\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,  \n",
        "\n",
        "        # Number of warmup steps for the learning rate scheduler. During these steps, \n",
        "        # the learning rate increases linearly from 0 to its initial value. Warmup helps\n",
        "        #  to reduce the risk of very large gradients at the beginning of training, \n",
        "        # which could destabilize the model.\n",
        "        # warmup_steps=100, \n",
        "\n",
        "        # The total number of training steps. The training process will end once this \n",
        "        # number is reached, even if not all the training epochs are completed.\n",
        "        # max_steps=1500, \n",
        "\n",
        "        # The total number of epochs (complete passes through the training data) \n",
        "        # to perform during the training process.\n",
        "        num_train_epochs=epochs,  \n",
        "\n",
        "        # The initial learning rate to be used during training.\n",
        "        learning_rate=learning_rate, \n",
        "\n",
        "        # Enables mixed precision training using 16-bit floating point numbers (FP16). \n",
        "        # This can speed up training and reduce GPU memory consumption without \n",
        "        # sacrificing too much model accuracy.\n",
        "        fp16=True,  \n",
        "\n",
        "        # The frequency (in terms of steps) of logging training metrics and statistics \n",
        "        # like loss, learning rate, etc. In this case, it logs after every 20 steps.\n",
        "        logging_steps=20, \n",
        "\n",
        "        # The output directory where the trained model, checkpoints, \n",
        "        # and other training artifacts will be saved.\n",
        "        output_dir=output_dir, \n",
        "\n",
        "        # The maximum number of checkpoints to keep. When this limit is reached, \n",
        "        # the oldest checkpoint will be deleted to save a new one. In this case, \n",
        "        # a maximum of 3 checkpoints will be kept.\n",
        "        save_total_limit=3,  \n",
        "    )\n",
        "\n",
        "\n",
        "    trainer = transformers.Trainer(\n",
        "        # The pre-trained model that you want to fine-tune or train from scratch. \n",
        "        # 'model' should be an instance of a Hugging Face Transformer model, such as BERT, GPT-2, T5, etc.\n",
        "        model=model, \n",
        "\n",
        "        # The dataset to be used for training. 'data' should be a PyTorch Dataset or \n",
        "        # a compatible format, containing the input samples and labels or masks (if required).\n",
        "        train_dataset=data, \n",
        "\n",
        "        # The TrainingArguments instance created earlier, which contains various \n",
        "        # hyperparameters and configurations for the training process.\n",
        "        args=training_args, \n",
        "\n",
        "        # A callable that takes a batch of samples and returns a batch of inputs for the model. \n",
        "        # This is used to prepare the input samples for training by batching, padding, and possibly masking.\n",
        "        data_collator=transformers.DataCollatorForLanguageModeling( \n",
        "            tokenizer,  \n",
        "            # Whether to use masked language modeling (MLM) during training. \n",
        "            # MLM is a training technique used in models like BERT, where some tokens in the \n",
        "            # input are replaced by a mask token, and the model tries to predict the \n",
        "            # original tokens. In this case, MLM is set to False, indicating that it will not be used.\n",
        "            mlm=False, \n",
        "        ),\n",
        "    )\n",
        "\n",
        "    result = trainer.train(resume_from_checkpoint=False)\n",
        "\n",
        "    model.save_pretrained(output_dir)\n",
        "    \n",
        "    reset_models()\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "with gr.Blocks(\n",
        "    css=\"#refresh-button { max-width: 32px }\", \n",
        "    title=\"Simple LLaMA Finetuner\") as demo:\n",
        "    \n",
        "    with gr.Tab(\"Finetuning\"):\n",
        "\n",
        "        with gr.Column():\n",
        "            training_text = gr.Textbox(lines=12, label=\"Training Data\", info=\"Each sequence must be separated by a double newline\")\n",
        "\n",
        "            max_seq_length = gr.Slider(\n",
        "                minimum=1, maximum=4096, value=512,\n",
        "                label=\"Max Sequence Length\", \n",
        "                info=\"The maximum length of each sample text sequence. Sequences longer than this will be truncated.\"\n",
        "            )\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                micro_batch_size = gr.Slider(\n",
        "                    minimum=1, maximum=100, value=1, \n",
        "                    label=\"Micro Batch Size\", \n",
        "                    info=\"The number of examples in each mini-batch for gradient computation. A smaller micro_batch_size reduces memory usage but may increase training time.\"\n",
        "                )\n",
        "\n",
        "                gradient_accumulation_steps = gr.Slider(\n",
        "                    minimum=1, maximum=10, value=1, \n",
        "                    label=\"Gradient Accumulation Steps\", \n",
        "                    info=\"The number of steps to accumulate gradients before updating model parameters. This can be used to simulate a larger effective batch size without increasing memory usage.\"\n",
        "                )\n",
        "\n",
        "                epochs = gr.Slider(\n",
        "                    minimum=1, maximum=100, value=1, \n",
        "                    label=\"Epochs\",\n",
        "                    info=\"The number of times to iterate over the entire training dataset. A larger number of epochs may improve model performance but also increase the risk of overfitting.\")\n",
        "\n",
        "                learning_rate = gr.Slider(\n",
        "                    minimum=0.00001, maximum=0.01, value=3e-4,\n",
        "                    label=\"Learning Rate\",\n",
        "                    info=\"The initial learning rate for the optimizer. A higher learning rate may speed up convergence but also cause instability or divergence. A lower learning rate may require more steps to reach optimal performance but also avoid overshooting or oscillating around local minima.\"\n",
        "                )\n",
        "\n",
        "            with gr.Column():\n",
        "                lora_r = gr.Slider(\n",
        "                    minimum=1, maximum=16, value=8, \n",
        "                    label=\"LoRA R\",\n",
        "                    info=\"The rank parameter for LoRA, which controls the dimensionality of the rank decomposition matrices. A larger lora_r increases the expressiveness and flexibility of LoRA but also increases the number of trainable parameters and memory usage.\"\n",
        "                )\n",
        "\n",
        "                lora_alpha = gr.Slider(\n",
        "                    minimum=1, maximum=128, value=16, \n",
        "                    label=\"LoRA Alpha\",\n",
        "                    info=\"The scaling parameter for LoRA, which controls how much LoRA affects the original pre-trained model weights. A larger lora_alpha amplifies the impact of LoRA but may also distort or override the pre-trained knowledge.\"\n",
        "                )\n",
        "                \n",
        "                lora_dropout = gr.Slider(\n",
        "                    minimum=0, maximum=1, value=0.01,\n",
        "                    label=\"LoRA Dropout\",\n",
        "                    info=\"The dropout probability for LoRA, which controls the fraction of LoRA parameters that are set to zero during training. A larger lora_dropout increases the regularization effect of LoRA but also increases the risk of underfitting.\"\n",
        "                )\n",
        "\n",
        "                with gr.Column():\n",
        "                    model_name = gr.Textbox(\n",
        "                        lines=1, label=\"LoRA Model Name\", value=random_hyphenated_word()\n",
        "                    )\n",
        "\n",
        "                    with gr.Row():\n",
        "                        train_btn = gr.Button(\n",
        "                            \"Train\", variant=\"primary\", label=\"Train\", \n",
        "                        )\n",
        "\n",
        "                        abort_button = gr.Button(\n",
        "                            \"Abort\", label=\"Abort\", \n",
        "                        )\n",
        "    \n",
        "        output_text = gr.Text(\"Training Status\")\n",
        "\n",
        "        train_progress = train_btn.click(\n",
        "            fn=tokenize_and_train,\n",
        "            inputs=[\n",
        "                training_text,\n",
        "                max_seq_length,\n",
        "                micro_batch_size,\n",
        "                gradient_accumulation_steps,\n",
        "                epochs,\n",
        "                learning_rate,\n",
        "                lora_r,\n",
        "                lora_alpha,\n",
        "                lora_dropout,\n",
        "                model_name\n",
        "            ],\n",
        "            outputs=output_text\n",
        "        )\n",
        "\n",
        "        abort_button.click(None, None, None, cancels=[train_progress])\n",
        "\n",
        "    with gr.Tab(\"Inference\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                with gr.Row():\n",
        "                        lora_model = gr.Dropdown(\n",
        "                            label=\"LoRA Model\",\n",
        "                        )\n",
        "                        refresh_models_list = gr.Button(\n",
        "                            \"Reload Models\",\n",
        "                            elem_id=\"refresh-button\"\n",
        "                        )\n",
        "                inference_text = gr.Textbox(lines=7, label=\"Input Text\")   \n",
        "            inference_output = gr.Textbox(lines=12, label=\"Output Text\")\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                #  temperature, top_p, top_k, repeat_penalty, max_new_tokens\n",
        "                temperature = gr.Slider(\n",
        "                    minimum=0, maximum=1.99, value=0.7, step=0.01,\n",
        "                    label=\"Temperature\",\n",
        "                    info=\"Controls the 'temperature' of the softmax distribution during sampling. Higher values (e.g., 1.0) make the model generate more diverse and random outputs, while lower values (e.g., 0.1) make it more deterministic and focused on the highest probability tokens.\"\n",
        "                )\n",
        "\n",
        "                top_p = gr.Slider(\n",
        "                    minimum=0, maximum=1, value=0.2, step=0.01,\n",
        "                    label=\"Top P\",\n",
        "                    info=\"Sets the nucleus sampling threshold. In nucleus sampling, only the tokens whose cumulative probability exceeds 'top_p' are considered  for sampling. This technique helps to reduce the number of low probability tokens considered during sampling, which can lead to more diverse and coherent outputs.\"\n",
        "                )\n",
        "\n",
        "                top_k = gr.Slider(\n",
        "                    minimum=0, maximum=200, value=50, step=1,\n",
        "                    label=\"Top K\",\n",
        "                    info=\"Sets the number of top tokens to consider during sampling. In top-k sampling, only the 'top_k' tokens with the highest probabilities are considered for sampling. This method can lead to more focused and coherent outputs by reducing the impact of low probability tokens.\"\n",
        "                )\n",
        "\n",
        "                repeat_penalty = gr.Slider(\n",
        "                    minimum=0, maximum=1.5, value=0.8, step=0.01,\n",
        "                    label=\"Repeat Penalty\",\n",
        "                    info=\"Applies a penalty to the probability of tokens that have already been generated, discouraging the model from repeating the same words or phrases. The penalty is applied by dividing the token probability by a factor based on the number of times the token has appeared in the generated text.\"\n",
        "                )\n",
        "\n",
        "                max_new_tokens = gr.Slider(\n",
        "                    minimum=0, maximum=4096, value=50, step=1,\n",
        "                    label=\"Max New Tokens\",\n",
        "                    info=\"Limits the maximum number of tokens generated in a single iteration.\"\n",
        "                )\n",
        "            with gr.Column():\n",
        "                with gr.Row():\n",
        "                    generate_btn = gr.Button(\n",
        "                        \"Generate\", variant=\"primary\", label=\"Generate\", \n",
        "                    )\n",
        "\n",
        "                    inference_abort_button = gr.Button(\n",
        "                        \"Abort\", label=\"Abort\", \n",
        "                    )\n",
        "            \n",
        "        inference_progress = generate_btn.click(\n",
        "            fn=generate_text,\n",
        "            inputs=[\n",
        "                lora_model,\n",
        "                inference_text,\n",
        "                temperature,\n",
        "                top_p,\n",
        "                top_k,\n",
        "                repeat_penalty,\n",
        "                max_new_tokens\n",
        "            ],\n",
        "            outputs=inference_output,\n",
        "        )\n",
        "\n",
        "        lora_model.change(\n",
        "            fn=reset_models\n",
        "        )\n",
        "\n",
        "        def update_models_list():\n",
        "            return gr.Dropdown.update(choices=[\"None\"] + [\n",
        "                d for d in os.listdir() if os.path.isdir(d) and d.startswith('lora-')\n",
        "            ], value=\"None\")\n",
        "\n",
        "        refresh_models_list.click(\n",
        "            update_models_list,  \n",
        "            inputs=None, \n",
        "            outputs=lora_model,\n",
        "        )\n",
        "\n",
        "    \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Simple LLaMA Finetuner\")\n",
        "    parser.add_argument(\"-s\", \"--share\", action=\"store_true\", help=\"Enable sharing of the Gradio interface\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    demo.queue().launch(share=args.share)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "aHShhrilEVte",
        "outputId": "f23084ce-aeec-40cf-b005-6a65c4cc34bb"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-105-79547b1c7651>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLlamaForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLlamaTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerationConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprepare_model_for_int8_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoraConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Dataset' from 'datasets' (unknown location)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path ='/content/datasets/actor_training_data.json'"
      ],
      "metadata": {
        "id": "0BcuoMN95gxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import logging\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, Dict, Sequence\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import Trainer\n",
        "import dataclasses\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import io\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "from typing import Optional, Sequence, Union\n",
        "\n",
        "import openai\n",
        "import tqdm\n",
        "from openai import openai_object\n",
        "import copy\n",
        "\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "\n",
        "def _make_r_io_base(f, mode: str):\n",
        "    if not isinstance(f, io.IOBase):\n",
        "        f = open(f, mode=mode)\n",
        "    return f\n",
        "\n",
        "def jload(f, mode=\"r\"):\n",
        "    \"\"\"Load a .json file into a dictionary.\"\"\"\n",
        "    f = _make_r_io_base(f, mode)\n",
        "    jdict = json.load(f)\n",
        "    f.close()\n",
        "    return jdict\n",
        "\n",
        "def smart_tokenizer_and_embedding_resize(\n",
        "    special_tokens_dict: Dict,\n",
        "    tokenizer: tokenizer,\n",
        "    model: model,\n",
        "):\n",
        "    \"\"\"Resize tokenizer and embedding.\n",
        "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
        "    \"\"\"\n",
        "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if num_new_tokens > 0:\n",
        "        input_embeddings = model.get_input_embeddings().weight.data\n",
        "        output_embeddings = model.get_output_embeddings().weight.data\n",
        "\n",
        "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "\n",
        "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
        "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
        "\n",
        "\n",
        "def _tokenize_fn(strings: Sequence[str], tokenizer: tokenizer) -> Dict:\n",
        "    \"\"\"Tokenize a list of strings.\"\"\"\n",
        "    tokenized_list = [\n",
        "        tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"longest\",\n",
        "            max_length=tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        for text in strings\n",
        "    ]\n",
        "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "    input_ids_lens = labels_lens = [\n",
        "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
        "    ]\n",
        "    return dict(\n",
        "        input_ids=input_ids,\n",
        "        labels=labels,\n",
        "        input_ids_lens=input_ids_lens,\n",
        "        labels_lens=labels_lens,\n",
        "    )\n",
        "\n",
        "\n",
        "def preprocess(\n",
        "    sources: Sequence[str],\n",
        "    targets: Sequence[str],\n",
        "    tokenizer: tokenizer,\n",
        ") -> Dict:\n",
        "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
        "    examples = [s + t for s, t in zip(sources, targets)]\n",
        "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
        "    input_ids = examples_tokenized[\"input_ids\"]\n",
        "    labels = copy.deepcopy(input_ids)\n",
        "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "        label[:source_len] = IGNORE_INDEX\n",
        "    return dict(input_ids=input_ids, labels=labels)\n",
        "\n",
        "\n",
        "class SupervisedDataset(Dataset):\n",
        "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
        "    def __init__(self, data_path: str, tokenizer: tokenizer) -> None:\n",
        "        logging.warning(\"Loading data...\")\n",
        "        self.data_path = data_path\n",
        "        with open(data_path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "            self.data =  [str(d[\"user_input\"]) + \"\\n\" + str(d[\"completion\"]) for d in data]\n",
        "        self.len = len(self.data)\n",
        "        sources = [f\"{example['user_input']}{tokenizer.eos_token}\" for example in data]\n",
        "        targets = [f\"{example['completion']}{tokenizer.eos_token}\" for example in data]\n",
        "        data_dict = preprocess(sources, targets, tokenizer)\n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "    \n",
        "    def __len__(self):\n",
        "        #print(f\"comparing length with chatlllama definition: chatllama: {self.data[idx]} v.s. stanford version{self.input_ids}\")\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
        "\n",
        "\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
        "    def __init__(self, tokenizer: tokenizer) -> None:\n",
        "        logging.warning(\"data collator for supervised learning...\")\n",
        "        self.tokenizer=tokenizer\n",
        "    \n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )\n",
        "\n",
        "def make_supervised_data_module(tokenizer: tokenizer, data_path) -> Dict:\n",
        "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
        "    train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_path)\n",
        "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
        "    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)"
      ],
      "metadata": {
        "id": "aahblvHVp2c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\"working_dir\")\n",
        "training_args = training_args.set_lr_scheduler(name=\"cosine\", warmup_ratio=0.05)\n",
        "training_args.fp16 = True\n",
        "training_args.auto_find_batch_size=False\n",
        "training_args.per_device_train_batch_size=1\n",
        "training_args.per_device_train_batch_size=1\n",
        "training_args.output_dir='./output'\n",
        "training_args.num_train_epochs = 3 \n",
        "training_args.gradient_accumulation_steps = 8 \n",
        "training_args.evaluation_strategy=\"no\" \n",
        "training_args.save_strategy=\"steps\" \n",
        "training_args.save_steps=2000 \n",
        "training_args.save_total_limit=1 \n",
        "training_args.learning_rate=2e-5 \n",
        "training_args.weight_decay=0. \n",
        "training_args.warmup_ratio=0.03 \n",
        "training_args.lr_scheduler_type=\"cosine\" \n",
        "training_args.logging_steps=1  \n",
        "training_args.fsdp=\"full_shard auto_wrap\"\n",
        "training_args.fsdp_transformer_layer_cls_to_wrap='LLaMADecoderLayer' \n",
        "training_args.tf32=True\n",
        "training_args"
      ],
      "metadata": {
        "id": "FucYlJbQ7Ayo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
        "from torch.distributed.fsdp.fully_sharded_data_parallel import (\n",
        "    CPUOffload,\n",
        "    BackwardPrefetch,\n",
        ")\n",
        "from torch.distributed.fsdp.wrap import (\n",
        "    size_based_auto_wrap_policy,\n",
        "    enable_wrap,\n",
        "    wrap,\n",
        ")\n",
        "\n",
        "def train(rank, model, tokenizer):\n",
        "    if tokenizer.pad_token is None:\n",
        "        smart_tokenizer_and_embedding_resize(\n",
        "            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
        "            tokenizer=tokenizer,\n",
        "            model=model,\n",
        "        )\n",
        "\n",
        "    tokenizer.add_special_tokens(\n",
        "        {\n",
        "             \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "             \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "             \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "         }\n",
        "     )\n",
        "    data_module = make_supervised_data_module(tokenizer=tokenizer, data_path=data_path)\n",
        "    my_auto_wrap_policy = functools.partial(\n",
        "        size_based_auto_wrap_policy, min_num_params=100\n",
        "    )\n",
        "    torch.cuda.set_device(rank)\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=200, num_training_steps=-1\n",
        "    )\n",
        "    train_loader = DataLoader(data_module, batch_size=2, shuffle=True)\n",
        "    for i in train_loader:\n",
        "      print(i)\n",
        "    model = model.to(rank)\n",
        "    #model = FSDP(model)\n",
        "    ddp_loss = torch.zeros(2).to(rank)\n",
        "    for epoch in range(3):\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = train_loader['input_ids'].to(rank)\n",
        "            attention_mask = batch['attention_mask'].to(rank)\n",
        "            labels = batch['labels'].to(rank)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs[0]\n",
        "            loss.backward()\n",
        "            scheduler.step()\n",
        "            ddp_loss[0] += loss.item()\n",
        "            ddp_loss[1] += len(data)\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n",
        "    if rank == 0:\n",
        "        print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, ddp_loss[0] / ddp_loss[1]))\n",
        "\n"
      ],
      "metadata": {
        "id": "V9ohcuy46ZHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(0, model, tokenizer)"
      ],
      "metadata": {
        "id": "ZlbfaokCecqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "os.environ['CUDA_VISIBLE_DEVICES']='0, 1, 2, 3, 4, 5, 6, 7, 8'\n",
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
      ],
      "metadata": {
        "id": "nl0oS2QmRuin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train(model, tokenizer)"
      ],
      "metadata": {
        "id": "ckc6RjnCKukt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install libnvinfer_plugin.so.8"
      ],
      "metadata": {
        "id": "1l6tdTyLtblG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!torchrun --nproc_per_node 1 /content/drive/MyDrive/train.py "
      ],
      "metadata": {
        "id": "fYIc9GM49PmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!deepspeed --num_gpus=0 /content/drive/MyDrive/train.py --deepspeed /content/drive/MyDrive/artifacts/config/ds_config.json"
      ],
      "metadata": {
        "id": "WuIA7nfn7VgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert(False)\n",
        "import copy\n",
        "import logging\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, Dict, Sequence\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import Trainer\n",
        "import dataclasses\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import io\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "from typing import Optional, Sequence, Union\n",
        "\n",
        "import openai\n",
        "import tqdm\n",
        "from openai import openai_object\n",
        "import copy\n",
        "\n",
        "#import utils\n",
        "\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "PROMPT_DICT = [{\n",
        "    \"user_input\":\"solidity program\",\n",
        "    \"completion\": \"invariants\"}]\n",
        "\n",
        "'''\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    model_name_or_path:Optional[str] = field(default='/content/drive/MyDrive/experiments/llama/LLaMA/7B')\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataArguments:\n",
        "    data_path: str = field(default=None, metadata={\"help\": \"/content/datasets\"})\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainingArguments(transformers.TrainingArguments):\n",
        "    cache_dir: Optional[str] = field(default=None)\n",
        "    optim: str = field(default=\"adamw_torch\")\n",
        "    model_max_length: int = field(\n",
        "        default=512,\n",
        "        metadata={\"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"},\n",
        "    )\n",
        "'''\n",
        "def _make_r_io_base(f, mode: str):\n",
        "    if not isinstance(f, io.IOBase):\n",
        "        f = open(f, mode=mode)\n",
        "    return f\n",
        "\n",
        "def jload(f, mode=\"r\"):\n",
        "    \"\"\"Load a .json file into a dictionary.\"\"\"\n",
        "    f = _make_r_io_base(f, mode)\n",
        "    jdict = json.load(f)\n",
        "    f.close()\n",
        "    return jdict\n",
        "\n",
        "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
        "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
        "    state_dict = trainer.model.state_dict()\n",
        "    if trainer.args.should_save:\n",
        "        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
        "        del state_dict\n",
        "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n",
        "\n",
        "\n",
        "def smart_tokenizer_and_embedding_resize(\n",
        "    special_tokens_dict: Dict,\n",
        "    tokenizer: transformers.PreTrainedTokenizer,\n",
        "    model: transformers.PreTrainedModel,\n",
        "):\n",
        "    \"\"\"Resize tokenizer and embedding.\n",
        "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
        "    \"\"\"\n",
        "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if num_new_tokens > 0:\n",
        "        input_embeddings = model.get_input_embeddings().weight.data\n",
        "        output_embeddings = model.get_output_embeddings().weight.data\n",
        "\n",
        "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "\n",
        "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
        "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
        "\n",
        "\n",
        "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
        "    \"\"\"Tokenize a list of strings.\"\"\"\n",
        "    tokenized_list = [\n",
        "        tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"longest\",\n",
        "            max_length=tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        for text in strings\n",
        "    ]\n",
        "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "    input_ids_lens = labels_lens = [\n",
        "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
        "    ]\n",
        "    return dict(\n",
        "        input_ids=input_ids,\n",
        "        labels=labels,\n",
        "        input_ids_lens=input_ids_lens,\n",
        "        labels_lens=labels_lens,\n",
        "    )\n",
        "\n",
        "\n",
        "def preprocess(\n",
        "    sources: Sequence[str],\n",
        "    targets: Sequence[str],\n",
        "    tokenizer: transformers.PreTrainedTokenizer,\n",
        ") -> Dict:\n",
        "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
        "    examples = [s + t for s, t in zip(sources, targets)]\n",
        "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
        "    input_ids = examples_tokenized[\"input_ids\"]\n",
        "    labels = copy.deepcopy(input_ids)\n",
        "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "        label[:source_len] = IGNORE_INDEX\n",
        "    return dict(input_ids=input_ids, labels=labels)\n",
        "\n",
        "\n",
        "class SupervisedDataset(Dataset):\n",
        "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer):\n",
        "        super(SupervisedDataset, self).__init__()\n",
        "        logging.warning(\"Loading data...\")\n",
        "        list_data_dict = jload(data_path)\n",
        "\n",
        "        logging.warning(\"Formatting inputs...\")\n",
        "        prompt_input, prompt_no_input = PROMPT_DICT[\"user_input\"], PROMPT_DICT[\"completion\"]\n",
        "        sources = [\n",
        "            prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\" else prompt_no_input.format_map(example)\n",
        "            for example in list_data_dict\n",
        "        ]\n",
        "        targets = [f\"{example['output']}{tokenizer.eos_token}\" for example in list_data_dict]\n",
        "\n",
        "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
        "        data_dict = preprocess(sources, targets, tokenizer)\n",
        "\n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )\n",
        "\n",
        "'''\n",
        "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer, data_args) -> Dict:\n",
        "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
        "    train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path)\n",
        "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
        "    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)\n",
        "\n",
        "\n",
        "def train():\n",
        "    parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
        "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
        "\n",
        "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        cache_dir=training_args.cache_dir,\n",
        "    )\n",
        "\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        cache_dir=training_args.cache_dir,\n",
        "        model_max_length=training_args.model_max_length,\n",
        "        padding_side=\"right\",\n",
        "        use_fast=False,\n",
        "    )\n",
        "    if tokenizer.pad_token is None:\n",
        "        smart_tokenizer_and_embedding_resize(\n",
        "            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
        "            tokenizer=tokenizer,\n",
        "            model=model,\n",
        "        )\n",
        "    if \"llama\" in model_args.model_name_or_path:\n",
        "        tokenizer.add_special_tokens(\n",
        "            {\n",
        "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n",
        "    trainer = Trainer(model=model, tokenizer=tokenizer, args=training_args, **data_module)\n",
        "    trainer.train()\n",
        "    trainer.save_state()\n",
        "    safe_save_model_for_hf_trainer(trainer=trainer, output_dir='/content/saved_model')\n",
        " \n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "#    train()"
      ],
      "metadata": {
        "id": "bfZC4kepAWMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tatsu-lab/stanford_alpaca.git"
      ],
      "metadata": {
        "id": "_hSIKVY9e7Vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chatllama-py\n",
        "!git clone https://github.com/facebookresearch/llama.git\n",
        "!pip install -r /content/llama/requirements.txt\n",
        "!pip install -e /content/llama/.\n"
      ],
      "metadata": {
        "id": "8p8DnNn4fLQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export WORLD_SIZE=1\n",
        "!export LOCAL_RANK=0\n",
        "!export RANK=0\n",
        "!export _MODEL_PARALLEL_GROUP=1"
      ],
      "metadata": {
        "id": "Q2zHLPqGzJ9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!torchrun --nproc_per_node=1 drive/MyDrive/artifacts/main.py -t ACTOR drive/MyDrive/artifacts/config/config.yaml"
      ],
      "metadata": {
        "id": "HdkGBmnnoo73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!torchrun --nproc_per_node=1 --master_port=22 /content/stanford_alpaca/train.py \\\n",
        "    --model_name_or_path '/content/drive/MyDrive/experiments/HF_Llama/llama-7b'\\\n",
        "    --data_path '/content/datasets/actor_training_data.json' \\\n",
        "    --bf16 True \\\n",
        "    --output_dir '/content/saved_model' \\\n",
        "    --num_train_epochs 3 \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --per_device_eval_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --evaluation_strategy \"no\" \\\n",
        "    --save_strategy \"steps\" \\\n",
        "    --save_steps 2000 \\\n",
        "    --save_total_limit 1 \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --weight_decay 0. \\\n",
        "    --warmup_ratio 0.03 \\\n",
        "    --lr_scheduler_type \"cosine\" \\\n",
        "    --logging_steps 1 \\\n",
        "    --fsdp \"full_shard auto_wrap\" \\\n",
        "    --fsdp_transformer_layer_cls_to_wrap 'LLaMADecoderLayer' \\\n",
        "    --tf32 True"
      ],
      "metadata": {
        "id": "OgPvq5Dvf4E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Finetuning##"
      ],
      "metadata": {
        "id": "SeKTNjbuD75U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chatllama-py"
      ],
      "metadata": {
        "id": "xqmOQKjvhhDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!zip -r /content/raw_datasets.zip /content/datasets\n",
        "#from google.colab import files\n",
        "#files.download(\"/content/raw_datasets.zip\")"
      ],
      "metadata": {
        "id": "2h5lq9EeDxZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepspeed\n",
        "!pip install mpi4py\n",
        "!pip install ninja"
      ],
      "metadata": {
        "id": "VzZ-zYLEwrOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget -O artifacts.zip https://nbllabartifacts.blob.core.windows.net/chatllama/artifacts.zip\\?sp\\=r\\&st\\=2023-03-08T14:53:24Z\\&se\\=2100-03-08T22:53:24Z\\&spr\\=https\\&sv\\=2021-06-08\\&sr\\=b\\&sig\\=jqr%2B2ZkR0SW9RjV0pDOdQ%2BDulLXLjbZ36vmNd4XxxyQ%3D\n",
        "#!unzip artifacts.zip\n"
      ],
      "metadata": {
        "id": "DrXX9phFLEm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/llama.git\n",
        "!pip install -r /content/llama/requirements.txt\n",
        "!pip install -e /content/llama/."
      ],
      "metadata": {
        "id": "X8irCLfEfULu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!deepspeed /content/artifacts/main.py -t ACTOR /content/artifacts/config/config.yaml"
      ],
      "metadata": {
        "id": "YlinPyD8mSlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPT-style finetuning: OOM error persisted##"
      ],
      "metadata": {
        "id": "1ksQlD8qhyV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "from numpy import random\n",
        "import pandas as pd\n",
        "import gspread\n",
        "#autenticating to google\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "#defining my worksheet\n",
        "worksheet = gc.open('short_invariants_for_gpt').sheet1\n",
        "#get_all_values gives a list of rows\n",
        "rows = worksheet.get_all_values()\n",
        "#Convert to a DataFrame \n",
        "cols = ['Target']\n",
        "df = pd.DataFrame(rows, columns=cols)\n",
        "df"
      ],
      "metadata": {
        "id": "SB1TLeCiD53K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_params={\n",
        "    \"MODEL\":\"llama_model\",             # model_type: t5-large\n",
        "    \"MAX_LENGTH\": 3036,  # max length of source text\n",
        "   # \"SEED\": random.randint(1000)    # randomized seeds to shuffle test set\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "Wm0QPuWiGPGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlYaKW9h4ai_"
      },
      "source": [
        "# Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vLQPGAn4v17"
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from rich.console import Console\n",
        "from tqdm import tqdm, trange\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import torch\n",
        "import csv\n",
        "\n",
        "class GPTDataSetClass(Dataset):\n",
        "  \"\"\"\n",
        "  Creating a custom dataset for reading the dataset and \n",
        "  loading it into the dataloader to pass it to the neural network for finetuning the model\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, target_label, truncate=False, \\\n",
        "               gpt2_type=model, \\\n",
        "               max_length=2048):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.target_tokens = []\n",
        "\n",
        "    for row in df['Target']:\n",
        "        self.target_tokens.append(torch.tensor(\n",
        "            self.tokenizer.encode(f\"<|{target_label}|>{row[:max_length]}<|endoftext|>\")\n",
        "            ))  \n",
        "    if truncate:\n",
        "            self.target_tokens = self.target_tokens[:20000]\n",
        "    self.length = len(self.target_tokens)   \n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.target_tokens[index]   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Accumulated batch size (since GPT2 is so big)\n",
        "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
        "    if packed_tensor is None:\n",
        "        return new_tensor, True, None\n",
        "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
        "        return packed_tensor, False, new_tensor\n",
        "    else:\n",
        "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
        "        return packed_tensor, True, None"
      ],
      "metadata": {
        "id": "gJ2ccsedzcAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    dataset, model, tokenizer,\n",
        "    batch_size=0.1, epochs=3, lr=2e-5,\n",
        "    max_seq_len=2048, warmup_steps=200,\n",
        "    gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\",\n",
        "    test_mode=False, save_model_on_epoch=False,\n",
        "):\n",
        "    acc_steps = 10\n",
        "    device=torch.device(\"cuda\")\n",
        "    model = model\n",
        "    model.train()\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
        "    )\n",
        "\n",
        "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "    loss=0\n",
        "    accumulating_batch_count = 0\n",
        "    input_tensor = None\n",
        "    loss_list = []\n",
        "    epoch_list = []\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = []\n",
        "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
        "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
        "\n",
        "            if carry_on and idx != len(train_dataloader) - 1:\n",
        "                continue\n",
        "\n",
        "            input_tensor = input_tensor.to('cuda')\n",
        "            outputs = model(input_tensor, labels=input_tensor)  \n",
        "            loss = outputs[0] \n",
        "            total_loss.append(float(loss.item()))       \n",
        "            loss.backward()\n",
        "            torch.cuda.empty_cache()                      \n",
        "            if (accumulating_batch_count % batch_size) == 0:\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                model.zero_grad()\n",
        "  \n",
        "            accumulating_batch_count += 1\n",
        "            input_tensor = None\n",
        "\n",
        "        #training_logger.add_row(str(epoch), str(np.mean(total_loss)))       \n",
        "        if save_model_on_epoch:\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
        "            )\n",
        "        loss_list.append(np.mean(total_loss))\n",
        "        epoch_list.append(epoch) \n",
        "        print(f\"for epoch {epoch} the loss is {np.mean(total_loss)}\\n\")\n",
        "   # console.print(training_logger)   \n",
        "   # plot_loss(epoch_list, loss_list)\n",
        "    return model"
      ],
      "metadata": {
        "id": "PTVyI2fzzn3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_GPT2(df, model_params):   \n",
        "  #console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
        "\n",
        "  # tokenzier for encoding the text\n",
        "  dataset = GPTDataSetClass(df['Target'], truncate=False, gpt2_type=model) \n",
        "  #Get the tokenizer and model\n",
        "  #tokenizer = GPT2Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
        " # model = GPT2LMHeadModel.from_pretrained(model_params[\"MODEL\"])  \n",
        "  trained_model = train(dataset, model, tokenizer)\n",
        "  console.log(f\"[Saving Model]...\\n\")\n",
        "  #Saving the model after training\n",
        "  path = os.path.join('/content/output', \"model_files\")\n",
        "  model.save_pretrained(path)\n",
        "  tokenizer.save_pretrained(path)\n",
        "  console.print(f\"\"\"[Model] Model saved @ {os.path.join('/content/output', \"model_files\")}\\n\"\"\")\n",
        "  \n",
        "  # logging\n",
        "  console.log(f\"[Data]: Reading Raw data...\\n\")\n",
        "\n",
        "\n",
        "  # Creation of Dataset and Dataloader\n",
        "  # Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
        "\n",
        "  console.print(f\"FULL Dataset: {df.shape}\")\n",
        "  return trained_model, tokenizer\n"
      ],
      "metadata": {
        "id": "nOHr5lM82h53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    entry_count=10,\n",
        "    entry_length=2048, #maximum number of words\n",
        "    top_p=0.8,\n",
        "    temperature=1.,\n",
        "):\n",
        "    model.eval()\n",
        "    generated_num = 0\n",
        "    generated_list = []\n",
        "\n",
        "    filter_value = -float(\"Inf\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for entry_idx in trange(entry_count):\n",
        "\n",
        "            entry_finished = False\n",
        "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "\n",
        "            for i in range(entry_length):\n",
        "                outputs = model(generated, labels=generated)\n",
        "                loss, logits = outputs[:2]\n",
        "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
        "                    ..., :-1\n",
        "                ].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                logits[:, indices_to_remove] = filter_value\n",
        "\n",
        "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
        "                generated = torch.cat((generated, next_token), dim=1)\n",
        "\n",
        "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
        "                    entry_finished = True\n",
        "\n",
        "                if entry_finished:\n",
        "\n",
        "                    generated_num = generated_num + 1\n",
        "\n",
        "                    output_list = list(generated.squeeze().numpy())\n",
        "                    output_text = tokenizer.decode(output_list)\n",
        "                    generated_list.append(output_text + '\\n')\n",
        "                    break\n",
        "            \n",
        "            if not entry_finished:\n",
        "              output_list = list(generated.squeeze().numpy())\n",
        "              output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n",
        "              generated_list.append(output_text + '\\n')\n",
        "                \n",
        "    return generated_list\n",
        "\n",
        "#Function to generate multiple sentences. Test data should be a dataframe\n",
        "def text_generation(model, tokenizer, test_data):\n",
        "  generated_code = []\n",
        "  for i in range(len(test_data)):    \n",
        "    x = generate(model.to('cpu'), tokenizer, test_data['Target'][i], entry_count=1)\n",
        "    generated_code.append(x)\n",
        "  return generated_code\n",
        "\n",
        "#Run the functions to generate the lyrics\n",
        "\n",
        "def test_fine_tuned_gpt2(model, tokenizer, df): \n",
        "    test_set = df.sample(n=1)\n",
        "    df = df.loc[~df.index.isin(test_set.index)]\n",
        "\n",
        "    #Reset the indexes\n",
        "    test_set = test_set.reset_index()\n",
        "    df = df.reset_index()\n",
        "\n",
        "    #For the test set only, keep last 20 words in a new column, then remove them from original column\n",
        "    test_set['Target'] = test_set['Target'].str.split().apply(' '.join)\n",
        "    generated_code = text_generation(model, tokenizer, test_set)\n",
        "    print(generated_code)\n"
      ],
      "metadata": {
        "id": "ujnNVivM1bMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To solve CUDA out of memory error\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "os.environ['CUDA_VISIBLE_DEVICES']='0, 1, 2, 3, 4, 5, 6, 7, 8'\n",
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
      ],
      "metadata": {
        "id": "atnMJANnJlf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model, trained_tokenizer = fine_tune_GPT2(df, model_params)"
      ],
      "metadata": {
        "id": "GtBLzx0bHiXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-4qjuS_RhdxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "import deepspeed\n",
        "import torch\n",
        "from beartype import beartype\n",
        "from beartype.typing import Tuple\n",
        "from einops import rearrange\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "local_rank, world_size = setup_model_parallel()\n",
        "            # use load_model_test for testing\n",
        "  self.model, self.tokenizer = load_model(\n",
        "   ckpt_dir=model,\n",
        "   tokenizer_path=tokenizer,\n",
        "   local_rank=local_rank,\n",
        "   world_size=world_size,\n",
        "   froze_embeddings=config.froze_embeddings,\n",
        "   use_fairscale=config.use_fairscale,\n",
        "   max_batch_size=config.batch_size,\n",
        "    )\n",
        "        elif config.model in hf_models_seq_2_seq:\n",
        "    \n",
        "            # galactica tokenizer eos_token is None\n",
        "            if self.tokenizer.eos_token is None:\n",
        "                self.tokenizer.eos_token = \"</s>\"\n",
        "                self.tokenizer.eos_token_id = 0\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                config.model,\n",
        "            )\n",
        "            self.model.to(config.device)\n",
        "\n",
        "        # load the model from model_folder\n",
        "        self.load()\n",
        "\n",
        "    @beartype\n",
        "    def load(self) -> None:\n",
        "        \"\"\"Load the model from the path\n",
        "\n",
        "        Args:\n",
        "            path (str): path to the model\n",
        "        \"\"\"\n",
        "        path = ModelLoader.check_model_path(\n",
        "            config=self.config,\n",
        "            is_checkpoint=False,\n",
        "            current_epoch=None,\n",
        "        )\n",
        "        if path is not None:\n",
        "            print(\"Loading ...\")\n",
        "            model_dict = torch.load(path)\n",
        "            self.model.load_state_dict(model_dict[\"model\"])\n",
        "            self.head.load_state_dict(model_dict[\"head\"])\n",
        "\n",
        "    @beartype\n",
        "    def save(self) -> None:\n",
        "        \"\"\"Save the model to the path\n",
        "\n",
        "        Args:\n",
        "            path (Optional[str], optional): Path to store the model.\n",
        "                Defaults to None.\n",
        "        \"\"\"\n",
        "        model_folder, model_name, path = ModelLoader.get_model_path(\n",
        "            config=self.config,\n",
        "            is_checkpoint=False,\n",
        "            current_epoch=None,\n",
        "        )\n",
        "        print(f\"Saving model to {path} ...\")\n",
        "        torch.save(\n",
        "            {\"model\": self.model.state_dict(), \"head\": self.head.state_dict()},\n",
        "            path,\n",
        "        )\n",
        "\n",
        "    def parameters(self, **kwargs):\n",
        "        \"\"\"Return the parameters of the model\n",
        "\n",
        "        Args:\n",
        "            **kwargs:\n",
        "        \"\"\"\n",
        "        return self.model.parameters()\n",
        "\n",
        "    @beartype\n",
        "    def forward(\n",
        "        self, sequences: torch.Tensor, sequences_mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Generate logits to have probability distribution over the vocabulary\n",
        "            of the actions\n",
        "\n",
        "        Args:\n",
        "            sequences (torch.Tensor): Sequences of states and actions used to\n",
        "                    compute token logits for the whole list of sequences\n",
        "            attention_mask (torch.Tensor): Mask for the sequences attention\n",
        "\n",
        "        Returns:\n",
        "            logits (torch.Tensor): Logits for the actions taken\n",
        "        \"\"\"\n",
        "        model_output = self.model.forward(\n",
        "            sequences, attention_mask=sequences_mask\n",
        "        )\n",
        "        # need to return logits for the actions\n",
        "        if self.config.model in hf_models_causal_lm:\n",
        "            model_output = model_output.logits\n",
        "        if self.config.debug:\n",
        "            print(\"ActorModel.forward\")\n",
        "            print(\"model_output_logits shape\", model_output.shape)\n",
        "            print(\"model_output logits\", model_output)\n",
        "        return model_output\n",
        "\n",
        "    @beartype\n",
        "    @torch.no_grad()\n",
        "    def generate(\n",
        "        self, states: torch.Tensor, state_mask: torch.Tensor\n",
        "    ) -> Tuple:\n",
        "        \"\"\"Generate actions and sequences=[states, actions] from state\n",
        "            (i.e. input of the prompt generator model)\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor): the input of the user\n",
        "            state_mask (torch.Tensor): Mask for the state input (for padding)\n",
        "\n",
        "        Returns:\n",
        "            actions (torch.Tensor): Actions generated from the state\n",
        "            sequences (torch.Tensor): Sequences generated from the\n",
        "                state as [states, actions]\n",
        "        \"\"\"\n",
        "        temperature = self.config.temperature\n",
        "        # max sequence length for the actor (i.e. prompt + completion)\n",
        "        # from config file - it depends by the model used\n",
        "        max_sequence_length = self.config.max_sequence_length\n",
        "        # max tokens generated by the actor (completion only) from config file\n",
        "        max_tokens = self.config.max_tokens\n",
        "        # temperature for the actor\n",
        "        max_generation_possible = max_sequence_length - states.shape[1]\n",
        "        # take the minimum between the maximum token that you want to generate\n",
        "        # and the token that is possible to generate given the maximum sequence\n",
        "        # supported\n",
        "        max_completion = min(max_tokens, max_generation_possible)\n",
        "        if max_completion <= 0:\n",
        "            raise ValueError(\n",
        "                \"The maximum completion available is <= 0 the prompt is too \"\n",
        "                + \"long w.r.t the model sequence length\"\n",
        "            )\n",
        "        # the max_length is then the input length + the completion length\n",
        "        max_length = states.shape[1] + max_completion\n",
        "        # generate\n",
        "        sequences = self.model.generate(\n",
        "            input_ids=states,\n",
        "            attention_mask=state_mask,\n",
        "            temperature=temperature,\n",
        "            max_length=max_length,\n",
        "        )\n",
        "        actions = sequences[:, states.shape[1] :]  # noqa E203\n",
        "        if self.config.debug:\n",
        "            print(\"ActorModel.generate\")\n",
        "            print(\"state\", states)\n",
        "            print(\"state shape\", states.shape)\n",
        "            print(\"sequence shape\", sequences.shape)\n",
        "            print(\"sequence\", sequences)\n",
        "            print(\"actions shape\", actions.shape)\n",
        "            print(\"actions\", actions)\n",
        "        return actions, sequences\n",
        "\n",
        "\n",
        "class ActorDataset(Dataset):\n",
        "    \"\"\"Dataset for the pretraining of the actor model\n",
        "    read a json file with the following format:\n",
        "    [\n",
        "        {\n",
        "            \"user_input\": \"...\"\n",
        "            \"completion\": \"...\"\n",
        "        },\n",
        "        ...\n",
        "    ]\n",
        "    Where:\n",
        "        user_input: the input of the user\n",
        "        completion: the output of the user\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path: str) -> None:\n",
        "        self.path = path\n",
        "        with open(path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "            self.data = [d[\"user_input\"] + \" \" + d[\"completion\"] for d in data]\n",
        "        self.len = len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def __len__(\n",
        "        self,\n",
        "    ):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "class ActorTrainer:\n",
        "    \"\"\"Used to pre-train the actor model to generate better prompts.\n",
        "\n",
        "    Args:\n",
        "        config (ConfigActor): Configuration for the actor model\n",
        "\n",
        "    Attributes:\n",
        "        config (ConfigActor): Configuration for the actor model\n",
        "        model (ActorModel): Actor model\n",
        "        loss_function (torch.nn.CrossEntropyLoss): Loss function\n",
        "        optimizer (torch.optim.Adam): Optimizer\n",
        "        validation_flag (bool): Flag to indicate if the validation dataset\n",
        "            is provided\n",
        "        training_stats (TrainingStats): Training statistics\n",
        "\n",
        "    Methods:\n",
        "        train: Train the actor model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: ConfigActor) -> None:\n",
        "        # load the model, optimizer, loss function and config\n",
        "        self.config = config\n",
        "        self.model = ActorModel(config)\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            self.model.parameters(), lr=config.lr\n",
        "        )\n",
        "\n",
        "        # check checkpoint, datasets and other data\n",
        "        self.validation_flag = False\n",
        "        self.training_stats = TrainingStats()\n",
        "        if config.validation_dataset_path is not None:\n",
        "            self.validation_flag = True\n",
        "\n",
        "        # create dataloaders\n",
        "        self.train_dataset = ActorDataset(config.train_dataset_path)\n",
        "        self.train_dataloader = DataLoader(\n",
        "            self.train_dataset, batch_size=config.batch_size\n",
        "        )\n",
        "        if self.validation_flag:\n",
        "            self.eval_dataset = ActorDataset(config.validation_dataset_path)\n",
        "            self.validation_dataloader = DataLoader(\n",
        "                self.eval_dataset, batch_size=config.batch_size\n",
        "            )\n",
        "\n",
        "        # initialize deepspeed\n",
        "        self.model_engine = None\n",
        "        if config.deepspeed_enable is True:\n",
        "            if config.deepspeed_config_path is None:\n",
        "                raise ValueError(\n",
        "                    \"DeepSpeed config path is None, but deepspeed is enabled\"\n",
        "                )\n",
        "            if os.path.exists(config.deepspeed_config_path) is False:\n",
        "                raise ValueError(\n",
        "                    f\"DeepSpeed config path {config.deepspeed_config_path}\"\n",
        "                    f\"does not exist\"\n",
        "                )\n",
        "            (\n",
        "                self.model_engine,\n",
        "                self.optimizer,\n",
        "                self.train_dataloader,\n",
        "                _,\n",
        "            ) = deepspeed.initialize(\n",
        "                args=None,\n",
        "                model=self.model,\n",
        "                model_parameters=self.model.parameters(),\n",
        "                training_data=self.train_dataloader,\n",
        "                config=self.config.deepspeed_config_path,\n",
        "            )\n",
        "\n",
        "    @beartype\n",
        "    def save_checkpoint(\n",
        "        self,\n",
        "        current_epoch: int,\n",
        "        current_step: int,\n",
        "        max_epochs: int,\n",
        "        max_steps: int,\n",
        "    ) -> None:\n",
        "\n",
        "        print(\n",
        "            f\"Saving checkpoint for epoch {current_epoch + 1}, \"\n",
        "            f\"step {current_step + 1} ...\"\n",
        "        )\n",
        "        model_folder, model_name, path = ModelLoader.get_model_path(\n",
        "            config=self.config,\n",
        "            is_checkpoint=True,\n",
        "            current_epoch=current_epoch,\n",
        "            current_step=current_step,\n",
        "            max_epochs=max_epochs,\n",
        "            max_steps=max_steps,\n",
        "        )\n",
        "        if os.path.exists(path):\n",
        "            os.remove(path)\n",
        "        torch.save(\n",
        "            {\n",
        "                \"state_dict\": self.model.state_dict(),\n",
        "                \"optim_state_dict\": self.optimizer.state_dict(),\n",
        "                \"training_stats\": self.training_stats,\n",
        "                \"epoch\": current_epoch,\n",
        "                \"step\": current_step,\n",
        "            },\n",
        "            path,\n",
        "        )\n",
        "\n",
        "    @beartype\n",
        "    def load_checkpoint(\n",
        "        self,\n",
        "    ) -> Tuple[int, int]:\n",
        "        \"\"\"Load a checkpoint from the model folder\"\"\"\n",
        "\n",
        "        print(\"Looking for checkpoints...\")\n",
        "        path = ModelLoader.check_model_path(\n",
        "            config=self.config,\n",
        "            is_checkpoint=True,\n",
        "            current_epoch=None,\n",
        "        )\n",
        "        if path is not None:\n",
        "            print(\"Loading ...\")\n",
        "            checkpoint = torch.load(path)\n",
        "            epoch = checkpoint[\"epoch\"]\n",
        "            self.model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "            self.optimizer.load_state_dict(checkpoint[\"optim_state_dict\"])\n",
        "            self.trainign_stats = checkpoint[\"training_stats\"]\n",
        "            step = checkpoint[\"step\"]\n",
        "            return epoch, step + 1  # return the next episode to train\n",
        "        return 0, 0\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "    ) -> None:\n",
        "        print(\"Start Actor Model Pretraining\")\n",
        "\n",
        "        # get config parameters\n",
        "        batch_size = self.config.batch_size\n",
        "        epochs = self.config.epochs\n",
        "        device = self.config.device\n",
        "        checkpoint_steps = self.config.checkpoint_steps\n",
        "\n",
        "        # compute the number of iterations\n",
        "        n_iter = int(len(self.train_dataset) / batch_size)\n",
        "\n",
        "        # load model_checkpoint\n",
        "        start_epoch, start_step = self.load_checkpoint()\n",
        "\n",
        "        # counter for the checkpoint\n",
        "        cnt_checkpoint = 1\n",
        "\n",
        "        # traing loop\n",
        "        for epoch in range(start_epoch, epochs):\n",
        "            self.model.train()\n",
        "            for i, input_output in enumerate(self.train_dataloader):\n",
        "                if i < start_step:\n",
        "                    continue\n",
        "                with torch.no_grad():\n",
        "                    input_output_tokenized = self.model.tokenizer(\n",
        "                        input_output,\n",
        "                        return_tensors=\"pt\",\n",
        "                        padding=True,\n",
        "                    )\n",
        "                    training_output = input_output_tokenized[\"input_ids\"][\n",
        "                        :, 1:\n",
        "                    ]\n",
        "                    training_input = input_output_tokenized[\"input_ids\"][\n",
        "                        :, :-1\n",
        "                    ]\n",
        "                    attention_mask = input_output_tokenized[\"attention_mask\"][\n",
        "                        :, :-1\n",
        "                    ]\n",
        "                    training_output = training_output.to(device)\n",
        "                    training_input = training_input.to(device)\n",
        "                    attention_mask = attention_mask.to(device)\n",
        "\n",
        "                # forward pass\n",
        "                if self.config.deepspeed_enable:\n",
        "                    est_output = self.model_engine(\n",
        "                        training_input, attention_mask\n",
        "                    )\n",
        "                else:\n",
        "                    est_output = self.model(training_input, attention_mask)\n",
        "                est_output = rearrange(est_output, \"b s v -> (b s) v\")\n",
        "                training_output = rearrange(training_output, \"b s -> (b s)\")\n",
        "                loss = self.loss_function(est_output, training_output)\n",
        "                self.training_stats.training_loss.append(loss.item())\n",
        "\n",
        "                # backward pass\n",
        "                if self.config.deepspeed_enable:\n",
        "                    self.model_engine.backward(loss)\n",
        "                    self.model_engine.step()\n",
        "                else:\n",
        "                    self.optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "\n",
        "                # print progress\n",
        "                if i % self.config.iteration_per_print == 0:\n",
        "                    print(\n",
        "                        f\"Epoch: {epoch+1}/{epochs}, \"\n",
        "                        f\"Iteration: {i+1}/{n_iter}, \"\n",
        "                        f\"Training Loss: {loss}\"\n",
        "                    )\n",
        "                # save checkpoint periodically\n",
        "                if cnt_checkpoint % checkpoint_steps == 0:\n",
        "                    self.save_checkpoint(epoch, i, epochs, n_iter)\n",
        "                    cnt_checkpoint = 1\n",
        "                else:\n",
        "                    cnt_checkpoint += 1\n",
        "\n",
        "            if self.validation_flag:\n",
        "                self.model.eval()\n",
        "                for i, input_output in enumerate(self.validation_dataloader):\n",
        "                    input_output_tokenized = self.model.tokenizer(\n",
        "                        input_output, return_tensors=\"pt\", padding=True\n",
        "                    )\n",
        "                    validation_output = input_output_tokenized[\"input_ids\"][\n",
        "                        :, 1:\n",
        "                    ]\n",
        "                    validation_input = input_output_tokenized[\"input_ids\"][\n",
        "                        :, :-1\n",
        "                    ]\n",
        "                    attention_mask = input_output_tokenized[\"attention_mask\"][\n",
        "                        :, :-1\n",
        "                    ]\n",
        "\n",
        "                    # forward pass\n",
        "                    est_output = self.model.forward(\n",
        "                        validation_input, attention_mask\n",
        "                    )\n",
        "                    validation_output = rearrange(\n",
        "                        validation_output, \"b s -> (b s)\"\n",
        "                    )\n",
        "                    est_output = rearrange(est_output, \"b s v -> (b s) v\")\n",
        "                    loss = self.loss_function(est_output, validation_output)\n",
        "                    self.training_stats.validation_loss.append(loss.item())\n",
        "                    # print progress\n",
        "                    if i % self.config.iteration_per_print == 0:\n",
        "                        print(\n",
        "                            f\"Epoch: {epoch+1}/{epochs}, \"\n",
        "                            f\"Iteration: {i+1}/{n_iter}, \"\n",
        "                            f\"Validation Loss: {loss}\"\n",
        "                        )\n",
        "        self.model.save()\n",
        "        print(\"Training Finished \")"
      ],
      "metadata": {
        "id": "5MreDvACfcCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_test_contrct(path, ratio): \n",
        "  col = ['Target']\n",
        "  f = open(path, \"r\")\n",
        "  file = f.read()\n",
        "  test_df = pd.DataFrame([file], columns=col)\n",
        "  program_length = len(test_df['Target'][0].split())\n",
        "  prompt_ratio = ratio\n",
        "  prompt_length = int(prompt_ratio * program_length)\n",
        "  return test_df, prompt_length\n",
        "\n",
        "def truncate_test(df, prompt_length):\n",
        "  copy_1 = df.copy(deep=True)\n",
        "  copy_2 = df.copy(deep=True)\n",
        "  #true\n",
        "  a = copy_1['Target'].str.split().str[-prompt_length:].apply(' '.join)[0]\n",
        "  #masked out program \n",
        "  b = copy_2['Target'].str.split().str[:-prompt_length].apply(' '.join)[0]\n",
        "  return a, b"
      ],
      "metadata": {
        "id": "_hKPs7PCcuHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df, n = generate_test_contrct('/content/drive/MyDrive/experiments/baseline benchmark/Replica.sol', 0.95)\n",
        "truth, prompt = truncate_test(test_df, n)\n",
        "col = ['Target']\n",
        "prompt_df = pd.DataFrame([prompt], columns=col)\n",
        "gpt_out = text_generation(trained_model, tokenizer, prompt_df)\n",
        "print('this is the GPT prompt without T5: \\n', prompt_df['Target'][0])\n",
        "print('this is the GPT prediction without T5: \\n', gpt_out)"
      ],
      "metadata": {
        "id": "z5vMNi2eHxkh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}