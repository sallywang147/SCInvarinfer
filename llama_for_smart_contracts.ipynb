{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNh99s708Iz58nUo97JXK/W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01352cd0a27b43978f189e4a1d48d454": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b05b9be000494ed8b4b614c3c19723e6",
              "IPY_MODEL_f4462934d24c4edb810c1da82b561b6b",
              "IPY_MODEL_b580e665c1764fbab395c258d2100539"
            ],
            "layout": "IPY_MODEL_863b05792324494c8981a94375605334"
          }
        },
        "b05b9be000494ed8b4b614c3c19723e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_842db080b5a649d6871bd94b40317e65",
            "placeholder": "​",
            "style": "IPY_MODEL_f282806e6b8c4ae2bd1306cb9b3e6a88",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "f4462934d24c4edb810c1da82b561b6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fd869f69d56470ebc19ff41584158d0",
            "max": 578,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_644919f5ed9f4136be46c8cd1445ae9b",
            "value": 578
          }
        },
        "b580e665c1764fbab395c258d2100539": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afcf86ce7a7d4490bc9aabdb1be6c2e8",
            "placeholder": "​",
            "style": "IPY_MODEL_42ffb6fba95944268e1673e6ae014a18",
            "value": " 578/578 [00:00&lt;00:00, 33.3kB/s]"
          }
        },
        "863b05792324494c8981a94375605334": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "842db080b5a649d6871bd94b40317e65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f282806e6b8c4ae2bd1306cb9b3e6a88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3fd869f69d56470ebc19ff41584158d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "644919f5ed9f4136be46c8cd1445ae9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "afcf86ce7a7d4490bc9aabdb1be6c2e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42ffb6fba95944268e1673e6ae014a18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d25c1b5760b4b1f8685735e8bbede1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c48bd1d148c4c9990ff568fd6b511c5",
              "IPY_MODEL_58bb90b8e67d480f92b36fdf41ed6422",
              "IPY_MODEL_57a3d0d224424677aa06a9b0cf01ae39"
            ],
            "layout": "IPY_MODEL_4024692e3bfe4a9c9e84568a38ad2d13"
          }
        },
        "6c48bd1d148c4c9990ff568fd6b511c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c4d75d91f4b4cce9904fa4d604b9b02",
            "placeholder": "​",
            "style": "IPY_MODEL_4faa3afcd37849c892d40f86087f2755",
            "value": "Downloading (…)model.bin.index.json: 100%"
          }
        },
        "58bb90b8e67d480f92b36fdf41ed6422": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53fe323691814c689f872455287bc9d0",
            "max": 26788,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_576aac786ed84e81bc10283e6e2fe1f8",
            "value": 26788
          }
        },
        "57a3d0d224424677aa06a9b0cf01ae39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a6d8d4a123b4be9808903afa6d95a97",
            "placeholder": "​",
            "style": "IPY_MODEL_cdcd77f942d049f59202f7aa6fc3600b",
            "value": " 26.8k/26.8k [00:00&lt;00:00, 121kB/s]"
          }
        },
        "4024692e3bfe4a9c9e84568a38ad2d13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c4d75d91f4b4cce9904fa4d604b9b02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4faa3afcd37849c892d40f86087f2755": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53fe323691814c689f872455287bc9d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "576aac786ed84e81bc10283e6e2fe1f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a6d8d4a123b4be9808903afa6d95a97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdcd77f942d049f59202f7aa6fc3600b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51201fd02d0a452c87af334abf5b2d45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f061a33315f447e87b7a243fff437e1",
              "IPY_MODEL_dfef44c6985f4d339a3bc919ac5c596a",
              "IPY_MODEL_48808645de504dbda49c7c73ae8e740b"
            ],
            "layout": "IPY_MODEL_a58e057e231e459ba9ddd31a8c380e37"
          }
        },
        "3f061a33315f447e87b7a243fff437e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f1aab1e6e26481f89bf5c852365f299",
            "placeholder": "​",
            "style": "IPY_MODEL_677de4442cdf4fb58deb2a3574934b7f",
            "value": "Downloading shards:   0%"
          }
        },
        "dfef44c6985f4d339a3bc919ac5c596a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d57889621594a4d9f2c1aed7d087fca",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72c3eec3f7d44d10ba8bf5290b93f271",
            "value": 0
          }
        },
        "48808645de504dbda49c7c73ae8e740b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f8d6b5ef6ce44f492ec7d00d78db223",
            "placeholder": "​",
            "style": "IPY_MODEL_c59be9f02a004ea094a9565c69a1e000",
            "value": " 0/3 [00:00&lt;?, ?it/s]"
          }
        },
        "a58e057e231e459ba9ddd31a8c380e37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f1aab1e6e26481f89bf5c852365f299": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "677de4442cdf4fb58deb2a3574934b7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d57889621594a4d9f2c1aed7d087fca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72c3eec3f7d44d10ba8bf5290b93f271": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f8d6b5ef6ce44f492ec7d00d78db223": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c59be9f02a004ea094a9565c69a1e000": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7854b95e45434d3fb7e00dc44bc391d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e3a2b71312e4b87906e2da9a32e1938",
              "IPY_MODEL_5eccdd98b6564cef8409d6b164b20eea",
              "IPY_MODEL_649799a83ec04cb1b3acf1f0eaf8d360"
            ],
            "layout": "IPY_MODEL_f3d499003ea94cd4b4b0fd2dc1f5bdba"
          }
        },
        "9e3a2b71312e4b87906e2da9a32e1938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3526c2569a94131ab3e1a7a032c6264",
            "placeholder": "​",
            "style": "IPY_MODEL_d2bfed3128f640e6b64e7701cfafb241",
            "value": "Downloading (…)l-00001-of-00003.bin:  96%"
          }
        },
        "5eccdd98b6564cef8409d6b164b20eea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37d7c65659db413ca1ca043b26e37db0",
            "max": 9877989586,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_817c547043494eec902b7ecaa94fae7f",
            "value": 9510584320
          }
        },
        "649799a83ec04cb1b3acf1f0eaf8d360": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb3476797813435096904f5cad2a22c5",
            "placeholder": "​",
            "style": "IPY_MODEL_15dd46bdf153444ea03d189cbb8a8ded",
            "value": " 9.51G/9.88G [00:46&lt;00:01, 353MB/s]"
          }
        },
        "f3d499003ea94cd4b4b0fd2dc1f5bdba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3526c2569a94131ab3e1a7a032c6264": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2bfed3128f640e6b64e7701cfafb241": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37d7c65659db413ca1ca043b26e37db0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "817c547043494eec902b7ecaa94fae7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb3476797813435096904f5cad2a22c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15dd46bdf153444ea03d189cbb8a8ded": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sallywang147/SCInvarinfer/blob/main/llama_for_smart_contracts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lILBOi-WpPWw",
        "outputId": "21773318-fe93-4ca0-e8d2-425b21be05bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install rich[jupyter]\n",
        "!pip install deepspeed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KELJ4pTduNrW",
        "outputId": "d21afa94-a096-49e4-8cb8-adfab61b0c51"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-_6sqry06\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-_6sqry06\n",
            "  Resolved https://github.com/huggingface/transformers to commit 5990743fddb4780b15b8af2ed7ab55145ab40455\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (3.10.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (2.0.12)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.28.0.dev0-py3-none-any.whl size=6758236 sha256=0a025e791ab6983ef08ab1cefa9fd8379be5006e70ea1e5bc0a416b6fee6a63b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4_5bs4pa/wheels/14/a0/7b/8f6b25ba4110aa215fcb8d6aedd6cd4f9b9b6619190999ac2b\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.28.0.dev0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rich[jupyter]\n",
            "  Downloading rich-13.3.2-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.7/238.7 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown-it-py<3.0.0,>=2.2.0\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygments<3.0.0,>=2.13.0\n",
            "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipywidgets<9,>=7.5.1 in /usr/local/lib/python3.9/dist-packages (from rich[jupyter]) (7.7.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (3.0.5)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (3.6.2)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (5.3.4)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (5.7.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (0.2.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (7.9.0)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.5.1->rich[jupyter]) (6.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.5.1->rich[jupyter]) (6.1.12)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (63.4.3)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.2.0)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (2.0.10)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.9/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (6.5.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.8.3)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.5.6)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.5.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (3.1.2)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (23.2.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (5.7.3)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (21.3.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (5.3.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.17.1)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (6.5.4)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.16.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.9/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets<9,>=7.5.1->rich[jupyter]) (2.8.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.16.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.2.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect->ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.7.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (3.1.1)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.9/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.23.6)\n",
            "Requirement already satisfied: notebook-shim>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (2.1.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (4.11.2)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.2.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (4.9.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (23.0)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.2.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.5.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.9/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.9/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (2.16.3)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.9/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (21.2.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (22.2.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.19.3)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (3.6.2)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.9/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.5.1)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.15.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (2.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.5.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.9/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.9/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (2.21)\n",
            "Installing collected packages: pygments, mdurl, jedi, markdown-it-py, rich\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.6.1\n",
            "    Uninstalling Pygments-2.6.1:\n",
            "      Successfully uninstalled Pygments-2.6.1\n",
            "Successfully installed jedi-0.18.2 markdown-it-py-2.2.0 mdurl-0.1.2 pygments-2.14.0 rich-13.3.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pygments"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deepspeed\n",
            "  Downloading deepspeed-0.8.3.tar.gz (765 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m765.4/765.4 KB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hjson\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 KB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from deepspeed) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from deepspeed) (23.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from deepspeed) (5.9.4)\n",
            "Collecting py-cpuinfo\n",
            "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.9/dist-packages (from deepspeed) (1.10.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from deepspeed) (1.13.1+cu116)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from deepspeed) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic->deepspeed) (4.5.0)\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.8.3-py3-none-any.whl size=776415 sha256=32e201b247eeb4ce421f5ff59fed9f54ecb98372f7d399341cc4052fff705f39\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/ea/8f/0768328ba436ed66f602d8d3b809624448c9eb627434176d04\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: py-cpuinfo, ninja, hjson, deepspeed\n",
            "Successfully installed deepspeed-0.8.3 hjson-3.1.0 ninja-1.11.1 py-cpuinfo-9.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Llama_Weights = '/content/drive/MyDrive/experiments/llama/LLaMA/7B/'\n",
        "Output = 'drive/MyDrive/experiments/HF_Llama'"
      ],
      "metadata": {
        "id": "2JW9JFKlv9US"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python /content/drive/MyDrive/experiments/llama/convert_llama_weights_to_hf.py \\\n",
        "#--input_dir '/content/drive/MyDrive/experiments/llama/LLaMA/7B' \\\n",
        "# --model_size 7B --output_dir 'drive/MyDrive/experiments/HF_Llama'"
      ],
      "metadata": {
        "id": "6K35upsguFjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vanilla Llama##"
      ],
      "metadata": {
        "id": "7bYljYiYAYcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import AutoTokenizer, LlamaTokenizer, LlamaForCausalLM\n",
        "\n",
        "\n",
        "model = LlamaForCausalLM.from_pretrained('sallywww/Llama-7B')\n",
        "tokenizer = AutoTokenizer.from_pretrained('sallywww/Llama-7B')\n",
        "\n",
        "#model.push_to_hub('sallywww/Llama-7B')\n",
        "#tokenizer.push_to_hub('sallywww/Llama-7B')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "01352cd0a27b43978f189e4a1d48d454",
            "b05b9be000494ed8b4b614c3c19723e6",
            "f4462934d24c4edb810c1da82b561b6b",
            "b580e665c1764fbab395c258d2100539",
            "863b05792324494c8981a94375605334",
            "842db080b5a649d6871bd94b40317e65",
            "f282806e6b8c4ae2bd1306cb9b3e6a88",
            "3fd869f69d56470ebc19ff41584158d0",
            "644919f5ed9f4136be46c8cd1445ae9b",
            "afcf86ce7a7d4490bc9aabdb1be6c2e8",
            "42ffb6fba95944268e1673e6ae014a18",
            "2d25c1b5760b4b1f8685735e8bbede1c",
            "6c48bd1d148c4c9990ff568fd6b511c5",
            "58bb90b8e67d480f92b36fdf41ed6422",
            "57a3d0d224424677aa06a9b0cf01ae39",
            "4024692e3bfe4a9c9e84568a38ad2d13",
            "9c4d75d91f4b4cce9904fa4d604b9b02",
            "4faa3afcd37849c892d40f86087f2755",
            "53fe323691814c689f872455287bc9d0",
            "576aac786ed84e81bc10283e6e2fe1f8",
            "6a6d8d4a123b4be9808903afa6d95a97",
            "cdcd77f942d049f59202f7aa6fc3600b",
            "51201fd02d0a452c87af334abf5b2d45",
            "3f061a33315f447e87b7a243fff437e1",
            "dfef44c6985f4d339a3bc919ac5c596a",
            "48808645de504dbda49c7c73ae8e740b",
            "a58e057e231e459ba9ddd31a8c380e37",
            "1f1aab1e6e26481f89bf5c852365f299",
            "677de4442cdf4fb58deb2a3574934b7f",
            "6d57889621594a4d9f2c1aed7d087fca",
            "72c3eec3f7d44d10ba8bf5290b93f271",
            "4f8d6b5ef6ce44f492ec7d00d78db223",
            "c59be9f02a004ea094a9565c69a1e000",
            "7854b95e45434d3fb7e00dc44bc391d9",
            "9e3a2b71312e4b87906e2da9a32e1938",
            "5eccdd98b6564cef8409d6b164b20eea",
            "649799a83ec04cb1b3acf1f0eaf8d360",
            "f3d499003ea94cd4b4b0fd2dc1f5bdba",
            "e3526c2569a94131ab3e1a7a032c6264",
            "d2bfed3128f640e6b64e7701cfafb241",
            "37d7c65659db413ca1ca043b26e37db0",
            "817c547043494eec902b7ecaa94fae7f",
            "fb3476797813435096904f5cad2a22c5",
            "15dd46bdf153444ea03d189cbb8a8ded"
          ]
        },
        "id": "D2oAIPSSxSNf",
        "outputId": "711e722e-7559-4481-fc32-ac2556b14b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01352cd0a27b43978f189e4a1d48d454"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d25c1b5760b4b1f8685735e8bbede1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51201fd02d0a452c87af334abf5b2d45"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)l-00001-of-00003.bin:   0%|          | 0.00/9.88G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7854b95e45434d3fb7e00dc44bc391d9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert(False)\n",
        "prompt = \"a loop invariant in computer science is \"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate\n",
        "generate_ids = model.generate(inputs.input_ids, max_length=40)\n",
        "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "SSQCT4xwDt2-",
        "outputId": "f4106922-d914-460b-9ae7-207ae144c769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-124-c55e020169f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"a loop invariant in computer science is \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Generate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##loading and json-ize data##"
      ],
      "metadata": {
        "id": "bdCPq8HpMeIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install simplejson"
      ],
      "metadata": {
        "id": "TBBRkS1uCl6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de8665a-bb36-4bf2-c665-c5c2d0d700fb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting simplejson\n",
            "  Downloading simplejson-3.18.4-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.8/136.8 KB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: simplejson\n",
            "Successfully installed simplejson-3.18.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "import simplejson as json\n",
        "from numpy import random\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import pandas\n",
        "import xlrd\n",
        "import json\n",
        "import xlrd\n",
        "\n",
        "#workbook = xlrd.open_workbook('/invariants_line_number.xlsx', on_demand = True)\n",
        "df = pandas.read_excel(open('/content/invariants_data.xlsx','rb'))\n",
        "df"
      ],
      "metadata": {
        "id": "XtHDzEL8Bvya",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "1007017d-8571-4760-d4c1-2d1534164119"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Source  \\\n",
              "0    1 pragma solidity >=0.4.24 <0.6.0;\\n\\n2 contra...   \n",
              "1    1 // SPDX-License-Identifier: MIT\\n2 pragma ex...   \n",
              "2    1 pragma solidity >=0.4.24 <0.6.0;\\n\\n2 contra...   \n",
              "3    1 pragma solidity >=0.4.24 <0.6.0;\\n2 contract...   \n",
              "4    1 pragma solidity >=0.4.24 <0.6.0;\\n\\n2 // Thi...   \n",
              "..                                                 ...   \n",
              "98   1 pragma solidity >=0.4.24 <0.6.0;\\n2 \\n3 cont...   \n",
              "99   1  // SPDX-License-Identifier: MIT\\n2  pragma ...   \n",
              "100  1  // SPDX-License-Identifier: MIT\\n2  pragma ...   \n",
              "101  1 / SPDX-License-Identifier: MIT\\n2  pragma so...   \n",
              "102  1   // SPDX-License-Identifier: MIT\\n2 pragma ...   \n",
              "\n",
              "                                                Target  \n",
              "0                             14+ assert (y == x + 4);  \n",
              "1    109+  require(admin == msg.sender, \"Ownable: c...  \n",
              "2                              7+ assert (y == x + 2);  \n",
              "3    20+ assert(funcA2(funcA1())==12);\\n20+ assert(...  \n",
              "4    24+ assert(a == x + 1);\\n32+ assert(a == x);\\n...  \n",
              "..                                                 ...  \n",
              "98            6+ assert (!false);\\n6+ assert (!false);  \n",
              "99   6+  assert(fee + value != 0);\\n6+  assert(fee ...  \n",
              "100  15+ assert(_required > 0); \\n15+  assert(m_num...  \n",
              "101  6+  assert(address(this)==msg.sender);\\n6+  as...  \n",
              "102  30+ assert(msg.sender == _contractRegistry);\\n...  \n",
              "\n",
              "[103 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bde313bf-5f90-44d6-84b4-f1563b13142c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1 pragma solidity &gt;=0.4.24 &lt;0.6.0;\\n\\n2 contra...</td>\n",
              "      <td>14+ assert (y == x + 4);</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1 // SPDX-License-Identifier: MIT\\n2 pragma ex...</td>\n",
              "      <td>109+  require(admin == msg.sender, \"Ownable: c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1 pragma solidity &gt;=0.4.24 &lt;0.6.0;\\n\\n2 contra...</td>\n",
              "      <td>7+ assert (y == x + 2);</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1 pragma solidity &gt;=0.4.24 &lt;0.6.0;\\n2 contract...</td>\n",
              "      <td>20+ assert(funcA2(funcA1())==12);\\n20+ assert(...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1 pragma solidity &gt;=0.4.24 &lt;0.6.0;\\n\\n2 // Thi...</td>\n",
              "      <td>24+ assert(a == x + 1);\\n32+ assert(a == x);\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>1 pragma solidity &gt;=0.4.24 &lt;0.6.0;\\n2 \\n3 cont...</td>\n",
              "      <td>6+ assert (!false);\\n6+ assert (!false);</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>1  // SPDX-License-Identifier: MIT\\n2  pragma ...</td>\n",
              "      <td>6+  assert(fee + value != 0);\\n6+  assert(fee ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>1  // SPDX-License-Identifier: MIT\\n2  pragma ...</td>\n",
              "      <td>15+ assert(_required &gt; 0); \\n15+  assert(m_num...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>1 / SPDX-License-Identifier: MIT\\n2  pragma so...</td>\n",
              "      <td>6+  assert(address(this)==msg.sender);\\n6+  as...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>1   // SPDX-License-Identifier: MIT\\n2 pragma ...</td>\n",
              "      <td>30+ assert(msg.sender == _contractRegistry);\\n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bde313bf-5f90-44d6-84b4-f1563b13142c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bde313bf-5f90-44d6-84b4-f1563b13142c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bde313bf-5f90-44d6-84b4-f1563b13142c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _make_w_io_base(f, mode: str):\n",
        "    if not isinstance(f, io.IOBase):\n",
        "        f_dirname = os.path.dirname(f)\n",
        "        if f_dirname != \"\":\n",
        "            os.makedirs(f_dirname, exist_ok=True)\n",
        "        f = open(f, mode=mode)\n",
        "    return f\n",
        "\n",
        "\n",
        "def _make_r_io_base(f, mode: str):\n",
        "    if not isinstance(f, io.IOBase):\n",
        "        f = open(f, mode=mode)\n",
        "    return f\n",
        "\n",
        "\n",
        "def jdump(obj, f, mode=\"w\", indent=4, default=str):\n",
        "    \"\"\"Dump a str or dictionary to a file in json format.\n",
        "    Args:\n",
        "        obj: An object to be written.\n",
        "        f: A string path to the location on disk.\n",
        "        mode: Mode for opening the file.\n",
        "        indent: Indent for storing json dictionaries.\n",
        "        default: A function to handle non-serializable entries; defaults to `str`.\n",
        "    \"\"\"\n",
        "    f = _make_w_io_base(f, mode)\n",
        "    if isinstance(obj, (dict, list)):\n",
        "        json.dump(obj, f, indent=indent, default=default)\n",
        "    elif isinstance(obj, str):\n",
        "        f.write(obj)\n",
        "    else:\n",
        "        raise ValueError(f\"Unexpected type: {type(obj)}\")\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def jload(f, mode=\"r\"):\n",
        "    \"\"\"Load a .json file into a dictionary.\"\"\"\n",
        "    f = _make_r_io_base(f, mode)\n",
        "    jdict = json.load(f)\n",
        "    f.close()\n",
        "    return jdict\n"
      ],
      "metadata": {
        "id": "ykNHXMR4K0eB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List to hold dictionaries\n",
        "def prepare_actor_model_data(df): \n",
        "    data_list=[]\n",
        "    for index, row in df.iterrows():\n",
        "        data = OrderedDict()\n",
        "        data[\"user_input\"] = row['Source']\n",
        "        data[\"completion\"] = row['Target']\n",
        "        data_list.append(data)\n",
        "    jdump(data_list, 'datasets/actor_training_data.json')\n",
        "\n",
        "def prepare_rlhf_model_data(df): \n",
        "    data_list=[]\n",
        "    for index, row in df.iterrows():\n",
        "        data = OrderedDict()\n",
        "        data[\"user_input\"] = row['Source']\n",
        "        data_list.append(data)\n",
        "    jdump(data, '/datasets/reward_training_data.json')\n",
        "\n",
        "\n",
        "def prepare_reward_model_data(df):\n",
        "    data_list=[]    \n",
        "    for index, row in df.iterrows():\n",
        "        data = OrderedDict()\n",
        "        data[\"user_input\"] = row['Source']\n",
        "        data[\"completion\"] = row['Target']\n",
        "        data[\"score\"] = 5\n",
        "        data_list.append(data)\n",
        "    jdump(data, '/content/datasets/rlhf_training_data.json')\n"
      ],
      "metadata": {
        "id": "VKnXo46McPRf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_actor_model_data(df)\n",
        "prepare_rlhf_model_data(df)\n",
        "prepare_reward_model_data(df)"
      ],
      "metadata": {
        "id": "7tfm3a3ql0gZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "#!zip -r /content/datasets.zip /content/datasets\n",
        "#files.download(\"/content/datasets.zip\")"
      ],
      "metadata": {
        "id": "5jgix6Qwj6XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(\n",
        "    sources: Sequence[str],\n",
        "    targets: Sequence[str],\n",
        "    tokenizer: tokenizer,\n",
        ") -> Dict:\n",
        "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
        "    examples = [s + t for s, t in zip(sources, targets)]\n",
        "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
        "    input_ids = examples_tokenized[\"input_ids\"]\n",
        "    labels = copy.deepcopy(input_ids)\n",
        "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "        label[:source_len] = IGNORE_INDEX\n",
        "    return dict(input_ids=input_ids, labels=labels)\n",
        "\n",
        "\n",
        "def _tokenize_fn(strings: Sequence[str], tokenizer: tokenizer) -> Dict:   \n",
        "    \"\"\"Tokenize a list of strings.\"\"\"\n",
        "    \n",
        "    tokenized_list = [\n",
        "        tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"longest\",\n",
        "            max_length=tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        for text in strings\n",
        "    ]\n",
        "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "    input_ids_lens = labels_lens = [\n",
        "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
        "    ]\n",
        "    return dict(\n",
        "        input_ids=input_ids,\n",
        "        labels=labels,\n",
        "        input_ids_lens=input_ids_lens,\n",
        "        labels_lens=labels_lens,\n",
        "    )"
      ],
      "metadata": {
        "id": "j4ZafBDGUss_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tokenizer.pad_token is None or tokenizer.eos_token:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))  \n",
        "print(tokenizer.pad_token)"
      ],
      "metadata": {
        "id": "w7VAcWvOYB8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "880da37c-61b9-4daf-d630-8267dc8c3db5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using pad_token, but it is not set yet.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PAD]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/datasets/actor_training_data.json', \"r\") as f:\n",
        "    data = json.load(f)\n",
        "    for d in data:\n",
        "     item = [str(d[\"user_input\"]) + \"\\n\" + str(d[\"completion\"]) for d in data]\n",
        "    length = len(item)\n",
        "sources = [f\"{example['user_input']}{tokenizer.eos_token}\" for example in data]#\n",
        "targets = [f\"{example['completion']}{tokenizer.eos_token}\" for example in data]\n",
        "\n",
        "data_dict = preprocess(sources, targets, tokenizer)\n",
        "input_ids = data_dict[\"input_ids\"]\n",
        "labels = data_dict[\"labels\"]"
      ],
      "metadata": {
        "id": "UTOQV6s7NkO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Finetuning using stanford dataprocessing## "
      ],
      "metadata": {
        "id": "Yea7vM37AxuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "AmsH2uSIOyIe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75afc211-216f-4993-bd37-f7e2b0c1e23f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.2-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.2 yarl-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path ='/content/datasets/actor_training_data.json'"
      ],
      "metadata": {
        "id": "0BcuoMN95gxl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import logging\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, Dict, Sequence\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import Trainer\n",
        "import dataclasses\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import io\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "from typing import Optional, Sequence, Union\n",
        "\n",
        "import openai\n",
        "import tqdm\n",
        "from openai import openai_object\n",
        "import copy\n",
        "\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "\n",
        "def _make_r_io_base(f, mode: str):\n",
        "    if not isinstance(f, io.IOBase):\n",
        "        f = open(f, mode=mode)\n",
        "    return f\n",
        "\n",
        "def jload(f, mode=\"r\"):\n",
        "    \"\"\"Load a .json file into a dictionary.\"\"\"\n",
        "    f = _make_r_io_base(f, mode)\n",
        "    jdict = json.load(f)\n",
        "    f.close()\n",
        "    return jdict\n",
        "\n",
        "def smart_tokenizer_and_embedding_resize(\n",
        "    special_tokens_dict: Dict,\n",
        "    tokenizer: tokenizer,\n",
        "    model: model,\n",
        "):\n",
        "    \"\"\"Resize tokenizer and embedding.\n",
        "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
        "    \"\"\"\n",
        "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if num_new_tokens > 0:\n",
        "        input_embeddings = model.get_input_embeddings().weight.data\n",
        "        output_embeddings = model.get_output_embeddings().weight.data\n",
        "\n",
        "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "\n",
        "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
        "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
        "\n",
        "\n",
        "def _tokenize_fn(strings: Sequence[str], tokenizer: tokenizer) -> Dict:\n",
        "    \"\"\"Tokenize a list of strings.\"\"\"\n",
        "    tokenized_list = [\n",
        "        tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"longest\",\n",
        "            max_length=tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        for text in strings\n",
        "    ]\n",
        "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "    input_ids_lens = labels_lens = [\n",
        "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
        "    ]\n",
        "    return dict(\n",
        "        input_ids=input_ids,\n",
        "        labels=labels,\n",
        "        input_ids_lens=input_ids_lens,\n",
        "        labels_lens=labels_lens,\n",
        "    )\n",
        "\n",
        "\n",
        "def preprocess(\n",
        "    sources: Sequence[str],\n",
        "    targets: Sequence[str],\n",
        "    tokenizer: tokenizer,\n",
        ") -> Dict:\n",
        "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
        "    examples = [s + t for s, t in zip(sources, targets)]\n",
        "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
        "    input_ids = examples_tokenized[\"input_ids\"]\n",
        "    labels = copy.deepcopy(input_ids)\n",
        "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "        label[:source_len] = IGNORE_INDEX\n",
        "    return dict(input_ids=input_ids, labels=labels)\n",
        "\n",
        "\n",
        "class SupervisedDataset(Dataset):\n",
        "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
        "    def __init__(self, data_path: str, tokenizer: tokenizer) -> None:\n",
        "        logging.warning(\"Loading data...\")\n",
        "        self.data_path = data_path\n",
        "        with open(data_path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "            self.data =  [str(d[\"user_input\"]) + \"\\n\" + str(d[\"completion\"]) for d in data]\n",
        "        self.len = len(self.data)\n",
        "        sources = [f\"{example['user_input']}{tokenizer.eos_token}\" for example in data]\n",
        "        targets = [f\"{example['completion']}{tokenizer.eos_token}\" for example in data]\n",
        "        data_dict = preprocess(sources, targets, tokenizer)\n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "    \n",
        "    def __len__(self):\n",
        "        #print(f\"comparing length with chatlllama definition: chatllama: {self.data[idx]} v.s. stanford version{self.input_ids}\")\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
        "\n",
        "\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
        "    def __init__(self, tokenizer: tokenizer) -> None:\n",
        "        logging.warning(\"data collator for supervised learning...\")\n",
        "        self.tokenizer=tokenizer\n",
        "    \n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )\n",
        "\n",
        "def make_supervised_data_module(tokenizer: tokenizer, data_path) -> Dict:\n",
        "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
        "    train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_path)\n",
        "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
        "    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)"
      ],
      "metadata": {
        "id": "aahblvHVp2c6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\"working_dir\")\n",
        "training_args = training_args.set_lr_scheduler(name=\"cosine\", warmup_ratio=0.05)\n",
        "training_args.fp16 = True\n",
        "training_args.auto_find_batch_size=False\n",
        "training_args.per_device_train_batch_size=1\n",
        "training_args.per_device_train_batch_size=1\n",
        "training_args.output_dir='./output'\n",
        "training_args.num_train_epochs = 3 \n",
        "training_args.gradient_accumulation_steps = 8 \n",
        "training_args.evaluation_strategy=\"no\" \n",
        "training_args.save_strategy=\"steps\" \n",
        "training_args.save_steps=2000 \n",
        "training_args.save_total_limit=1 \n",
        "training_args.learning_rate=2e-5 \n",
        "training_args.weight_decay=0. \n",
        "training_args.warmup_ratio=0.03 \n",
        "training_args.lr_scheduler_type=\"cosine\" \n",
        "training_args.logging_steps=1  \n",
        "training_args.fsdp=\"full_shard auto_wrap\"\n",
        "training_args.fsdp_transformer_layer_cls_to_wrap='LLaMADecoderLayer' \n",
        "training_args.tf32=True\n",
        "training_args"
      ],
      "metadata": {
        "id": "FucYlJbQ7Ayo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56f61441-0494-44d0-85ea-03f2890ae68c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainingArguments(\n",
              "_n_gpu=1,\n",
              "adafactor=False,\n",
              "adam_beta1=0.9,\n",
              "adam_beta2=0.999,\n",
              "adam_epsilon=1e-08,\n",
              "auto_find_batch_size=False,\n",
              "bf16=False,\n",
              "bf16_full_eval=False,\n",
              "data_seed=None,\n",
              "dataloader_drop_last=False,\n",
              "dataloader_num_workers=0,\n",
              "dataloader_pin_memory=True,\n",
              "ddp_bucket_cap_mb=None,\n",
              "ddp_find_unused_parameters=None,\n",
              "ddp_timeout=1800,\n",
              "debug=[],\n",
              "deepspeed=None,\n",
              "disable_tqdm=False,\n",
              "do_eval=False,\n",
              "do_predict=False,\n",
              "do_train=False,\n",
              "eval_accumulation_steps=None,\n",
              "eval_delay=0,\n",
              "eval_steps=None,\n",
              "evaluation_strategy=no,\n",
              "fp16=True,\n",
              "fp16_backend=auto,\n",
              "fp16_full_eval=False,\n",
              "fp16_opt_level=O1,\n",
              "fsdp=[],\n",
              "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
              "fsdp_min_num_params=0,\n",
              "fsdp_transformer_layer_cls_to_wrap=None,\n",
              "full_determinism=False,\n",
              "gradient_accumulation_steps=1,\n",
              "gradient_checkpointing=False,\n",
              "greater_is_better=None,\n",
              "group_by_length=False,\n",
              "half_precision_backend=auto,\n",
              "hub_model_id=None,\n",
              "hub_private_repo=False,\n",
              "hub_strategy=every_save,\n",
              "hub_token=<HUB_TOKEN>,\n",
              "ignore_data_skip=False,\n",
              "include_inputs_for_metrics=False,\n",
              "jit_mode_eval=False,\n",
              "label_names=None,\n",
              "label_smoothing_factor=0.0,\n",
              "learning_rate=5e-05,\n",
              "length_column_name=length,\n",
              "load_best_model_at_end=False,\n",
              "local_rank=-1,\n",
              "log_level=passive,\n",
              "log_level_replica=warning,\n",
              "log_on_each_node=True,\n",
              "logging_dir=working_dir/runs/Mar21_23-20-16_2d41e1d75fb5,\n",
              "logging_first_step=False,\n",
              "logging_nan_inf_filter=True,\n",
              "logging_steps=500,\n",
              "logging_strategy=steps,\n",
              "lr_scheduler_type=cosine,\n",
              "max_grad_norm=1.0,\n",
              "max_steps=-1,\n",
              "metric_for_best_model=None,\n",
              "mp_parameters=,\n",
              "no_cuda=False,\n",
              "num_train_epochs=3.0,\n",
              "optim=adamw_hf,\n",
              "optim_args=None,\n",
              "output_dir=working_dir,\n",
              "overwrite_output_dir=False,\n",
              "past_index=-1,\n",
              "per_device_eval_batch_size=1,\n",
              "per_device_train_batch_size=1,\n",
              "prediction_loss_only=False,\n",
              "push_to_hub=False,\n",
              "push_to_hub_model_id=None,\n",
              "push_to_hub_organization=None,\n",
              "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
              "ray_scope=last,\n",
              "remove_unused_columns=True,\n",
              "report_to=['tensorboard'],\n",
              "resume_from_checkpoint=None,\n",
              "run_name=working_dir,\n",
              "save_on_each_node=False,\n",
              "save_steps=500,\n",
              "save_strategy=steps,\n",
              "save_total_limit=None,\n",
              "seed=42,\n",
              "sharded_ddp=[],\n",
              "skip_memory_metrics=True,\n",
              "tf32=None,\n",
              "torch_compile=False,\n",
              "torch_compile_backend=None,\n",
              "torch_compile_mode=None,\n",
              "torchdynamo=None,\n",
              "tpu_metrics_debug=False,\n",
              "tpu_num_cores=None,\n",
              "use_ipex=False,\n",
              "use_legacy_prediction_loop=False,\n",
              "use_mps_device=False,\n",
              "warmup_ratio=0.05,\n",
              "warmup_steps=0,\n",
              "weight_decay=0.0,\n",
              "xpu_backend=None,\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
        "from torch.distributed.fsdp.fully_sharded_data_parallel import (\n",
        "    CPUOffload,\n",
        "    BackwardPrefetch,\n",
        ")\n",
        "from torch.distributed.fsdp.wrap import (\n",
        "    size_based_auto_wrap_policy,\n",
        "    enable_wrap,\n",
        "    wrap,\n",
        ")\n",
        "\n",
        "def train(rank, model, tokenizer):\n",
        "    if tokenizer.pad_token is None:\n",
        "        smart_tokenizer_and_embedding_resize(\n",
        "            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
        "            tokenizer=tokenizer,\n",
        "            model=model,\n",
        "        )\n",
        "\n",
        "    tokenizer.add_special_tokens(\n",
        "        {\n",
        "             \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "             \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "             \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "         }\n",
        "     )\n",
        "    data_module = make_supervised_data_module(tokenizer=tokenizer, data_path=data_path)\n",
        "    my_auto_wrap_policy = functools.partial(\n",
        "        size_based_auto_wrap_policy, min_num_params=100\n",
        "    )\n",
        "    torch.cuda.set_device(rank)\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=200, num_training_steps=-1\n",
        "    )\n",
        "    train_loader = DataLoader(data_module, batch_size=1, shuffle=True)\n",
        "    model = model.to(rank)\n",
        "    model = FSDP(model, fsdp_auto_wrap_policy=my_auto_wrap_policy)\n",
        "    ddp_loss = torch.zeros(2).to(rank)\n",
        "    for epoch in range(3):\n",
        "        for batch in train_loader:\n",
        "            optim.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(rank)\n",
        "            attention_mask = batch['attention_mask'].to(rank)\n",
        "            labels = batch['labels'].to(rank)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs[0]\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            ddp_loss[0] += loss.item()\n",
        "            ddp_loss[1] += len(data)\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n",
        "    if rank == 0:\n",
        "        print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, ddp_loss[0] / ddp_loss[1]))\n",
        "\n"
      ],
      "metadata": {
        "id": "V9ohcuy46ZHv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(0, model, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "ZlbfaokCecqc",
        "outputId": "502f88e9-0967-477a-dd50-6d8395e7ef41"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Loading data...\n",
            "WARNING:root:data collator for supervised learning...\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-a922c6a3b694>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-0c9725276d2f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(rank, model, tokenizer)\u001b[0m\n\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     42\u001b[0m     \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFSDP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfsdp_auto_wrap_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_auto_wrap_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mddp_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1806\u001b[0m             )\n\u001b[1;32m   1807\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1808\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1810\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         \"\"\"\n\u001b[0m\u001b[1;32m    990\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0mTo\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m         \u001b[0mwould\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mget_submodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"net_b.linear\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mTo\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mwhether\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m         \u001b[0mwe\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mwould\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mget_submodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"net_b.net_c.conv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0mTo\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m         \u001b[0mwould\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mget_submodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"net_b.linear\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mTo\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mwhether\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m         \u001b[0mwe\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mwould\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mget_submodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"net_b.net_c.conv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0mTo\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m         \u001b[0mwould\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mget_submodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"net_b.linear\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mTo\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mwhether\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m         \u001b[0mwe\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mwould\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mget_submodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"net_b.net_c.conv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0mTo\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m         \u001b[0mwould\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mget_submodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"net_b.linear\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mTo\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mwhether\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m         \u001b[0mwe\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mwould\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mget_submodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"net_b.net_c.conv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0mTo\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m         \u001b[0mwould\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mget_submodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"net_b.linear\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mTo\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mwhether\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m         \u001b[0mwe\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mwould\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mget_submodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"net_b.net_c.conv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    662\u001b[0m                 \u001b[0mpath\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mresolves\u001b[0m \u001b[0mto\u001b[0m \u001b[0msomething\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0man\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m                 \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         \"\"\"\n\u001b[0m\u001b[1;32m    665\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mmodifies\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m         \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \"\"\"\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 14.75 GiB total capacity; 13.93 GiB already allocated; 32.81 MiB free; 13.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "os.environ['CUDA_VISIBLE_DEVICES']='0, 1, 2, 3, 4, 5, 6, 7, 8'\n",
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
      ],
      "metadata": {
        "id": "nl0oS2QmRuin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train(model, tokenizer)"
      ],
      "metadata": {
        "id": "ckc6RjnCKukt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install libnvinfer_plugin.so.8"
      ],
      "metadata": {
        "id": "1l6tdTyLtblG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!torchrun --nproc_per_node 1 /content/drive/MyDrive/train.py "
      ],
      "metadata": {
        "id": "fYIc9GM49PmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!deepspeed --num_gpus=0 /content/drive/MyDrive/train.py --deepspeed /content/drive/MyDrive/artifacts/config/ds_config.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuIA7nfn7VgS",
        "outputId": "ec3044c0-3ecc-4916-c2c3-b7fac7ede375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-03-21 08:31:54,168] [WARNING] [runner.py:186:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "[2023-03-21 08:31:54,765] [INFO] [runner.py:550:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /content/drive/MyDrive/train.py --deepspeed /content/drive/MyDrive/artifacts/config/ds_config.json\n",
            "[2023-03-21 08:31:57,334] [INFO] [launch.py:135:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.2-1+cuda11.8\n",
            "[2023-03-21 08:31:57,334] [INFO] [launch.py:135:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.2-1\n",
            "[2023-03-21 08:31:57,334] [INFO] [launch.py:135:main] 0 NCCL_VERSION=2.16.2-1\n",
            "[2023-03-21 08:31:57,334] [INFO] [launch.py:135:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
            "[2023-03-21 08:31:57,334] [INFO] [launch.py:135:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.2-1+cuda11.8\n",
            "[2023-03-21 08:31:57,334] [INFO] [launch.py:135:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
            "[2023-03-21 08:31:57,334] [INFO] [launch.py:135:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.2-1\n",
            "[2023-03-21 08:31:57,334] [INFO] [launch.py:142:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2023-03-21 08:31:57,334] [INFO] [launch.py:148:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2023-03-21 08:31:57,334] [INFO] [launch.py:161:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2023-03-21 08:31:57,334] [INFO] [launch.py:162:main] dist_world_size=1\n",
            "[2023-03-21 08:31:57,334] [INFO] [launch.py:164:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "2023-03-21 08:32:01.019300: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-21 08:32:01.019961: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-21 08:32:01.019986: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Loading checkpoint shards: 100% 33/33 [01:14<00:00,  2.25s/it]\n",
            "Using pad_token, but it is not set yet.\n",
            "WARNING:root:Loading data...\n",
            "WARNING:root:data collator for supervised learning...\n",
            "train_dataset: \n",
            " <__main__.SupervisedDataset object at 0x7fed20a6ad30>\n",
            "data_collator: \n",
            " <__main__.DataCollatorForSupervisedDataset object at 0x7feceeccf6d0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/train.py\", line 222, in <module>\n",
            "    train(model, tokenizer)\n",
            "  File \"/content/drive/MyDrive/train.py\", line 217, in train\n",
            "    trainer.train()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\", line 1636, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\", line 1723, in _inner_training_loop\n",
            "    model = self._wrap_model(self.model_wrapped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\", line 1461, in _wrap_model\n",
            "    self.model = model = FSDP(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 1079, in __init__\n",
            "    self._fsdp_wrapped_module = FlattenParamsWrapper(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/fsdp/flatten_params_wrapper.py\", line 103, in __init__\n",
            "    self._flat_param_handle = FlatParamHandle(params, module, device, config)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/fsdp/flat_param.py\", line 270, in __init__\n",
            "    self._init_flat_param(params, module)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/fsdp/flat_param.py\", line 357, in _init_flat_param\n",
            "    self.flat_param = FlatParamHandle.flatten_params(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/fsdp/flat_param.py\", line 388, in flatten_params\n",
            "    flat_param_data = torch.cat(flat_params, dim=0)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 25.10 GiB (GPU 0; 39.56 GiB total capacity; 25.17 GiB already allocated; 13.22 GiB free; 25.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "[2023-03-21 08:34:34,505] [INFO] [launch.py:318:sigkill_handler] Killing subprocess 101698\n",
            "[2023-03-21 08:34:34,505] [ERROR] [launch.py:324:sigkill_handler] ['/usr/bin/python3', '-u', '/content/drive/MyDrive/train.py', '--local_rank=0', '--deepspeed', '/content/drive/MyDrive/artifacts/config/ds_config.json'] exits with return code = 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert(False)\n",
        "import copy\n",
        "import logging\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, Dict, Sequence\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import Trainer\n",
        "import dataclasses\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import io\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "from typing import Optional, Sequence, Union\n",
        "\n",
        "import openai\n",
        "import tqdm\n",
        "from openai import openai_object\n",
        "import copy\n",
        "\n",
        "#import utils\n",
        "\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "PROMPT_DICT = [{\n",
        "    \"user_input\":\"solidity program\",\n",
        "    \"completion\": \"invariants\"}]\n",
        "\n",
        "'''\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    model_name_or_path:Optional[str] = field(default='/content/drive/MyDrive/experiments/llama/LLaMA/7B')\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataArguments:\n",
        "    data_path: str = field(default=None, metadata={\"help\": \"/content/datasets\"})\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainingArguments(transformers.TrainingArguments):\n",
        "    cache_dir: Optional[str] = field(default=None)\n",
        "    optim: str = field(default=\"adamw_torch\")\n",
        "    model_max_length: int = field(\n",
        "        default=512,\n",
        "        metadata={\"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"},\n",
        "    )\n",
        "'''\n",
        "def _make_r_io_base(f, mode: str):\n",
        "    if not isinstance(f, io.IOBase):\n",
        "        f = open(f, mode=mode)\n",
        "    return f\n",
        "\n",
        "def jload(f, mode=\"r\"):\n",
        "    \"\"\"Load a .json file into a dictionary.\"\"\"\n",
        "    f = _make_r_io_base(f, mode)\n",
        "    jdict = json.load(f)\n",
        "    f.close()\n",
        "    return jdict\n",
        "\n",
        "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
        "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
        "    state_dict = trainer.model.state_dict()\n",
        "    if trainer.args.should_save:\n",
        "        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
        "        del state_dict\n",
        "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n",
        "\n",
        "\n",
        "def smart_tokenizer_and_embedding_resize(\n",
        "    special_tokens_dict: Dict,\n",
        "    tokenizer: transformers.PreTrainedTokenizer,\n",
        "    model: transformers.PreTrainedModel,\n",
        "):\n",
        "    \"\"\"Resize tokenizer and embedding.\n",
        "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
        "    \"\"\"\n",
        "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if num_new_tokens > 0:\n",
        "        input_embeddings = model.get_input_embeddings().weight.data\n",
        "        output_embeddings = model.get_output_embeddings().weight.data\n",
        "\n",
        "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "\n",
        "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
        "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
        "\n",
        "\n",
        "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
        "    \"\"\"Tokenize a list of strings.\"\"\"\n",
        "    tokenized_list = [\n",
        "        tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"longest\",\n",
        "            max_length=tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        for text in strings\n",
        "    ]\n",
        "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "    input_ids_lens = labels_lens = [\n",
        "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
        "    ]\n",
        "    return dict(\n",
        "        input_ids=input_ids,\n",
        "        labels=labels,\n",
        "        input_ids_lens=input_ids_lens,\n",
        "        labels_lens=labels_lens,\n",
        "    )\n",
        "\n",
        "\n",
        "def preprocess(\n",
        "    sources: Sequence[str],\n",
        "    targets: Sequence[str],\n",
        "    tokenizer: transformers.PreTrainedTokenizer,\n",
        ") -> Dict:\n",
        "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
        "    examples = [s + t for s, t in zip(sources, targets)]\n",
        "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
        "    input_ids = examples_tokenized[\"input_ids\"]\n",
        "    labels = copy.deepcopy(input_ids)\n",
        "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "        label[:source_len] = IGNORE_INDEX\n",
        "    return dict(input_ids=input_ids, labels=labels)\n",
        "\n",
        "\n",
        "class SupervisedDataset(Dataset):\n",
        "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer):\n",
        "        super(SupervisedDataset, self).__init__()\n",
        "        logging.warning(\"Loading data...\")\n",
        "        list_data_dict = jload(data_path)\n",
        "\n",
        "        logging.warning(\"Formatting inputs...\")\n",
        "        prompt_input, prompt_no_input = PROMPT_DICT[\"user_input\"], PROMPT_DICT[\"completion\"]\n",
        "        sources = [\n",
        "            prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\" else prompt_no_input.format_map(example)\n",
        "            for example in list_data_dict\n",
        "        ]\n",
        "        targets = [f\"{example['output']}{tokenizer.eos_token}\" for example in list_data_dict]\n",
        "\n",
        "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
        "        data_dict = preprocess(sources, targets, tokenizer)\n",
        "\n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )\n",
        "\n",
        "'''\n",
        "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer, data_args) -> Dict:\n",
        "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
        "    train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path)\n",
        "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
        "    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)\n",
        "\n",
        "\n",
        "def train():\n",
        "    parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
        "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
        "\n",
        "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        cache_dir=training_args.cache_dir,\n",
        "    )\n",
        "\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        cache_dir=training_args.cache_dir,\n",
        "        model_max_length=training_args.model_max_length,\n",
        "        padding_side=\"right\",\n",
        "        use_fast=False,\n",
        "    )\n",
        "    if tokenizer.pad_token is None:\n",
        "        smart_tokenizer_and_embedding_resize(\n",
        "            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
        "            tokenizer=tokenizer,\n",
        "            model=model,\n",
        "        )\n",
        "    if \"llama\" in model_args.model_name_or_path:\n",
        "        tokenizer.add_special_tokens(\n",
        "            {\n",
        "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n",
        "    trainer = Trainer(model=model, tokenizer=tokenizer, args=training_args, **data_module)\n",
        "    trainer.train()\n",
        "    trainer.save_state()\n",
        "    safe_save_model_for_hf_trainer(trainer=trainer, output_dir='/content/saved_model')\n",
        " \n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "#    train()"
      ],
      "metadata": {
        "id": "bfZC4kepAWMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tatsu-lab/stanford_alpaca.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hSIKVY9e7Vb",
        "outputId": "1d868bce-8cae-4e9c-822c-bd4ec5608f3e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stanford_alpaca'...\n",
            "remote: Enumerating objects: 100, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 100 (delta 44), reused 38 (delta 38), pack-reused 41\u001b[K\n",
            "Receiving objects: 100% (100/100), 9.13 MiB | 5.43 MiB/s, done.\n",
            "Resolving deltas: 100% (48/48), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8p8DnNn4fLQ1",
        "outputId": "76c5574a-e143-405b-e066-b3b29bb897f4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.13.1+cu116)\n",
            "Collecting torch\n",
            "  Downloading torch-2.0.0-cp39-cp39-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 KB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0\n",
            "  Downloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 KB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.10.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (63.4.3)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Collecting lit\n",
            "  Downloading lit-16.0.0.tar.gz (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 KB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-16.0.0-py3-none-any.whl size=93601 sha256=229278d1fb3d1ea2f5b534b470f6b3f43e8a7e89233a313ad3464ed3447ff1ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/ee/80/1520ca86c3557f70e5504b802072f7fc3b0e2147f376b133ed\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 2.0.0 which is incompatible.\n",
            "fastai 2.7.11 requires torch<1.14,>=1.7, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-16.0.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 triton-2.0.0\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!torchrun --nproc_per_node=1 --master_port=22 /content/stanford_alpaca/train.py \\\n",
        "    --model_name_or_path '/content/drive/MyDrive/experiments/HF_Llama/llama-7b'\\\n",
        "    --data_path '/content/datasets/actor_training_data.json' \\\n",
        "    --bf16 True \\\n",
        "    --output_dir '/content/saved_model' \\\n",
        "    --num_train_epochs 3 \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --per_device_eval_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --evaluation_strategy \"no\" \\\n",
        "    --save_strategy \"steps\" \\\n",
        "    --save_steps 2000 \\\n",
        "    --save_total_limit 1 \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --weight_decay 0. \\\n",
        "    --warmup_ratio 0.03 \\\n",
        "    --lr_scheduler_type \"cosine\" \\\n",
        "    --logging_steps 1 \\\n",
        "    --fsdp \"full_shard auto_wrap\" \\\n",
        "    --fsdp_transformer_layer_cls_to_wrap 'LLaMADecoderLayer' \\\n",
        "    --tf32 True"
      ],
      "metadata": {
        "id": "OgPvq5Dvf4E-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1617889a-cccf-467c-a512-4d8c3a3f3722"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-21 23:26:16.854166: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-21 23:26:16.854251: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-21 23:26:16.854263: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/stanford_alpaca/train.py\", line 231, in <module>\n",
            "    train()\n",
            "  File \"/content/stanford_alpaca/train.py\", line 194, in train\n",
            "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/hf_argparser.py\", line 332, in parse_args_into_dataclasses\n",
            "    obj = dtype(**inputs)\n",
            "  File \"<string>\", line 111, in __init__\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/training_args.py\", line 1190, in __post_init__\n",
            "    raise ValueError(\n",
            "ValueError: Your setup doesn't support bf16/gpu. You need torch>=1.10, using Ampere GPU with cuda>=11.0\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 12570) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/run.py\", line 762, in main\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/run.py\", line 753, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 246, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "/content/stanford_alpaca/train.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2023-03-21_23:26:19\n",
            "  host      : 2d41e1d75fb5\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 1 (pid: 12570)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Finetuning##"
      ],
      "metadata": {
        "id": "SeKTNjbuD75U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chatllama-py"
      ],
      "metadata": {
        "id": "xqmOQKjvhhDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!zip -r /content/raw_datasets.zip /content/datasets\n",
        "#from google.colab import files\n",
        "#files.download(\"/content/raw_datasets.zip\")"
      ],
      "metadata": {
        "id": "2h5lq9EeDxZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepspeed\n",
        "!pip install mpi4py\n",
        "!pip install ninja"
      ],
      "metadata": {
        "id": "VzZ-zYLEwrOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget -O artifacts.zip https://nbllabartifacts.blob.core.windows.net/chatllama/artifacts.zip\\?sp\\=r\\&st\\=2023-03-08T14:53:24Z\\&se\\=2100-03-08T22:53:24Z\\&spr\\=https\\&sv\\=2021-06-08\\&sr\\=b\\&sig\\=jqr%2B2ZkR0SW9RjV0pDOdQ%2BDulLXLjbZ36vmNd4XxxyQ%3D\n",
        "#!unzip artifacts.zip\n"
      ],
      "metadata": {
        "id": "DrXX9phFLEm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/llama.git\n",
        "!pip install -r /content/llama/requirements.txt\n",
        "!pip install -e /content/llama/."
      ],
      "metadata": {
        "id": "X8irCLfEfULu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!deepspeed /content/artifacts/main.py -t ACTOR /content/artifacts/config/config.yaml"
      ],
      "metadata": {
        "id": "YlinPyD8mSlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPT-style finetuning: OOM error persisted##"
      ],
      "metadata": {
        "id": "1ksQlD8qhyV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "from numpy import random\n",
        "import pandas as pd\n",
        "import gspread\n",
        "#autenticating to google\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "#defining my worksheet\n",
        "worksheet = gc.open('short_invariants_for_gpt').sheet1\n",
        "#get_all_values gives a list of rows\n",
        "rows = worksheet.get_all_values()\n",
        "#Convert to a DataFrame \n",
        "cols = ['Target']\n",
        "df = pd.DataFrame(rows, columns=cols)\n",
        "df"
      ],
      "metadata": {
        "id": "SB1TLeCiD53K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_params={\n",
        "    \"MODEL\":\"llama_model\",             # model_type: t5-large\n",
        "    \"MAX_LENGTH\": 1024,  # max length of source text\n",
        "   # \"SEED\": random.randint(1000)    # randomized seeds to shuffle test set\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "Wm0QPuWiGPGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlYaKW9h4ai_"
      },
      "source": [
        "# Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vLQPGAn4v17"
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from rich.console import Console\n",
        "from tqdm import tqdm, trange\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import torch\n",
        "import csv\n",
        "\n",
        "class GPTDataSetClass(Dataset):\n",
        "  \"\"\"\n",
        "  Creating a custom dataset for reading the dataset and \n",
        "  loading it into the dataloader to pass it to the neural network for finetuning the model\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, target_label, truncate=False, \\\n",
        "               gpt2_type=model, \\\n",
        "               max_length=2048):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.target_tokens = []\n",
        "\n",
        "    for row in df['Target']:\n",
        "        self.target_tokens.append(torch.tensor(\n",
        "            self.tokenizer.encode(f\"<|{target_label}|>{row[:max_length]}<|endoftext|>\")\n",
        "            ))  \n",
        "    if truncate:\n",
        "            self.target_tokens = self.target_tokens[:20000]\n",
        "    self.length = len(self.target_tokens)   \n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.target_tokens[index]   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Accumulated batch size (since GPT2 is so big)\n",
        "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
        "    if packed_tensor is None:\n",
        "        return new_tensor, True, None\n",
        "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
        "        return packed_tensor, False, new_tensor\n",
        "    else:\n",
        "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
        "        return packed_tensor, True, None"
      ],
      "metadata": {
        "id": "gJ2ccsedzcAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    dataset, model, tokenizer,\n",
        "    batch_size=0.1, epochs=3, lr=2e-5,\n",
        "    max_seq_len=2048, warmup_steps=200,\n",
        "    gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\",\n",
        "    test_mode=False, save_model_on_epoch=False,\n",
        "):\n",
        "    acc_steps = 10\n",
        "    device=torch.device(\"gpu\")\n",
        "    model = model.to_device('gpu')\n",
        "    model.train()\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
        "    )\n",
        "\n",
        "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "    loss=0\n",
        "    accumulating_batch_count = 0\n",
        "    input_tensor = None\n",
        "    loss_list = []\n",
        "    epoch_list = []\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = []\n",
        "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
        "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
        "\n",
        "            if carry_on and idx != len(train_dataloader) - 1:\n",
        "                continue\n",
        "\n",
        "            input_tensor = input_tensor.to('gpu')\n",
        "            outputs = model(input_tensor, labels=input_tensor)  \n",
        "            loss = outputs[0] \n",
        "            total_loss.append(float(loss.item()))       \n",
        "            loss.backward()\n",
        "            torch.cuda.empty_cache()                      \n",
        "            if (accumulating_batch_count % batch_size) == 0:\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                model.zero_grad()\n",
        "  \n",
        "            accumulating_batch_count += 1\n",
        "            input_tensor = None\n",
        "\n",
        "        #training_logger.add_row(str(epoch), str(np.mean(total_loss)))       \n",
        "        if save_model_on_epoch:\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
        "            )\n",
        "        loss_list.append(np.mean(total_loss))\n",
        "        epoch_list.append(epoch) \n",
        "        print(f\"for epoch {epoch} the loss is {np.mean(total_loss)}\\n\")\n",
        "   # console.print(training_logger)   \n",
        "   # plot_loss(epoch_list, loss_list)\n",
        "    return model"
      ],
      "metadata": {
        "id": "PTVyI2fzzn3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_GPT2(df, model_params):   \n",
        "  #console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
        "\n",
        "  # tokenzier for encoding the text\n",
        "  dataset = GPTDataSetClass(df['Target'], truncate=False, gpt2_type=model) \n",
        "  #Get the tokenizer and model\n",
        "  #tokenizer = GPT2Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
        " # model = GPT2LMHeadModel.from_pretrained(model_params[\"MODEL\"])  \n",
        "  trained_model = train(dataset, model, tokenizer)\n",
        "  console.log(f\"[Saving Model]...\\n\")\n",
        "  #Saving the model after training\n",
        "  path = os.path.join('/content/output', \"model_files\")\n",
        "  model.save_pretrained(path)\n",
        "  tokenizer.save_pretrained(path)\n",
        "  console.print(f\"\"\"[Model] Model saved @ {os.path.join('/content/output', \"model_files\")}\\n\"\"\")\n",
        "  \n",
        "  # logging\n",
        "  console.log(f\"[Data]: Reading Raw data...\\n\")\n",
        "\n",
        "\n",
        "  # Creation of Dataset and Dataloader\n",
        "  # Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
        "\n",
        "  console.print(f\"FULL Dataset: {df.shape}\")\n",
        "  return trained_model, tokenizer\n"
      ],
      "metadata": {
        "id": "nOHr5lM82h53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    entry_count=10,\n",
        "    entry_length=2048, #maximum number of words\n",
        "    top_p=0.8,\n",
        "    temperature=1.,\n",
        "):\n",
        "    model.eval()\n",
        "    generated_num = 0\n",
        "    generated_list = []\n",
        "\n",
        "    filter_value = -float(\"Inf\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for entry_idx in trange(entry_count):\n",
        "\n",
        "            entry_finished = False\n",
        "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "\n",
        "            for i in range(entry_length):\n",
        "                outputs = model(generated, labels=generated)\n",
        "                loss, logits = outputs[:2]\n",
        "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
        "                    ..., :-1\n",
        "                ].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                logits[:, indices_to_remove] = filter_value\n",
        "\n",
        "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
        "                generated = torch.cat((generated, next_token), dim=1)\n",
        "\n",
        "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
        "                    entry_finished = True\n",
        "\n",
        "                if entry_finished:\n",
        "\n",
        "                    generated_num = generated_num + 1\n",
        "\n",
        "                    output_list = list(generated.squeeze().numpy())\n",
        "                    output_text = tokenizer.decode(output_list)\n",
        "                    generated_list.append(output_text + '\\n')\n",
        "                    break\n",
        "            \n",
        "            if not entry_finished:\n",
        "              output_list = list(generated.squeeze().numpy())\n",
        "              output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n",
        "              generated_list.append(output_text + '\\n')\n",
        "                \n",
        "    return generated_list\n",
        "\n",
        "#Function to generate multiple sentences. Test data should be a dataframe\n",
        "def text_generation(model, tokenizer, test_data):\n",
        "  generated_code = []\n",
        "  for i in range(len(test_data)):    \n",
        "    x = generate(model.to('cpu'), tokenizer, test_data['Target'][i], entry_count=1)\n",
        "    generated_code.append(x)\n",
        "  return generated_code\n",
        "\n",
        "#Run the functions to generate the lyrics\n",
        "\n",
        "def test_fine_tuned_gpt2(model, tokenizer, df): \n",
        "    test_set = df.sample(n=1)\n",
        "    df = df.loc[~df.index.isin(test_set.index)]\n",
        "\n",
        "    #Reset the indexes\n",
        "    test_set = test_set.reset_index()\n",
        "    df = df.reset_index()\n",
        "\n",
        "    #For the test set only, keep last 20 words in a new column, then remove them from original column\n",
        "    test_set['Target'] = test_set['Target'].str.split().apply(' '.join)\n",
        "    generated_code = text_generation(model, tokenizer, test_set)\n",
        "    print(generated_code)\n"
      ],
      "metadata": {
        "id": "ujnNVivM1bMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To solve CUDA out of memory error\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "os.environ['CUDA_VISIBLE_DEVICES']='0, 1, 2, 3, 4, 5, 6, 7, 8'\n",
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
      ],
      "metadata": {
        "id": "atnMJANnJlf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model, trained_tokenizer = fine_tune_GPT2(df, model_params)"
      ],
      "metadata": {
        "id": "GtBLzx0bHiXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-4qjuS_RhdxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "import deepspeed\n",
        "import torch\n",
        "from beartype import beartype\n",
        "from beartype.typing import Tuple\n",
        "from einops import rearrange\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "local_rank, world_size = setup_model_parallel()\n",
        "            # use load_model_test for testing\n",
        "  self.model, self.tokenizer = load_model(\n",
        "   ckpt_dir=model,\n",
        "   tokenizer_path=tokenizer,\n",
        "   local_rank=local_rank,\n",
        "   world_size=world_size,\n",
        "   froze_embeddings=config.froze_embeddings,\n",
        "   use_fairscale=config.use_fairscale,\n",
        "   max_batch_size=config.batch_size,\n",
        "    )\n",
        "        elif config.model in hf_models_seq_2_seq:\n",
        "    \n",
        "            # galactica tokenizer eos_token is None\n",
        "            if self.tokenizer.eos_token is None:\n",
        "                self.tokenizer.eos_token = \"</s>\"\n",
        "                self.tokenizer.eos_token_id = 0\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                config.model,\n",
        "            )\n",
        "            self.model.to(config.device)\n",
        "\n",
        "        # load the model from model_folder\n",
        "        self.load()\n",
        "\n",
        "    @beartype\n",
        "    def load(self) -> None:\n",
        "        \"\"\"Load the model from the path\n",
        "\n",
        "        Args:\n",
        "            path (str): path to the model\n",
        "        \"\"\"\n",
        "        path = ModelLoader.check_model_path(\n",
        "            config=self.config,\n",
        "            is_checkpoint=False,\n",
        "            current_epoch=None,\n",
        "        )\n",
        "        if path is not None:\n",
        "            print(\"Loading ...\")\n",
        "            model_dict = torch.load(path)\n",
        "            self.model.load_state_dict(model_dict[\"model\"])\n",
        "            self.head.load_state_dict(model_dict[\"head\"])\n",
        "\n",
        "    @beartype\n",
        "    def save(self) -> None:\n",
        "        \"\"\"Save the model to the path\n",
        "\n",
        "        Args:\n",
        "            path (Optional[str], optional): Path to store the model.\n",
        "                Defaults to None.\n",
        "        \"\"\"\n",
        "        model_folder, model_name, path = ModelLoader.get_model_path(\n",
        "            config=self.config,\n",
        "            is_checkpoint=False,\n",
        "            current_epoch=None,\n",
        "        )\n",
        "        print(f\"Saving model to {path} ...\")\n",
        "        torch.save(\n",
        "            {\"model\": self.model.state_dict(), \"head\": self.head.state_dict()},\n",
        "            path,\n",
        "        )\n",
        "\n",
        "    def parameters(self, **kwargs):\n",
        "        \"\"\"Return the parameters of the model\n",
        "\n",
        "        Args:\n",
        "            **kwargs:\n",
        "        \"\"\"\n",
        "        return self.model.parameters()\n",
        "\n",
        "    @beartype\n",
        "    def forward(\n",
        "        self, sequences: torch.Tensor, sequences_mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Generate logits to have probability distribution over the vocabulary\n",
        "            of the actions\n",
        "\n",
        "        Args:\n",
        "            sequences (torch.Tensor): Sequences of states and actions used to\n",
        "                    compute token logits for the whole list of sequences\n",
        "            attention_mask (torch.Tensor): Mask for the sequences attention\n",
        "\n",
        "        Returns:\n",
        "            logits (torch.Tensor): Logits for the actions taken\n",
        "        \"\"\"\n",
        "        model_output = self.model.forward(\n",
        "            sequences, attention_mask=sequences_mask\n",
        "        )\n",
        "        # need to return logits for the actions\n",
        "        if self.config.model in hf_models_causal_lm:\n",
        "            model_output = model_output.logits\n",
        "        if self.config.debug:\n",
        "            print(\"ActorModel.forward\")\n",
        "            print(\"model_output_logits shape\", model_output.shape)\n",
        "            print(\"model_output logits\", model_output)\n",
        "        return model_output\n",
        "\n",
        "    @beartype\n",
        "    @torch.no_grad()\n",
        "    def generate(\n",
        "        self, states: torch.Tensor, state_mask: torch.Tensor\n",
        "    ) -> Tuple:\n",
        "        \"\"\"Generate actions and sequences=[states, actions] from state\n",
        "            (i.e. input of the prompt generator model)\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor): the input of the user\n",
        "            state_mask (torch.Tensor): Mask for the state input (for padding)\n",
        "\n",
        "        Returns:\n",
        "            actions (torch.Tensor): Actions generated from the state\n",
        "            sequences (torch.Tensor): Sequences generated from the\n",
        "                state as [states, actions]\n",
        "        \"\"\"\n",
        "        temperature = self.config.temperature\n",
        "        # max sequence length for the actor (i.e. prompt + completion)\n",
        "        # from config file - it depends by the model used\n",
        "        max_sequence_length = self.config.max_sequence_length\n",
        "        # max tokens generated by the actor (completion only) from config file\n",
        "        max_tokens = self.config.max_tokens\n",
        "        # temperature for the actor\n",
        "        max_generation_possible = max_sequence_length - states.shape[1]\n",
        "        # take the minimum between the maximum token that you want to generate\n",
        "        # and the token that is possible to generate given the maximum sequence\n",
        "        # supported\n",
        "        max_completion = min(max_tokens, max_generation_possible)\n",
        "        if max_completion <= 0:\n",
        "            raise ValueError(\n",
        "                \"The maximum completion available is <= 0 the prompt is too \"\n",
        "                + \"long w.r.t the model sequence length\"\n",
        "            )\n",
        "        # the max_length is then the input length + the completion length\n",
        "        max_length = states.shape[1] + max_completion\n",
        "        # generate\n",
        "        sequences = self.model.generate(\n",
        "            input_ids=states,\n",
        "            attention_mask=state_mask,\n",
        "            temperature=temperature,\n",
        "            max_length=max_length,\n",
        "        )\n",
        "        actions = sequences[:, states.shape[1] :]  # noqa E203\n",
        "        if self.config.debug:\n",
        "            print(\"ActorModel.generate\")\n",
        "            print(\"state\", states)\n",
        "            print(\"state shape\", states.shape)\n",
        "            print(\"sequence shape\", sequences.shape)\n",
        "            print(\"sequence\", sequences)\n",
        "            print(\"actions shape\", actions.shape)\n",
        "            print(\"actions\", actions)\n",
        "        return actions, sequences\n",
        "\n",
        "\n",
        "class ActorDataset(Dataset):\n",
        "    \"\"\"Dataset for the pretraining of the actor model\n",
        "    read a json file with the following format:\n",
        "    [\n",
        "        {\n",
        "            \"user_input\": \"...\"\n",
        "            \"completion\": \"...\"\n",
        "        },\n",
        "        ...\n",
        "    ]\n",
        "    Where:\n",
        "        user_input: the input of the user\n",
        "        completion: the output of the user\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path: str) -> None:\n",
        "        self.path = path\n",
        "        with open(path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "            self.data = [d[\"user_input\"] + \" \" + d[\"completion\"] for d in data]\n",
        "        self.len = len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def __len__(\n",
        "        self,\n",
        "    ):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "class ActorTrainer:\n",
        "    \"\"\"Used to pre-train the actor model to generate better prompts.\n",
        "\n",
        "    Args:\n",
        "        config (ConfigActor): Configuration for the actor model\n",
        "\n",
        "    Attributes:\n",
        "        config (ConfigActor): Configuration for the actor model\n",
        "        model (ActorModel): Actor model\n",
        "        loss_function (torch.nn.CrossEntropyLoss): Loss function\n",
        "        optimizer (torch.optim.Adam): Optimizer\n",
        "        validation_flag (bool): Flag to indicate if the validation dataset\n",
        "            is provided\n",
        "        training_stats (TrainingStats): Training statistics\n",
        "\n",
        "    Methods:\n",
        "        train: Train the actor model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: ConfigActor) -> None:\n",
        "        # load the model, optimizer, loss function and config\n",
        "        self.config = config\n",
        "        self.model = ActorModel(config)\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            self.model.parameters(), lr=config.lr\n",
        "        )\n",
        "\n",
        "        # check checkpoint, datasets and other data\n",
        "        self.validation_flag = False\n",
        "        self.training_stats = TrainingStats()\n",
        "        if config.validation_dataset_path is not None:\n",
        "            self.validation_flag = True\n",
        "\n",
        "        # create dataloaders\n",
        "        self.train_dataset = ActorDataset(config.train_dataset_path)\n",
        "        self.train_dataloader = DataLoader(\n",
        "            self.train_dataset, batch_size=config.batch_size\n",
        "        )\n",
        "        if self.validation_flag:\n",
        "            self.eval_dataset = ActorDataset(config.validation_dataset_path)\n",
        "            self.validation_dataloader = DataLoader(\n",
        "                self.eval_dataset, batch_size=config.batch_size\n",
        "            )\n",
        "\n",
        "        # initialize deepspeed\n",
        "        self.model_engine = None\n",
        "        if config.deepspeed_enable is True:\n",
        "            if config.deepspeed_config_path is None:\n",
        "                raise ValueError(\n",
        "                    \"DeepSpeed config path is None, but deepspeed is enabled\"\n",
        "                )\n",
        "            if os.path.exists(config.deepspeed_config_path) is False:\n",
        "                raise ValueError(\n",
        "                    f\"DeepSpeed config path {config.deepspeed_config_path}\"\n",
        "                    f\"does not exist\"\n",
        "                )\n",
        "            (\n",
        "                self.model_engine,\n",
        "                self.optimizer,\n",
        "                self.train_dataloader,\n",
        "                _,\n",
        "            ) = deepspeed.initialize(\n",
        "                args=None,\n",
        "                model=self.model,\n",
        "                model_parameters=self.model.parameters(),\n",
        "                training_data=self.train_dataloader,\n",
        "                config=self.config.deepspeed_config_path,\n",
        "            )\n",
        "\n",
        "    @beartype\n",
        "    def save_checkpoint(\n",
        "        self,\n",
        "        current_epoch: int,\n",
        "        current_step: int,\n",
        "        max_epochs: int,\n",
        "        max_steps: int,\n",
        "    ) -> None:\n",
        "\n",
        "        print(\n",
        "            f\"Saving checkpoint for epoch {current_epoch + 1}, \"\n",
        "            f\"step {current_step + 1} ...\"\n",
        "        )\n",
        "        model_folder, model_name, path = ModelLoader.get_model_path(\n",
        "            config=self.config,\n",
        "            is_checkpoint=True,\n",
        "            current_epoch=current_epoch,\n",
        "            current_step=current_step,\n",
        "            max_epochs=max_epochs,\n",
        "            max_steps=max_steps,\n",
        "        )\n",
        "        if os.path.exists(path):\n",
        "            os.remove(path)\n",
        "        torch.save(\n",
        "            {\n",
        "                \"state_dict\": self.model.state_dict(),\n",
        "                \"optim_state_dict\": self.optimizer.state_dict(),\n",
        "                \"training_stats\": self.training_stats,\n",
        "                \"epoch\": current_epoch,\n",
        "                \"step\": current_step,\n",
        "            },\n",
        "            path,\n",
        "        )\n",
        "\n",
        "    @beartype\n",
        "    def load_checkpoint(\n",
        "        self,\n",
        "    ) -> Tuple[int, int]:\n",
        "        \"\"\"Load a checkpoint from the model folder\"\"\"\n",
        "\n",
        "        print(\"Looking for checkpoints...\")\n",
        "        path = ModelLoader.check_model_path(\n",
        "            config=self.config,\n",
        "            is_checkpoint=True,\n",
        "            current_epoch=None,\n",
        "        )\n",
        "        if path is not None:\n",
        "            print(\"Loading ...\")\n",
        "            checkpoint = torch.load(path)\n",
        "            epoch = checkpoint[\"epoch\"]\n",
        "            self.model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "            self.optimizer.load_state_dict(checkpoint[\"optim_state_dict\"])\n",
        "            self.trainign_stats = checkpoint[\"training_stats\"]\n",
        "            step = checkpoint[\"step\"]\n",
        "            return epoch, step + 1  # return the next episode to train\n",
        "        return 0, 0\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "    ) -> None:\n",
        "        print(\"Start Actor Model Pretraining\")\n",
        "\n",
        "        # get config parameters\n",
        "        batch_size = self.config.batch_size\n",
        "        epochs = self.config.epochs\n",
        "        device = self.config.device\n",
        "        checkpoint_steps = self.config.checkpoint_steps\n",
        "\n",
        "        # compute the number of iterations\n",
        "        n_iter = int(len(self.train_dataset) / batch_size)\n",
        "\n",
        "        # load model_checkpoint\n",
        "        start_epoch, start_step = self.load_checkpoint()\n",
        "\n",
        "        # counter for the checkpoint\n",
        "        cnt_checkpoint = 1\n",
        "\n",
        "        # traing loop\n",
        "        for epoch in range(start_epoch, epochs):\n",
        "            self.model.train()\n",
        "            for i, input_output in enumerate(self.train_dataloader):\n",
        "                if i < start_step:\n",
        "                    continue\n",
        "                with torch.no_grad():\n",
        "                    input_output_tokenized = self.model.tokenizer(\n",
        "                        input_output,\n",
        "                        return_tensors=\"pt\",\n",
        "                        padding=True,\n",
        "                    )\n",
        "                    training_output = input_output_tokenized[\"input_ids\"][\n",
        "                        :, 1:\n",
        "                    ]\n",
        "                    training_input = input_output_tokenized[\"input_ids\"][\n",
        "                        :, :-1\n",
        "                    ]\n",
        "                    attention_mask = input_output_tokenized[\"attention_mask\"][\n",
        "                        :, :-1\n",
        "                    ]\n",
        "                    training_output = training_output.to(device)\n",
        "                    training_input = training_input.to(device)\n",
        "                    attention_mask = attention_mask.to(device)\n",
        "\n",
        "                # forward pass\n",
        "                if self.config.deepspeed_enable:\n",
        "                    est_output = self.model_engine(\n",
        "                        training_input, attention_mask\n",
        "                    )\n",
        "                else:\n",
        "                    est_output = self.model(training_input, attention_mask)\n",
        "                est_output = rearrange(est_output, \"b s v -> (b s) v\")\n",
        "                training_output = rearrange(training_output, \"b s -> (b s)\")\n",
        "                loss = self.loss_function(est_output, training_output)\n",
        "                self.training_stats.training_loss.append(loss.item())\n",
        "\n",
        "                # backward pass\n",
        "                if self.config.deepspeed_enable:\n",
        "                    self.model_engine.backward(loss)\n",
        "                    self.model_engine.step()\n",
        "                else:\n",
        "                    self.optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "\n",
        "                # print progress\n",
        "                if i % self.config.iteration_per_print == 0:\n",
        "                    print(\n",
        "                        f\"Epoch: {epoch+1}/{epochs}, \"\n",
        "                        f\"Iteration: {i+1}/{n_iter}, \"\n",
        "                        f\"Training Loss: {loss}\"\n",
        "                    )\n",
        "                # save checkpoint periodically\n",
        "                if cnt_checkpoint % checkpoint_steps == 0:\n",
        "                    self.save_checkpoint(epoch, i, epochs, n_iter)\n",
        "                    cnt_checkpoint = 1\n",
        "                else:\n",
        "                    cnt_checkpoint += 1\n",
        "\n",
        "            if self.validation_flag:\n",
        "                self.model.eval()\n",
        "                for i, input_output in enumerate(self.validation_dataloader):\n",
        "                    input_output_tokenized = self.model.tokenizer(\n",
        "                        input_output, return_tensors=\"pt\", padding=True\n",
        "                    )\n",
        "                    validation_output = input_output_tokenized[\"input_ids\"][\n",
        "                        :, 1:\n",
        "                    ]\n",
        "                    validation_input = input_output_tokenized[\"input_ids\"][\n",
        "                        :, :-1\n",
        "                    ]\n",
        "                    attention_mask = input_output_tokenized[\"attention_mask\"][\n",
        "                        :, :-1\n",
        "                    ]\n",
        "\n",
        "                    # forward pass\n",
        "                    est_output = self.model.forward(\n",
        "                        validation_input, attention_mask\n",
        "                    )\n",
        "                    validation_output = rearrange(\n",
        "                        validation_output, \"b s -> (b s)\"\n",
        "                    )\n",
        "                    est_output = rearrange(est_output, \"b s v -> (b s) v\")\n",
        "                    loss = self.loss_function(est_output, validation_output)\n",
        "                    self.training_stats.validation_loss.append(loss.item())\n",
        "                    # print progress\n",
        "                    if i % self.config.iteration_per_print == 0:\n",
        "                        print(\n",
        "                            f\"Epoch: {epoch+1}/{epochs}, \"\n",
        "                            f\"Iteration: {i+1}/{n_iter}, \"\n",
        "                            f\"Validation Loss: {loss}\"\n",
        "                        )\n",
        "        self.model.save()\n",
        "        print(\"Training Finished \")"
      ],
      "metadata": {
        "id": "5MreDvACfcCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_test_contrct(path, ratio): \n",
        "  col = ['Target']\n",
        "  f = open(path, \"r\")\n",
        "  file = f.read()\n",
        "  test_df = pd.DataFrame([file], columns=col)\n",
        "  program_length = len(test_df['Target'][0].split())\n",
        "  prompt_ratio = ratio\n",
        "  prompt_length = int(prompt_ratio * program_length)\n",
        "  return test_df, prompt_length\n",
        "\n",
        "def truncate_test(df, prompt_length):\n",
        "  copy_1 = df.copy(deep=True)\n",
        "  copy_2 = df.copy(deep=True)\n",
        "  #true\n",
        "  a = copy_1['Target'].str.split().str[-prompt_length:].apply(' '.join)[0]\n",
        "  #masked out program \n",
        "  b = copy_2['Target'].str.split().str[:-prompt_length].apply(' '.join)[0]\n",
        "  return a, b"
      ],
      "metadata": {
        "id": "_hKPs7PCcuHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df, n = generate_test_contrct('/content/drive/MyDrive/experiments/baseline benchmark/Replica.sol', 0.95)\n",
        "truth, prompt = truncate_test(test_df, n)\n",
        "col = ['Target']\n",
        "prompt_df = pd.DataFrame([prompt], columns=col)\n",
        "gpt_out = text_generation(trained_model, tokenizer, prompt_df)\n",
        "print('this is the GPT prompt without T5: \\n', prompt_df['Target'][0])\n",
        "print('this is the GPT prediction without T5: \\n', gpt_out)"
      ],
      "metadata": {
        "id": "z5vMNi2eHxkh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}